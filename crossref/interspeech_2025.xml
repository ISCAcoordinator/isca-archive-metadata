<doi_batch xmlns="http://www.crossref.org/schema/4.3.7" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.crossref.org/schema/4.3.7 http://www.crossref.org/schemas/crossref4.3.7.xsd" version="4.3.7">
	<head>
		<doi_batch_id>interspeech_2025</doi_batch_id>
		<timestamp>1755182925381162</timestamp>
		<depositor>
			<depositor_name>Martin Cooke</depositor_name>
			<email_address>m.cooke@ikerbasque.org</email_address>
		</depositor>
		<registrant>International Speech Communication Association</registrant>
	</head>
	<body>
		<conference>
			<event_metadata>
				<conference_name>Interspeech 2025</conference_name>
				<conference_acronym>interspeech_2025</conference_acronym>
				<conference_date>17-21 August 2025</conference_date>
			</event_metadata>
			<proceedings_metadata language="en">
				<proceedings_title>Interspeech 2025</proceedings_title>
				<publisher>
					<publisher_name>ISCA</publisher_name>
					<publisher_place>ISCA</publisher_place>
				</publisher>
				<publication_date>
					<year>2025</year>
				</publication_date>
				<noisbn reason='simple_series'/>
				<doi_data>
					<doi>10.21437/Interspeech.2025</doi>
					<timestamp>1755182925381162</timestamp>
					<resource>https://www.isca-archive.org/interspeech_2025/</resource>
				</doi_data>
			</proceedings_metadata>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan Kumar</given_name>
<surname>Das</surname>
</person_name>
					</contributors>
					<titles><title>TF-Mamba: A Time-Frequency Network for Sound Source Localization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>948</first_page>
						<last_page>952</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-9</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xiao25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naohiro</given_name>
<surname>Tawara</surname>
</person_name>
					</contributors>
					<titles><title>Mitigating Non-Target Speaker Bias in Guided Speaker Embedding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5208</first_page>
						<last_page>5212</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-10</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/horiguchi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianyi</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanghao</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan Kumar</given_name>
<surname>Das</surname>
</person_name>
					</contributors>
					<titles><title>AdaKWS: Towards Robust Keyword Spotting with Test-Time Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5408</first_page>
						<last_page>5412</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-15</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xiao25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan Kumar</given_name>
<surname>Das</surname>
</person_name>
					</contributors>
					<titles><title>Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class Incremental Learning Method for Audio Deepfake Source Tracing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1563</first_page>
						<last_page>1567</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-16</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xiao25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiangyu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daijiao</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianyi</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cihan</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tünde</given_name>
<surname>Szalay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mostafa</given_name>
<surname>Shahin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beena</given_name>
<surname>Ahmed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Epps</surname>
</person_name>
					</contributors>
					<titles><title>Auto-Landmark: Acoustic Landmark Dataset and Open-Source Toolkit for Landmark Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4263</first_page>
						<last_page>4267</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-17</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhaoqing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoning</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingwei</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huimeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youjun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Towards One-bit ASR: Extremely Low-bit Conformer Quantization Using Co-training and Stochastic Precision</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1973</first_page>
						<last_page>1977</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-18</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yan Ru</given_name>
<surname>Pei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ritik</given_name>
<surname>Shrivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fnu</given_name>
<surname>Sidharth</surname>
</person_name>
					</contributors>
					<titles><title>Optimized Real-time Speech Enhancement with Deep SSMs on Raw Audio</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>51</first_page>
						<last_page>55</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-19</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pei25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Müller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Kawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Herng</given_name>
<surname>Choong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adriana</given_name>
<surname>Stan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aditya Tirumala</given_name>
<surname>Bukkapatnam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karla</given_name>
<surname>Pizzi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Sperl</surname>
</person_name>
					</contributors>
					<titles><title>Replay Attacks Against Audio Deepfake Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2245</first_page>
						<last_page>2249</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-20</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/muller25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyung-gun</given_name>
<surname>Chi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zakaria</given_name>
<surname>Aldeneh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatiana</given_name>
<surname>Likhomanenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oggi</given_name>
<surname>Rudovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Higuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Wei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed Hussen</given_name>
<surname>Abdelaziz</surname>
</person_name>
					</contributors>
					<titles><title>DiceHuBERT: Distilling HuBERT with a Self-Supervised Learning Objective</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1218</first_page>
						<last_page>1222</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-29</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyung-gun</given_name>
<surname>Chi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Pesce</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wonil</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oggi</given_name>
<surname>Rudovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arturo</given_name>
<surname>Argueta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vineet</given_name>
<surname>Garg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed Hussen</given_name>
<surname>Abdelaziz</surname>
</person_name>
					</contributors>
					<titles><title>Adaptive Knowledge Distillation for Device-Directed Speech Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5788</first_page>
						<last_page>5792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-30</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chi25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryan</given_name>
<surname>Whetten</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Maison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Dinarelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name>
					</contributors>
					<titles><title>Towards Early Prediction of Self-Supervised Speech Model Performance</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1228</first_page>
						<last_page>1232</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-34</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/whetten25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bingliang</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiyu</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Glottal Stop Coda Loss During Sound Change of Checked Syllables Based on Speech-EGG Voice Offset Alignment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2960</first_page>
						<last_page>2964</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-38</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhao25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiheng</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoxu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yafeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gang</given_name>
<surname>Qiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biao</given_name>
<surname>Tian</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Efficient Directional and Distance Cues for Regional Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1478</first_page>
						<last_page>1482</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-40</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jiang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yubin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taehan</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wonjune</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eugene</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joonsik</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongjae</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>McDuff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeonhoon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cynthia</given_name>
<surname>Breazeal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hae Won</given_name>
<surname>Park</surname>
</person_name>
					</contributors>
					<titles><title>VocalAgent: Large Language Models for Vocal Health Diagnostics with Safety-Aware Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4618</first_page>
						<last_page>4622</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-41</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuhang</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinnuo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiqi</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shugong</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Towards Robust Speaker Recognition against Intrinsic Variation with Foundation Model Few-shot Tuning and Effective Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1118</first_page>
						<last_page>1122</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-42</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zexu</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wupeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengkui</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Online Audio-Visual Autoregressive Speaker Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1928</first_page>
						<last_page>1932</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-43</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yaroslav</given_name>
<surname>Getman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamás</given_name>
<surname>Grósz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tommi</given_name>
<surname>Lehtonen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>Is your model big enough? Training and interpreting large-scale monolingual speech foundation models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>231</first_page>
						<last_page>235</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-46</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/getman25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minsu</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pingchuan</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Honglie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stavros</given_name>
<surname>Petridis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maja</given_name>
<surname>Pantic</surname>
</person_name>
					</contributors>
					<titles><title>Revival with Voice: Multi-modal Controllable Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3773</first_page>
						<last_page>3777</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-47</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xue</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guiru</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Separation for an Unknown Number of Speakers with Encoder-Decoder-Based Contextual Information Module</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1448</first_page>
						<last_page>1452</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-49</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhiqi</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meixuan</given_name>
<surname>Bao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhi</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinnuo</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shugong</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3648</first_page>
						<last_page>3652</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-57</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ai25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Lyu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuemin</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenshu</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Temporal Modeling of Room Impulse Response Generation via Multi-Scale Autoregressive Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>923</first_page>
						<last_page>927</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-60</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lyu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingyi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju Seung</given_name>
<surname>Byun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Micha</given_name>
<surname>Elsner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pichao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Perrault</surname>
</person_name>
					</contributors>
					<titles><title>Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3454</first_page>
						<last_page>3458</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-63</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yaqi</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongqing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liming</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Gan</surname>
</person_name>
					</contributors>
					<titles><title>A Robust Hybrid ACC-PM Approach for Personal Sound Zones</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3853</first_page>
						<last_page>3857</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-65</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuting</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuefei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Linguistic Masking and Its Release in Simulated Electric-acoustic Hearing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>848</first_page>
						<last_page>852</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-72</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ding25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaolong</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiawang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shixi</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenpeng</given_name>
<surname>Zhan</surname>
</person_name>
					</contributors>
					<titles><title>Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese Speech Annotation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2540</first_page>
						<last_page>2544</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-76</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Manjie</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenxing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyi</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruibo</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Towards Diverse and Efficient Audio Captioning via Diffusion Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>191</first_page>
						<last_page>195</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-79</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sho</given_name>
<surname>Inoue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>181</first_page>
						<last_page>185</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-80</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/inoue25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiteng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhao</given_name>
<surname>Mei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xubo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangyang</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name>
					</contributors>
					<titles><title>MASV: Speaker Verification with Global and Local Context Mamba</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3703</first_page>
						<last_page>3707</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-82</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Niizumi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daiki</given_name>
<surname>Takeuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masahiro</given_name>
<surname>Yasuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binh Thien</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yasunori</given_name>
<surname>Ohishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noboru</given_name>
<surname>Harada</surname>
</person_name>
					</contributors>
					<titles><title>Towards Pre-training an Effective Respiratory Audio Foundation Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>998</first_page>
						<last_page>1002</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-84</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/niizumi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yujie</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiran</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haolin</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songyi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xihong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Overestimated performance of auditory attention decoding caused by experimental design in EEG recordings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1053</first_page>
						<last_page>1057</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-85</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alice</given_name>
<surname>Ross</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cliodhna</given_name>
<surname>Hughes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eddie L.</given_name>
<surname>Ungless</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lai</surname>
</person_name>
					</contributors>
					<titles><title>Conveying Gender Through Speech: Insights from Trans Men</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>674</first_page>
						<last_page>678</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-86</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ross25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>I-Ting</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chung-Hsien</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Dysarthric Speech Recognition Using Curriculum Learning and Multi-stream Architecture</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2210</first_page>
						<last_page>2214</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-89</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hsieh25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Catarina</given_name>
<surname>Botelho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Gimeno-Gómez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francisco</given_name>
<surname>Teixeira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John</given_name>
<surname>Mendonça</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrícia</given_name>
<surname>Pereira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diogo A. P.</given_name>
<surname>Nunes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Rolland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Pompili</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rubén</given_name>
<surname>Solera-Ureña</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Ponte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David Martins de</given_name>
<surname>Matos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos-D.</given_name>
<surname>Martínez-Hinarejos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic and Linguistic Biomarkers for Cognitive Impairment Detection from Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1418</first_page>
						<last_page>1422</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-91</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/botelho25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oluwasegun</given_name>
<surname>Amoniyan</surname>
</person_name>
					</contributors>
					<titles><title>Is it all about race?: A Cross-examination of /s/ in a Multilingual (Nigerian) Context</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3364</first_page>
						<last_page>3368</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-94</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/amoniyan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuke</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenjie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longshuai</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Weng</surname>
</person_name>
					</contributors>
					<titles><title>Robust Personal Voice Activity Detection for Mitigating Domain Mismatch and False Acceptance Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5783</first_page>
						<last_page>5787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-98</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lin25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Combei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adriana</given_name>
<surname>Stan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Oneata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Müller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Horia</given_name>
<surname>Cucu</surname>
</person_name>
					</contributors>
					<titles><title>Unmasking real-world audio deepfakes: A data-centric approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5343</first_page>
						<last_page>5347</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-100</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/combei25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martijn</given_name>
<surname>Bentum</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis</given_name>
<surname>ten Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomas O.</given_name>
<surname>Lentz</surname>
</person_name>
					</contributors>
					<titles><title>Word stress in self-supervised speech models: A cross-linguistic comparison</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>251</first_page>
						<last_page>255</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-106</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bentum25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nikolai Lund</given_name>
<surname>Kühne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Østergaard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesper</given_name>
<surname>Jensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng-Hua</given_name>
<surname>Tan</surname>
</person_name>
					</contributors>
					<titles><title>xLSTM-SENet: xLSTM for Single-Channel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5148</first_page>
						<last_page>5152</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-108</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kuhne25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Umberto</given_name>
<surname>Cappellazzo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minsu</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stavros</given_name>
<surname>Petridis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniele</given_name>
<surname>Falavigna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessio</given_name>
<surname>Brutti</surname>
</person_name>
					</contributors>
					<titles><title>Scaling and Enhancing LLM-based AVSR:  A Sparse Mixture of Projectors Approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1823</first_page>
						<last_page>1827</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-111</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cappellazzo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yizhou</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiyu</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Perception of Emotional Speech by Individuals with High Borderline Personality Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2028</first_page>
						<last_page>2032</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-113</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sun-Kyung</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong-Hwan</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>CAMER: Contribution-Aware Multimodal Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2690</first_page>
						<last_page>2694</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-114</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lee25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samir</given_name>
<surname>Sadok</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Hauret</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éric</given_name>
<surname>Bavu</surname>
</person_name>
					</contributors>
					<titles><title>Bringing Interpretability to Neural Audio Codecs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5023</first_page>
						<last_page>5027</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-115</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sadok25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chengjia</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James M.</given_name>
<surname>McQueen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hans Rutger</given_name>
<surname>Bosker</surname>
</person_name>
					</contributors>
					<titles><title>A Gradient Effect of Hand Beat Timing on Spoken Word Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3793</first_page>
						<last_page>3797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-116</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ye25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lorenz</given_name>
<surname>Gutscher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Pucher</surname>
</person_name>
					</contributors>
					<titles><title>Audio-Based Classification and Geographic Regression of Austrian Dialects</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2765</first_page>
						<last_page>2769</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-119</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gutscher25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Ando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naohiro</given_name>
<surname>Tawara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name>
					</contributors>
					<titles><title>Pretraining Multi-Speaker Identification for Neural Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1608</first_page>
						<last_page>1612</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-120</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/horiguchi25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wen</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyun</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinan</given_name>
<surname>Duan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diyang</given_name>
<surname>Qu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Runsen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>The 1st SpeechWellness Challenge: Detecting Suicide Risk Among Adolescents</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>399</first_page>
						<last_page>403</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-124</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Riccarda</given_name>
<surname>Funk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melanie</given_name>
<surname>Weirich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Simpson</surname>
</person_name>
					</contributors>
					<titles><title>How sibilant spectra shape gender perception in prepubertal children: A voice morphing study</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>963</first_page>
						<last_page>967</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-125</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/funk25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zijiang</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meishu</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Jing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haojie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kota</given_name>
<surname>Tamada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toru</given_name>
<surname>Takumi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiharu</given_name>
<surname>Yamamoto</surname>
</person_name>
					</contributors>
					<titles><title>MADUV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound Vocalization Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1718</first_page>
						<last_page>1722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-127</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dianwen</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Thinking Fast and Slow: Robust Speech Recognition via Deep Filter-Tuning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3593</first_page>
						<last_page>3597</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-128</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ng25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yui</given_name>
<surname>Sudo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Fukumoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad</given_name>
<surname>Shakeel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chyi-Jiunn</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>DYNAC: Dynamic Vocabulary-based Non-Autoregressive Contextualization for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2215</first_page>
						<last_page>2219</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-129</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sudo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuetonghui</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiwen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xihong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaobing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Position also matters! Separating Same Instruments in String Quartet using Timbral and Positional Cues</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3105</first_page>
						<last_page>3109</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-130</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xu25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Melanie</given_name>
<surname>Weirich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrian</given_name>
<surname>Simpson</surname>
</person_name>
					</contributors>
					<titles><title>Investigating effects of sex hormones, cycle phases and age on female fundamental frequency</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3349</first_page>
						<last_page>3353</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-139</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/weirich25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bogdan</given_name>
<surname>Vlasenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew Magimai</given_name>
<surname>Doss</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Prosody Modeling: A Use Case for Multilingual Sentence Mode Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5388</first_page>
						<last_page>5392</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-143</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/vlasenko25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sandro</given_name>
<surname>Cumani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Silnova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Barahona</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ladislav</given_name>
<surname>Mošner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan</given_name>
<surname>Rohdin</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of the ABC Classification Backends for NIST SRE24</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3978</first_page>
						<last_page>3982</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-146</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cumani25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sandro</given_name>
<surname>Cumani</surname>
</person_name>
					</contributors>
					<titles><title>A Copula-Based Generative Score-Level Fusion Model for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3723</first_page>
						<last_page>3727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-147</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cumani25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hainan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Bataev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lilit</given_name>
<surname>Grigoryan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>WIND: Accelerated RNN-T Decoding with Windowed Inference for Non-blank Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>639</first_page>
						<last_page>643</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-148</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xu25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ankush</given_name>
<surname>Raut</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Projna</given_name>
<surname>Paromita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sydney</given_name>
<surname>Begerowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suzanne</given_name>
<surname>Bell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Theodora</given_name>
<surname>Chaspari</surname>
</person_name>
					</contributors>
					<titles><title>Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5453</first_page>
						<last_page>5457</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-149</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/raut25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aditya</given_name>
<surname>Kommineni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Digbalay</given_name>
<surname>Bose</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tiantian</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>So Hyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Tager-Flusberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Somer</given_name>
<surname>Bishop</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lord</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sudarsana</given_name>
<surname>Kadiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Can Multimodal Foundation Models Help Analyze Child-Inclusive Autism Diagnostic Videos?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3050</first_page>
						<last_page>3054</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-150</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kommineni25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suhita</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melanie</given_name>
<surname>Jouaiti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan-Ole</given_name>
<surname>Perschewski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Stober</surname>
</person_name>
					</contributors>
					<titles><title>StutterCut: Uncertainty-Guided Normalised Cut for Dysfluency Segmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>808</first_page>
						<last_page>812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-151</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ghosh25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kakeru</given_name>
<surname>Yazawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takayuki</given_name>
<surname>Konishi</surname>
</person_name>
					</contributors>
					<titles><title>A Bayesian Approach to L2 Fluency Ratings by Native and Nonnative Listeners</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>106</first_page>
						<last_page>110</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-152</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yazawa25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yicheng</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaoren</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhizheng</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lauri</given_name>
<surname>Juvela</surname>
</person_name>
					</contributors>
					<titles><title>Neurodyne: Neural Pitch Manipulation with Representation Learning and Cycle-Consistency GAN</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1253</first_page>
						<last_page>1257</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-154</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Miseul</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seyun</given_name>
<surname>Um</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeonjin</given_name>
<surname>Cha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>SpeechMLC: Speech Multi-label Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>514</first_page>
						<last_page>518</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-155</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruofan</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minjie</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jieming</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoda</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minghui</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Vela: Scalable Embeddings with Voice Large Language Models for Multimodal Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2640</first_page>
						<last_page>2644</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-159</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hu25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>William</given_name>
<surname>Ravenscroft</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Close</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kit</given_name>
<surname>Bower-Morris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jamie</given_name>
<surname>Stacey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dmitry</given_name>
<surname>Sityaev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kris Y.</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>Whilter: A Whisper-based Data Filter for &quot;In-the-Wild&quot; Speech Corpora Using Utterance-level Multi-Task Classification </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4288</first_page>
						<last_page>4292</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-160</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ravenscroft25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Debasmita</given_name>
<surname>Bhattacharya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aanya</given_name>
<surname>Tolat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name>
					</contributors>
					<titles><title>From Context to Code-switching: Examining the Interplay of Language Proficiency and Multilingualism in Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4528</first_page>
						<last_page>4532</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-165</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bhattacharya25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wonjune</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junteng</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunyang</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Egor</given_name>
<surname>Lakomkin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yashesh</given_name>
<surname>Gaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leda</given_name>
<surname>Sari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suyoun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jay</given_name>
<surname>Mahadeokar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ozlem</given_name>
<surname>Kalinli</surname>
</person_name>
					</contributors>
					<titles><title>Frozen Large Language Models Can Perceive Paralinguistic Aspects of Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4323</first_page>
						<last_page>4327</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-166</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seongsil</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christi</given_name>
<surname>Miller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Calvin</given_name>
<surname>Murdock</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Proulx</surname>
</person_name>
					</contributors>
					<titles><title>Gaze-Enhanced Multimodal Turn-Taking Prediction in Triadic Conversations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1068</first_page>
						<last_page>1072</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-167</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/heo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chin Yuen</given_name>
<surname>Kwok</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia Qi</given_name>
<surname>Yip</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen</given_name>
<surname>Qiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi Hung</given_name>
<surname>Chi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kwok Yan</given_name>
<surname>Lam</surname>
</person_name>
					</contributors>
					<titles><title>Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2230</first_page>
						<last_page>2234</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-172</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kwok25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chin Yuen</given_name>
<surname>Kwok</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia Qi</given_name>
<surname>Yip</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3888</first_page>
						<last_page>3892</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-173</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kwok25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pierre</given_name>
<surname>Lepagnol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sahar</given_name>
<surname>Ghannay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Gerald</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christophe</given_name>
<surname>Servan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sophie</given_name>
<surname>Rosset</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4108</first_page>
						<last_page>4112</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-175</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lepagnol25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pawel</given_name>
<surname>Pawlowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krystian</given_name>
<surname>Zawistowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wojciech</given_name>
<surname>Lapacz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adam</given_name>
<surname>Wiacek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcin</given_name>
<surname>Skorupa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastien</given_name>
<surname>Postansque</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jakub</given_name>
<surname>Hoscilowicz</surname>
</person_name>
					</contributors>
					<titles><title>TinyClick: Single-Turn Agent for Empowering GUI Automation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3035</first_page>
						<last_page>3039</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-176</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pawlowski25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matteo</given_name>
<surname>Maran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Renske</given_name>
<surname>Rötjes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna R. E.</given_name>
<surname>Schreurs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hans Rutger</given_name>
<surname>Bosker</surname>
</person_name>
					</contributors>
					<titles><title>Beat gestures made by human-like avatars affect speech perception</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5038</first_page>
						<last_page>5042</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-178</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/maran25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziyang</given_name>
<surname>Zhuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Towards Efficiently Whisper Fine-tuning with Monotonic Alignments </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3598</first_page>
						<last_page>3602</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-179</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhuang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chloe</given_name>
<surname>Patman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Foulkes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kirsty</given_name>
<surname>McDougall</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating the suitability of acoustic parameters for capturing breathy voice in non-pathological female speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4198</first_page>
						<last_page>4202</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-180</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/patman25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexandre</given_name>
<surname>Ducorroy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rachid</given_name>
<surname>Riad</surname>
</person_name>
					</contributors>
					<titles><title>Robust fine-tuning of speech recognition models via model merging: application to disordered speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3279</first_page>
						<last_page>3283</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-182</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ducorroy25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Theo</given_name>
<surname>Lepage</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reda</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1098</first_page>
						<last_page>1102</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-183</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lepage25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yingzhi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anas</given_name>
<surname>Alhmoud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad</given_name>
<surname>Alqurishi</surname>
</person_name>
					</contributors>
					<titles><title>Open Universal Arabic ASR Leaderboard</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1168</first_page>
						<last_page>1172</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-184</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yong</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenxing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Le</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duzhen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manjie</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruibo</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Hearing from Silence: Reasoning Audio Descriptions from Silent Videos via Vision-Language Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>156</first_page>
						<last_page>160</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-190</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ren25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chanho</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name>
					</contributors>
					<titles><title>Semi-Supervised Learning for Automatic Speech Recognition with Word Error Rate Estimation and Targeted Domain Data Selection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3663</first_page>
						<last_page>3667</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-191</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/park25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dohyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiwook</given_name>
<surname>Hwang</surname>
</person_name>
					</contributors>
					<titles><title>Fully End-to-end Streaming Open-vocabulary Keyword Spotting with W-CTC Forced Alignment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>519</first_page>
						<last_page>523</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-193</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jixun</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hexin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>EASY: Emotion-aware Speaker Anonymization via Factorized Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3219</first_page>
						<last_page>3223</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-194</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yao25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liang</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lizhong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxing</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weijing</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kwang Pyo</given_name>
<surname>Choi</surname>
</person_name>
					</contributors>
					<titles><title>SPCODEC: Split and Prediction for Neural Speech Codec</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5008</first_page>
						<last_page>5012</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-196</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wen25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fei</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shulin</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Room Impulse Response as a Prompt for Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>763</first_page>
						<last_page>767</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-197</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhao25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingguang</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoqin</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhui</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinkang</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Discrete Audio Representations for Automated Audio Captioning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3125</first_page>
						<last_page>3129</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-199</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tian25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Phoebe</given_name>
<surname>Parsons</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heming Strømholt</given_name>
<surname>Bremnes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Knut</given_name>
<surname>Kvale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Torbjørn</given_name>
<surname>Svendsen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giampiero</given_name>
<surname>Salvi</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Prosodic Information on Dialect Classification Using Whisper Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2785</first_page>
						<last_page>2789</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-200</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/parsons25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yingzhi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anas</given_name>
<surname>Alhmoud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saad</given_name>
<surname>Alsahly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad</given_name>
<surname>Alqurishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name>
					</contributors>
					<titles><title>Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3414</first_page>
						<last_page>3418</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-201</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanni</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuguang</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jixun</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianhao</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongbin</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianjun</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4583</first_page>
						<last_page>4587</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-203</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pan25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nikola</given_name>
<surname>Ljubešić</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Porupski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Rupnik</surname>
</person_name>
					</contributors>
					<titles><title>Identifying Primary Stress Across Related Languages and Dialects with Transformer-based Speech Encoder Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5768</first_page>
						<last_page>5772</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-205</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ljubesic25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chaeyoung</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hojoon</given_name>
<surname>Ki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hoon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junmo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>InfiniteAudio: Infinite-Length Audio Generation with Consistency</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4213</first_page>
						<last_page>4217</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-209</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jung25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Malek</given_name>
<surname>Itani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashton</given_name>
<surname>Graves</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sefik</given_name>
<surname>Emre Eskimez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shyamnath</given_name>
<surname>Gollakota</surname>
</person_name>
					</contributors>
					<titles><title>Neural Speech Extraction with Human Feedback</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4998</first_page>
						<last_page>5002</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-214</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/itani25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yongqi</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Fang</surname>
</person_name>
					</contributors>
					<titles><title>Alzheimer’s Disease Detection Using Co-Attention Mechanism for Acoustic and ASR-Transcribed Text Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5673</first_page>
						<last_page>5677</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-219</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shao25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Erfan</given_name>
<surname>Loweimi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sofia</given_name>
<surname>de la Fuente Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saturnino</given_name>
<surname>Luz</surname>
</person_name>
					</contributors>
					<titles><title>Zero-Shot Speech-Based Depression and Anxiety Assessment with LLMs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>489</first_page>
						<last_page>493</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-235</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/loweimi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Yuen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katherine</given_name>
<surname>Demuth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefanie</given_name>
<surname>Shattuck-Hufnagel</surname>
</person_name>
					</contributors>
					<titles><title>How do both phonological and syntactic complexity influence speech planning?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>340</first_page>
						<last_page>343</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-236</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yuen25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kunlong</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gongping</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xudong</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingdong</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacob</given_name>
<surname>Benesty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoran</given_name>
<surname>Cvetkovic</surname>
</person_name>
					</contributors>
					<titles><title>On the Design of a Robust Superdirective Beamformer and Topology Parameter Optimization with Frustum-Shaped Microphone Arrays Featuring Multiple Rings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3444</first_page>
						<last_page>3448</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-238</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhao25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shenghui</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hukai</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinanglong</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaidi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>A Two-Stage Hierarchical Deep Filtering Framework for Real-Time Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>56</first_page>
						<last_page>60</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-242</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xijie</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Rudzicz</surname>
</person_name>
					</contributors>
					<titles><title>How to Recover Long Audio Sequences Through Gradient Inversion Attack With Dynamic Segment-based Reconstruction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5118</first_page>
						<last_page>5122</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-244</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zeng25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Heng-Jui</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongyu</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changhan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Glass</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-An</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5723</first_page>
						<last_page>5727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-246</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Terumichi</given_name>
<surname>Ariga</surname>
</person_name>
					</contributors>
					<titles><title>Coping with segmental–prosodic incongruity in spoken word recognition in Japanese</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2320</first_page>
						<last_page>2324</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-247</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ariga25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sehyun</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunhee</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhwa</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal and Multitask Learning for Predicting Multiple Scores in L2 English Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2425</first_page>
						<last_page>2429</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-248</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/oh25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sehyun</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhwa</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunhee</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Multilingual Speech Assessment Using Cross-Attention and Multitask Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5073</first_page>
						<last_page>5077</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-249</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/oh25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nabarun</given_name>
<surname>Goswami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Harada</surname>
</person_name>
					</contributors>
					<titles><title>FUSE: Universal Speech Enhancement using Multi‐Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>883</first_page>
						<last_page>887</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-251</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/goswami25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhipeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofen</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyuan</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hangrui</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heng</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangmin</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Long-Context Speech Synthesis with Context-Aware Memory</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2455</first_page>
						<last_page>2459</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-253</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xue</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guiru</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Attention-Based Target Sound Extraction by Fully Leveraging Enrollment in a Shared Latent Space</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4988</first_page>
						<last_page>4992</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-257</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Selina S.</given_name>
<surname>Sung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seunghee</given_name>
<surname>Ha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tae-Jin</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jungmin</given_name>
<surname>So</surname>
</person_name>
					</contributors>
					<titles><title>Multitask Learning with Fused Attention for Improved ASR and Mispronunciation Detection in Children's Speech Sound Disorders</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5698</first_page>
						<last_page>5702</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-259</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sung25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jule Valendo</given_name>
<surname>Halim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ting</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>Token-Level Logits Matter: A Closer Look at Speech Foundation Models for Ambiguous Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5448</first_page>
						<last_page>5452</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-261</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/halim25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abdul</given_name>
<surname>Hannan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad Arslan</given_name>
<surname>Manzoor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shah</given_name>
<surname>Nawaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad Irzam</given_name>
<surname>Liaqat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Schedl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mubashir</given_name>
<surname>Noman</surname>
</person_name>
					</contributors>
					<titles><title>PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for Face-Voice Association</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2710</first_page>
						<last_page>2714</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-268</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hannan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ankita</given_name>
<surname>Ankita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shambhavi</given_name>
<surname>Shambhavi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Syed</given_name>
<surname>Shahnawazuddin</surname>
</person_name>
					</contributors>
					<titles><title>On Enhancing the Performance of Children's ASR Task in Limited Data Scenario</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2830</first_page>
						<last_page>2834</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-273</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ankita25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaixuan</given_name>
<surname>Luan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoda</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shile</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruofan</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minghui</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenrui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jialong</given_name>
<surname>Zuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaqi</given_name>
<surname>Duan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhang</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junyu</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>MelRe: Vision-Based Mel-Spectrogram Restoration</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3823</first_page>
						<last_page>3827</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-274</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/luan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masato</given_name>
<surname>Murata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Miyazaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Koriyama</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-agnostic Emotion Vector for Cross-speaker Emotion Intensity Control</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4383</first_page>
						<last_page>4387</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-276</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/murata25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masato</given_name>
<surname>Murata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Miyazaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Koriyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Eigenvoice Synthesis based on Model Editing for Speaker Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5523</first_page>
						<last_page>5527</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-277</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/murata25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenrui</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuezhen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>DepressGEN: Synthetic Data Generation Framework for Depression Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>464</first_page>
						<last_page>468</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-280</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anne</given_name>
<surname>Hermes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivana</given_name>
<surname>Didirková</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Buech</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gilles</given_name>
<surname>Vannuscorps</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic similarities, articulatory uniqueness: Speech production mechanisms in individuals with congenital lip paralysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3728</first_page>
						<last_page>3732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-281</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hermes25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuying</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fang</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>On Apical Vowels in Eastern Zhenjiang Mandarin</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4758</first_page>
						<last_page>4762</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-282</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Changhong</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fang</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>Tonal Contrasts in the Malipo Variety of the Mienic Language</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>719</first_page>
						<last_page>722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-283</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/du25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhenrui</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fang</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>Tonal Perception in Changde Mandarin</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>714</first_page>
						<last_page>718</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-284</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dzmitry</given_name>
<surname>Saladukha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Koriabkin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kanstantsin</given_name>
<surname>Artsiom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aliaksei</given_name>
<surname>Rak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikita</given_name>
<surname>Ryzhikov</surname>
</person_name>
					</contributors>
					<titles><title>Multichannel Keyword Spotting for Noisy Conditions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2670</first_page>
						<last_page>2674</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-285</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/saladukha25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhe</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Unlearning LLM-Based Speech Recognition Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3214</first_page>
						<last_page>3218</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-287</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cecilia</given_name>
<surname>Bolaños</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Pepino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Meza</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luciana</given_name>
<surname>Ferrer</surname>
</person_name>
					</contributors>
					<titles><title>Benchmarking Time-localized Explanations for Audio Classification Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>211</first_page>
						<last_page>215</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-288</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bolanos25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liang</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maoshen</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonggang</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>Direct-path Relative Harmonic Coefficients Detection for Multi-source Direction-of-Arrival Estimation in Reverberant Environments</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2505</first_page>
						<last_page>2509</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-296</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tao25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liming</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuehan</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianwei</given_name>
<surname>Zhuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxin</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongkang</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>FoleyMaster: High-Quality Video-to-Audio Synthesis via MLLM-Augmented Prompt Tuning and Joint Semantic-Temporal Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4218</first_page>
						<last_page>4222</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-300</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liang25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binzhu</given_name>
<surname>Sha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuo</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fan</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>DAFMSVC: One-Shot Singing Voice Conversion with Dual Attention Mechanism and Flow Matching</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1263</first_page>
						<last_page>1267</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-305</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wieke</given_name>
<surname>Harmsen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roeland</given_name>
<surname>van Hout</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catia</given_name>
<surname>Cucchiarini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>Can ASR generate valid measures of child reading fluency?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2395</first_page>
						<last_page>2399</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-306</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/harmsen25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name>
					</contributors>
					<titles><title>Acquiring Pronunciation from Speech Audio via Multi-task Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2275</first_page>
						<last_page>2279</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-308</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sun25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shunsuke</given_name>
<surname>Kando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Miyao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5728</first_page>
						<last_page>5732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-310</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kando25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiawen</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felipe</given_name>
<surname>Sousa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emir</given_name>
<surname>Demirel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanouil</given_name>
<surname>Benetos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Igor</given_name>
<surname>Gadelha</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Lyrics Transcription on Music Mixtures with Consistency Loss</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3080</first_page>
						<last_page>3084</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-311</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/huang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shengyu</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lipeng</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zuoliang</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Parameter-Efficient Fine-tuning with Instance-Aware Prompt and Parallel Adapters for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1643</first_page>
						<last_page>1647</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-312</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/peng25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Niebuhr</surname>
</person_name>
					</contributors>
					<titles><title>On the cross-modal makeup of charisma: Insights from a field-data analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4548</first_page>
						<last_page>4552</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-313</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/niebuhr25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minyoung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sehwan</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungmin</given_name>
<surname>Cha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul Hongsuck</given_name>
<surname>Seo</surname>
</person_name>
					</contributors>
					<titles><title>Cross-Modal Watermarking for Authentic Audio Recovery and Tamper Localization in Synthesized Audiovisual Forgeries</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5103</first_page>
						<last_page>5107</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-316</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Higuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuji</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsunori</given_name>
<surname>Kobayashi</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Speech Translation Guided by Robust Translation Capability of Large Language Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>21</first_page>
						<last_page>25</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-317</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/higuchi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Verena</given_name>
<surname>Blaschke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miriam</given_name>
<surname>Winkler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Constantin</given_name>
<surname>Förster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriele</given_name>
<surname>Wenger-Glemser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Plank</surname>
</person_name>
					</contributors>
					<titles><title>A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>913</first_page>
						<last_page>917</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-318</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/blaschke25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ye-Xin</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui-Peng</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2465</first_page>
						<last_page>2469</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-319</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lu25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>En-Lun</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chien-Chun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeih-Weih</given_name>
<surname>Hung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shih-Chieh</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berlin</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Flexible VAD-PVAD Transition: A Detachable PVAD Module for Dynamic Encoder RNN VAD</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5793</first_page>
						<last_page>5797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-322</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chun-Yi</given_name>
<surname>Kuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2073</first_page>
						<last_page>2077</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-324</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kuan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xun</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anqi</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wangyou</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huijia</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>BR-ASR: Efficient and Scalable Bias Retrieval Framework for Contextual Biasing ASR in Speech LLM</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4043</first_page>
						<last_page>4047</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-326</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gong25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sashi</given_name>
<surname>Novitasari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Fukuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gakuto</given_name>
<surname>Kurata</surname>
</person_name>
					</contributors>
					<titles><title>Voice Activity-based Text Segmentation for ASR Text Denormalization </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3668</first_page>
						<last_page>3672</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-328</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/novitasari25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>You</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xun</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Wan</surname>
</person_name>
					</contributors>
					<titles><title>M3L: A Multi-Modal and Multi-Lingual Depression Detection Framework</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5268</first_page>
						<last_page>5272</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-329</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/you25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sashi</given_name>
<surname>Novitasari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Fukuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gakuto</given_name>
<surname>Kurata</surname>
</person_name>
					</contributors>
					<titles><title>Improving End-to-end Mixed-case ASR with Knowledge Distillation and Integration of Voice Activity Cues </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3334</first_page>
						<last_page>3338</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-330</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/novitasari25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prabhav</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesus</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name>
					</contributors>
					<titles><title>Count Your Speakers! Multitask Learning for Multimodal Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1588</first_page>
						<last_page>1592</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-334</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/singh25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nina R</given_name>
<surname>Benway</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saba</given_name>
<surname>Tabatabaee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Munson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Preston</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Subtyping Speech Errors in Childhood Speech Sound Disorders with Acoustic-to-Articulatory Speech Inversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2800</first_page>
						<last_page>2804</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-339</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/benway25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicol</given_name>
<surname>Visser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name>
					</contributors>
					<titles><title>Spoken Language Modeling with Duration-Penalized Self-Supervised Units</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1968</first_page>
						<last_page>1972</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-340</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/visser25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titia</given_name>
<surname>Benders</surname>
</person_name>
					</contributors>
					<titles><title>ASR-based segmentation for the analysis of larger child-speech datasets: Performance evaluation on vowels from Australian-English speaking children aged 4 to 11 years</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4273</first_page>
						<last_page>4277</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-342</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cai25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dan</given_name>
<surname>Oneata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leanne</given_name>
<surname>Nortje</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yevgen</given_name>
<surname>Matusevych</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name>
					</contributors>
					<titles><title>The mutual exclusivity bias of bilingual visually grounded speech models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5043</first_page>
						<last_page>5047</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-343</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/oneata25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masato</given_name>
<surname>Takagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miku</given_name>
<surname>Nishihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukiya</given_name>
<surname>Hono</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kei</given_name>
<surname>Hashimoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshihiko</given_name>
<surname>Nankaku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keiichi</given_name>
<surname>Tokuda</surname>
</person_name>
					</contributors>
					<titles><title>PeriodCodec: A Pitch-Controllable Neural Audio Codec Using Periodic Signals for Singing Voice Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4913</first_page>
						<last_page>4917</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-347</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/takagi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaolong</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaonan</given_name>
<surname>Zhi</surname>
</person_name>
					</contributors>
					<titles><title>MDDM: A Multi-view Discriminative Enhanced Diffusion-based Model for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4078</first_page>
						<last_page>4082</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-350</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xu25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sayaka</given_name>
<surname>Shiota</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suzuka</given_name>
<surname>Horie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kouta</given_name>
<surname>Kanno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name>
					</contributors>
					<titles><title>J-SPAW: Japanese speaker verification and spoofing attacks recorded in-the-wild dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3913</first_page>
						<last_page>3917</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-352</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shiota25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoning</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoqing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youjun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huimeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guinan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengxi</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1983</first_page>
						<last_page>1987</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-353</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xu25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei-Cheng</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Harwath</surname>
</person_name>
					</contributors>
					<titles><title>Probing the Robustness Properties of Neural Speech Codecs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5013</first_page>
						<last_page>5017</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-355</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tseng25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaojie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qintuya</given_name>
<surname>Si</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>De</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>Temporal Convolutional Network with Smoothed and Weighted Losses for Distant Voice Activity and Overlapped Speech Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>504</first_page>
						<last_page>508</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-357</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yue</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liwei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changxin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingyao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yili</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanyue</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Chu</surname>
</person_name>
					</contributors>
					<titles><title>A Chinese Heart Failure Status Speech Database with Universal and Personalised Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2008</first_page>
						<last_page>2012</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-358</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pan25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thanapat</given_name>
<surname>Trachu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thanathai</given_name>
<surname>Lertpetchpun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekapol</given_name>
<surname>Chuangsuwanich</surname>
</person_name>
					</contributors>
					<titles><title>Amplifying Artifacts with Speech Enhancement in Voice Anti-spoofing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5638</first_page>
						<last_page>5642</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-362</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/trachu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ying</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihua</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Federated Learning with Feature Space Separation for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1518</first_page>
						<last_page>1522</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-364</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/meng25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seung-jae</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul Hongsuck</given_name>
<surname>Seo</surname>
</person_name>
					</contributors>
					<titles><title>Bridging Audio and Vision: Zero-Shot Audiovisual Segmentation by Connecting Pretrained Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3145</first_page>
						<last_page>3149</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-366</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lee25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soumya</given_name>
<surname>Dutta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Smruthi</given_name>
<surname>Balaji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Varada</given_name>
<surname>R</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viveka</given_name>
<surname>Salinamakki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>ABHINAYA - A System for Speech Emotion Recognition In Naturalistic Conditions Challenge </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4663</first_page>
						<last_page>4667</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-368</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dutta25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nurali</given_name>
<surname>Alip</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianrui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingru</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>A Three-Stage Beamforming with Harmonic Guidance for Multi-Channel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1193</first_page>
						<last_page>1197</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-369</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/alip25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuheng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenjie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diqun</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>CBA: Backdoor Attack on Deep Speech Classification via Audio Compression</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5648</first_page>
						<last_page>5652</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-372</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/huang25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yang</given_name>
<surname>Xiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Canan</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Desheng</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingguang</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhui</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>A Semantic Information-based Hierarchical Speech Enhancement Method Using Factorized Codec and Diffusion Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4063</first_page>
						<last_page>4067</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-374</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xiang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jangyeon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ui-Hyeop</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaehyun</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyung-Min</given_name>
<surname>Park</surname>
</person_name>
					</contributors>
					<titles><title>Stack Less, Repeat More: A Block Reusing Approach for Progressive Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5158</first_page>
						<last_page>5162</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-376</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youngmoon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong-Hyeok</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myunghun</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaeyoung</given_name>
<surname>Roh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang Woo</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoon-Young</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2645</first_page>
						<last_page>2649</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-378</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jung25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kirandevraj</given_name>
<surname>R</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinod K</given_name>
<surname>Kurmi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinay</given_name>
<surname>Namboodiri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>CV</given_name>
<surname>Jawahar</surname>
</person_name>
					</contributors>
					<titles><title>Multilingual Query-by-Example KWS for Indian Languages using Transliteration</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>903</first_page>
						<last_page>907</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-380</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/r25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Koji</given_name>
<surname>Okabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hitoshi</given_name>
<surname>Yamamoto</surname>
</person_name>
					</contributors>
					<titles><title>Simultaneous Masked and Unmasked Decoding with Speculative Decoding Masking for Fast ASR without Accuracy Loss</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>634</first_page>
						<last_page>638</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-382</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/okabe25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hitoshi</given_name>
<surname>Suda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoru</given_name>
<surname>Fukayama</surname>
</person_name>
					</contributors>
					<titles><title>Voice Conversion for Likability Control via Automated Rating of Speech Synthesis Corpora</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1363</first_page>
						<last_page>1367</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-383</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/suda25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zixuan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shulin</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinglin</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>TF-SkiMNet: Speech Enhancement Based on Inplace Modeling and Skipping Memory in Time-Frequency Domain</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5143</first_page>
						<last_page>5147</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-391</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xihao</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanting</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>SaD: A Scenario-Aware Discriminator for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4838</first_page>
						<last_page>4842</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-392</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yuan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyebin</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kangwook</given_name>
<surname>Jang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoirin</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3419</first_page>
						<last_page>3423</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-397</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ahn25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jianheng</given_name>
<surname>Zhuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiwen</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1163</first_page>
						<last_page>1167</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-398</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhuo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sébastien</given_name>
<surname>Le Maguer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gwénolé</given_name>
<surname>Lecorvé</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Damien</given_name>
<surname>Lolive</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naomi</given_name>
<surname>Harte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juraj</given_name>
<surname>Šimko</surname>
</person_name>
					</contributors>
					<titles><title>Enabling the replicability of speech synthesis perceptual evaluations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2545</first_page>
						<last_page>2549</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-401</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lemaguer25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jia-Jyu</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yen-Ting</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu-Hao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao-Kai</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan-Zhi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen-Yu</given_name>
<surname>Chiang</surname>
</person_name>
					</contributors>
					<titles><title>Lightweight Speech Enhancement for Mandarin Esophageal Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4613</first_page>
						<last_page>4617</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-404</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/su25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alon</given_name>
<surname>Levkovitch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Salazar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soroosh</given_name>
<surname>Mariooryad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>RJ</given_name>
<surname>Skerry-Ryan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nadav</given_name>
<surname>Bar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bastiaan</given_name>
<surname>Kleijn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliya</given_name>
<surname>Nachmani</surname>
</person_name>
					</contributors>
					<titles><title>Zero-Shot Mono-to-Binaural Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4168</first_page>
						<last_page>4172</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-406</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/levkovitch25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shoki</given_name>
<surname>Kawanishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akinori</given_name>
<surname>Ito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuya</given_name>
<surname>Chiba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Nose</surname>
</person_name>
					</contributors>
					<titles><title>Improving User Impression of Spoken Dialogue Systems by Controlling Para-linguistic Expression Based on Intimacy</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3040</first_page>
						<last_page>3044</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-408</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kawanishi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chi-Yuan</given_name>
<surname>Hsiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke-Han</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai-Wei</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chih-Kai</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Chih</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3234</first_page>
						<last_page>3238</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-409</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hsiao25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mehedi Hasan</given_name>
<surname>Bijoy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dejan</given_name>
<surname>Porjazovski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamás</given_name>
<surname>Grósz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>146</first_page>
						<last_page>150</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-418</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bijoy25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>Amartyaveer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Jointly Improving Dialect Identification and ASR in Indian Languages using Multimodal Feature Fusion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2770</first_page>
						<last_page>2774</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-421</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kumar25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seung-bin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun-seo</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jungwoo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chan-yeong</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyo-Won</given_name>
<surname>Koo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jisoo</given_name>
<surname>Son</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanghyun</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Souhwan</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha-Jin</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Audio Deepfake Detection by Improving Representation Similarity of Bonafide Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2250</first_page>
						<last_page>2254</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-422</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarthak</given_name>
<surname>Yadav</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sergios</given_name>
<surname>Theodoridis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng-Hua</given_name>
<surname>Tan</surname>
</person_name>
					</contributors>
					<titles><title>AxLSTMs: learning self-supervised audio representations with xLSTMs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3524</first_page>
						<last_page>3528</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-426</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yadav25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hangting</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingshan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingcong</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>TSDT-Net: Ultra-Low-Complexity Two-Stage Model Combining Dual-Path-Transformer and Transform-Average-Concatenate Network for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>71</first_page>
						<last_page>75</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-429</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gao25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sung-Feng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heng-Cheng</given_name>
<surname>Kuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhehuai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuesong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pin-Jui</given_name>
<surname>Ku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ante</given_name>
<surname>Jukic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huck</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Chiang Frank</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szu-Wei</given_name>
<surname>Fu</surname>
</person_name>
					</contributors>
					<titles><title>VoiceNoNG: Robust High-Quality Speech Editing Model without Hallucinations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3469</first_page>
						<last_page>3473</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-431</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/huang25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Stucki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Deriu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Cieliebak</surname>
</person_name>
					</contributors>
					<titles><title>Voice Adaptation for Swiss German</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4143</first_page>
						<last_page>4147</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-432</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/stucki25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minsu</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seolhee</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Choonghyeon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Namhyun</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>When Humans Growl and Birds Speak: High-Fidelity Voice Conversion from Human to Animal and Designed Sounds</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4153</first_page>
						<last_page>4157</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-433</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kang25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Biel</given_name>
<surname>Tura-Vecino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Subhadeep</given_name>
<surname>Maji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aravind</given_name>
<surname>Varier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonio</given_name>
<surname>Bonafonte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Valles</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Owen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Constantinos</given_name>
<surname>Papayiannis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leif</given_name>
<surname>Radel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grant</given_name>
<surname>Strimel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oluwaseyi</given_name>
<surname>Feyisetan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roberto</given_name>
<surname>Barra-Chicote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ariya</given_name>
<surname>Rastrow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Volker</given_name>
<surname>Leutnant</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trevor</given_name>
<surname>Wood</surname>
</person_name>
					</contributors>
					<titles><title>Universal Semantic Disentangled Privacy-preserving Speech Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3633</first_page>
						<last_page>3637</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-437</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/turavecino25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Herman</given_name>
<surname>Kamper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>van Niekerk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Zaïdi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc-André</given_name>
<surname>Carbonneau</surname>
</person_name>
					</contributors>
					<titles><title>LinearVC: Linear Transformations of Self-Supervised Features Through the Lens of Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1398</first_page>
						<last_page>1402</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-438</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kamper25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andres</given_name>
<surname>Fernandez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Azcarreta</given_name>
<surname>Ortiz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Çağdaş</given_name>
<surname>Bilen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesus</given_name>
<surname>Monge Alvarez</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Neural and Numerical Methods for High-QualityOnline Speech Spectrogram Inversion via Gradient Theorem</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3449</first_page>
						<last_page>3453</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-439</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fernandez25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Masztalski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michał</given_name>
<surname>Romaniuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jakub</given_name>
<surname>Żak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mateusz</given_name>
<surname>Matuszewski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konrad</given_name>
<surname>Kowalczyk</surname>
</person_name>
					</contributors>
					<titles><title>Clustering-based Hard Negative Sampling for Supervised Contrastive Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3698</first_page>
						<last_page>3702</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-442</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/masztalski25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Astik</given_name>
<surname>Biswas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Shevelev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amine</given_name>
<surname>Abdaoui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vivek</given_name>
<surname>Tyagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abdelmoumene</given_name>
<surname>Boumadane</surname>
</person_name>
					</contributors>
					<titles><title>Adapting Whisper for low-resource Hindi-English Code-Mix speech with on-the-fly Augmentation &amp; LLM-Synthesised Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4293</first_page>
						<last_page>4297</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-447</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/biswas25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junqi</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiping</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cedar</given_name>
<surname>Lin</surname>
</person_name>
					</contributors>
					<titles><title>Band-SCNet: A Causal, Lightweight Model for High-Performance Real-Time Music Source Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4973</first_page>
						<last_page>4977</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-448</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Makishima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naotaka</given_name>
<surname>Kawata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taiga</given_name>
<surname>Yamane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Orihashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name>
					</contributors>
					<titles><title>SOMSRED-SVC: Sequential Output Modeling with Speaker Vector Constraints for Joint Multi-Talker Overlapped ASR and Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3893</first_page>
						<last_page>3897</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-449</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/makishima25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Makishima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naotaka</given_name>
<surname>Kawata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taiga</given_name>
<surname>Yamane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mana</given_name>
<surname>Ihori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoshi</given_name>
<surname>Suzuki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Orihashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name>
					</contributors>
					<titles><title>Unified Audio-Visual Modeling for Recognizing Which Face Spoke When and What in Multi-Talker Overlapped Speech and Video</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1838</first_page>
						<last_page>1842</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-451</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/makishima25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lucía</given_name>
<surname>Ormaechea</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikos</given_name>
<surname>Tsourakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierrette</given_name>
<surname>Bouillon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Lecouteux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Didier</given_name>
<surname>Schwab</surname>
</person_name>
					</contributors>
					<titles><title>Towards High-Quality LLM-Based Data for French Spontaneous Speech Simplification: an Exo-Refinement Approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4038</first_page>
						<last_page>4042</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-452</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ormaechea25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyungchan</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chanwoo</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoodong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stanley Jungkyu</given_name>
<surname>Choi</surname>
</person_name>
					</contributors>
					<titles><title>APTTS: Adversarial Post-training in Latent Flow Matching for Fast and High-fidelity Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5518</first_page>
						<last_page>5522</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-455</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yoon25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yanir</given_name>
<surname>Marmor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yair</given_name>
<surname>Lifshitz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoad</given_name>
<surname>Snapir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kinneret</given_name>
<surname>Misgav</surname>
</person_name>
					</contributors>
					<titles><title>Building an Accurate Open-Source Hebrew ASR System through Crowdsourcing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>723</first_page>
						<last_page>727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-460</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/marmor25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Komei</given_name>
<surname>Hiruta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Yamano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hideaki</given_name>
<surname>Tamori</surname>
</person_name>
					</contributors>
					<titles><title>Hybrid Data Sampling for ASR: Integrating Acoustic Diversity and Transcription Uncertainty</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4283</first_page>
						<last_page>4287</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-462</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hiruta25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nik</given_name>
<surname>Vaessen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roeland</given_name>
<surname>Ordelman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David A.</given_name>
<surname>van Leeuwen</surname>
</person_name>
					</contributors>
					<titles><title>Self-supervised learning of speech representations with Dutch archival data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1208</first_page>
						<last_page>1212</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-463</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/vaessen25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junchuan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xintong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via In-Context Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4893</first_page>
						<last_page>4897</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-464</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhao25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dominika C</given_name>
<surname>Woszczyk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ranya</given_name>
<surname>Aloufi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soteris</given_name>
<surname>Demetriou</surname>
</person_name>
					</contributors>
					<titles><title>ClaritySpeech: Dementia Obfuscation in Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1433</first_page>
						<last_page>1437</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-465</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/woszczyk25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ho-Hsiang</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Cheng</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abinaya</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luca</given_name>
<surname>Bondi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shabnam</given_name>
<surname>Ghaffarzadegan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Pablo</given_name>
<surname>Bello</surname>
</person_name>
					</contributors>
					<titles><title>Towards Few-Shot Training-Free Anomaly Sound Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3384</first_page>
						<last_page>3388</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-467</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wu25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiaqi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaolong</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhekai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shixi</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuancheng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaoren</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhenpeng</given_name>
<surname>Zhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhizheng</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4883</first_page>
						<last_page>4887</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-468</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fatima</given_name>
<surname>Naseem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maham</given_name>
<surname>Sajid</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Farah</given_name>
<surname>Adeeba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sahar</given_name>
<surname>Rauf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Asad</given_name>
<surname>Mustafa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarmad</given_name>
<surname>Hussain</surname>
</person_name>
					</contributors>
					<titles><title>Developing High-Quality TTS for Punjabi and Urdu: Benchmarking against MMS Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1628</first_page>
						<last_page>1632</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-469</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/naseem25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Adriana</given_name>
<surname>Stan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Combei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Oneata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Horia</given_name>
<surname>Cucu</surname>
</person_name>
					</contributors>
					<titles><title>TADA: Training-free Attribution and Out-of-Domain Detection of Audio Deepfakes</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1543</first_page>
						<last_page>1547</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-472</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/stan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lipeng</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengyu</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wu</given_name>
<surname>Guo</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Multi-Level Features of ATST with Conformer-Based Dual-Branch Network for Sound Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2595</first_page>
						<last_page>2599</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-474</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dai25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hang</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxiang</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lichun</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Luan</surname>
</person_name>
					</contributors>
					<titles><title>Text-Enhanced Audio Encoder for Large Language Model based Speech Recognition via Cross-Modality Pre-training with Unpaired Audio-Text Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>569</first_page>
						<last_page>573</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-476</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/su25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jean-Luc</given_name>
<surname>Rouas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charles</given_name>
<surname>Brazier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leila</given_name>
<surname>Ben Letaifa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rafael</given_name>
<surname>Medina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro</given_name>
<surname>Palacios</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Atienza</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giovanni</given_name>
<surname>Ansaloni</surname>
</person_name>
					</contributors>
					<titles><title>Structured pruning for efficient systolic array accelerated cascade Speech-to-Text Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>893</first_page>
						<last_page>897</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-478</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/rouas25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kemal</given_name>
<surname>Altwlkany</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amar</given_name>
<surname>Kuric</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emanuel</given_name>
<surname>Lacic</surname>
</person_name>
					</contributors>
					<titles><title>On the Language and Gender Biases in PSTN, VoIP and Neural Audio Codecs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1348</first_page>
						<last_page>1352</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-481</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/altwlkany25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianyi</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Evaluation of a model for sound radiation from the vocal tract wall</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>359</first_page>
						<last_page>363</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-482</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/birkholz25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chandra Mohan</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnab Kumar</given_name>
<surname>Roy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anupam</given_name>
<surname>Mandal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanna Kumar</given_name>
<surname>Kr</surname>
</person_name>
					</contributors>
					<titles><title>Boosting StoRM Convergence with Metric Guidance and Non-uniform State-Sampling for Optimal Dereverberation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3813</first_page>
						<last_page>3817</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-483</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sharma25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiangyu</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Federico</given_name>
<surname>Landini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan</given_name>
<surname>Rohdin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Silnova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mireia</given_name>
<surname>Diez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name>
					</contributors>
					<titles><title>Fine-tune Before Structured Pruning: Towards Compact and Accurate Self-Supervised Models for Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1583</first_page>
						<last_page>1587</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-484</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/han25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyue</given_name>
<surname>Qiu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Pu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinpeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuchu</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>PPGs-BERT: Leveraging Phoneme Sequence and BERT for Alzheimer’s Disease Detection from Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>554</first_page>
						<last_page>558</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-489</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sun25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>James</given_name>
<surname>Tanner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Morgan</given_name>
<surname>Sonderegger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jane</given_name>
<surname>Stuart-Smith</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeff</given_name>
<surname>Mielke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tyler</given_name>
<surname>Kendall</surname>
</person_name>
					</contributors>
					<titles><title>Automatic classification of stop realisation with wav2vec2.0</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2270</first_page>
						<last_page>2274</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-491</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tanner25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lauren</given_name>
<surname>Harrington</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Hughes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Harrison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Foulkes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jessica</given_name>
<surname>Wormald</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Finnian</given_name>
<surname>Kelly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>van der Vloed</surname>
</person_name>
					</contributors>
					<titles><title>Variability in performance across four generations of automatic speaker recognition systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3993</first_page>
						<last_page>3997</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-494</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/harrington25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Natacha</given_name>
<surname>Miniconi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meysam</given_name>
<surname>Shamsi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anthony</given_name>
<surname>Larcher</surname>
</person_name>
					</contributors>
					<titles><title>When The MOS Predictor Asks For Training Annotation In Cross Lingual/Domain Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2550</first_page>
						<last_page>2554</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-496</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/miniconi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jan</given_name>
<surname>Schuster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Wölfel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fabian</given_name>
<surname>Brunner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Bergler</surname>
</person_name>
					</contributors>
					<titles><title>PredTrAD – Prediction-based Transformer for Anomaly Detection in Multivariate Time Series Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3873</first_page>
						<last_page>3877</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-501</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/schuster25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Silke</given_name>
<surname>Hamann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Alićehajić</surname>
</person_name>
					</contributors>
					<titles><title>Are loan sequences different from foreign sequences? A perception study with Japanese listeners on coronal obstruent – high front vowel sequences</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>111</first_page>
						<last_page>115</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-502</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hamann25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianteng</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bei</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Ultra-Low Bit Post-Training Quantization of Large Speech Models via K-Means Clustering and Mixed Precision Allocation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1988</first_page>
						<last_page>1992</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-503</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gu25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xulin</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jialu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nancy L.</given_name>
<surname>McElwain</surname>
</person_name>
					</contributors>
					<titles><title>Band-Split Self-supervised Mamba for Infant-centered Audio Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2795</first_page>
						<last_page>2799</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-504</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Pepino</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Riera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luciana</given_name>
<surname>Ferrer</surname>
</person_name>
					</contributors>
					<titles><title>EnCodecMAE: leveraging neural codecs for universal audio representation learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3519</first_page>
						<last_page>3523</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-506</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pepino25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoran</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingchen</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brendan</given_name>
<surname>Fahy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiaochu</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binbin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhendong</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anshul</given_name>
<surname>Wadhawan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Denglin</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Apurv</given_name>
<surname>Verma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinay</given_name>
<surname>Ramesh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srivas</given_name>
<surname>Prasad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michele M.</given_name>
<surname>Franceschini</surname>
</person_name>
					</contributors>
					<titles><title>Adapting Whisper for Streaming Speech Recognition via Two-Pass Decoding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4428</first_page>
						<last_page>4432</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-511</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhou25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chin-Jou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunjung</given_name>
<surname>Yeo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kwanghee</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paula Andrea</given_name>
<surname>Pérez-Toro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masao</given_name>
<surname>Someki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan Kumar</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengjun</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David R.</given_name>
<surname>Mortensen</surname>
</person_name>
					</contributors>
					<titles><title>Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2128</first_page>
						<last_page>2132</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-512</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Robin</given_name>
<surname>Huo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewan</given_name>
<surname>Dunbar</surname>
</person_name>
					</contributors>
					<titles><title>Iterative Refinement, Not Training Objective, Makes HuBERT Behave Differently from wav2vec 2.0</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>261</first_page>
						<last_page>265</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-514</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/huo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Magdalena</given_name>
<surname>Gołębiowska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Syga</surname>
</person_name>
					</contributors>
					<titles><title>EmoSpeechAuth: Emotion-Aware Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5743</first_page>
						<last_page>5747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-515</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/goebiowska25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fredrik</given_name>
<surname>Cumlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyu</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Victor</given_name>
<surname>Ungureanu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandan K.A.</given_name>
<surname>Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Schüldt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saikat</given_name>
<surname>Chatterjee</surname>
</person_name>
					</contributors>
					<titles><title>Multivariate Probabilistic Assessment of Speech Quality</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5413</first_page>
						<last_page>5417</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-518</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cumlin25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Escobar-Grisales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristian David</given_name>
<surname>Ríos-Urrego</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adolfo M.</given_name>
<surname>Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yamile</given_name>
<surname>Bocanegra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Moreno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Rafael</given_name>
<surname>Orozco-Arroyave</surname>
</person_name>
					</contributors>
					<titles><title>Synchronous analysis of abnormal acoustic and linguistic production in Parkinson's speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5283</first_page>
						<last_page>5287</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-521</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/escobargrisales25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kunxiao</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Favaro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano Moro</given_name>
<surname>Velazquez</surname>
</person_name>
					</contributors>
					<titles><title>ADCeleb: A Longitudinal Speech Dataset from Public Figures for Early Detection of Alzheimer’s Disease</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5688</first_page>
						<last_page>5692</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-523</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gao25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paul M.</given_name>
<surname>Reuter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Jessen</surname>
</person_name>
					</contributors>
					<titles><title>On the influence of language similarity in non-target speaker verification trials</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3998</first_page>
						<last_page>4002</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-526</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/reuter25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yassine</given_name>
<surname>El Kheir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Polzehl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Möller</surname>
</person_name>
					</contributors>
					<titles><title>BiCrossMamba-ST: Speech Deepfake Detection with Bidirectional Mamba Spectro-Temporal Cross-Attention</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2235</first_page>
						<last_page>2239</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-527</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/elkheir25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amro</given_name>
<surname>Asali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yehuda</given_name>
<surname>Ben-Shimol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Itshak</given_name>
<surname>Lapidot</surname>
</person_name>
					</contributors>
					<titles><title>ATMM-SAGA: Alternating Training for Multi-Module with Score-Aware Gated Attention SASV system</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3708</first_page>
						<last_page>3712</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-529</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/asali25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Varsha</given_name>
<surname>Pendyala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro</given_name>
<surname>Morgado</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Sethares</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4503</first_page>
						<last_page>4507</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-530</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pendyala25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Subrata</given_name>
<surname>Biswas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammad Nur Hossain</given_name>
<surname>Khan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bashima</given_name>
<surname>Islam</surname>
</person_name>
					</contributors>
					<titles><title>QUADS: Quantized Distillation Framework for Efficient Speech Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4098</first_page>
						<last_page>4102</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-532</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/biswas25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ju</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiteng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frank</given_name>
<surname>Seide</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name>
					</contributors>
					<titles><title>Directional Speech Recognition with Full-Duplex Capability</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2570</first_page>
						<last_page>2574</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-535</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lin25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suhas</given_name>
<surname>BN</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Han-Chin</given_name>
<surname>Shing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitch</given_name>
<surname>Strong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Burnsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jessica</given_name>
<surname>Ofor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jordan R.</given_name>
<surname>Mason</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Susan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sundararajan</given_name>
<surname>Srinivasan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaitanya</given_name>
<surname>Shivade</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jack</given_name>
<surname>Moriarty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph Paul</given_name>
<surname>Cohen</surname>
</person_name>
					</contributors>
					<titles><title>Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3070</first_page>
						<last_page>3074</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-537</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bn25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thien-Phuc</given_name>
<surname>Doan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kihun</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Souhwan</given_name>
<surname>Jung</surname>
</person_name>
					</contributors>
					<titles><title>VIB-based Real Pre-emphasis Audio Deepfake Source Tracing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1568</first_page>
						<last_page>1572</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-538</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/doan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tünde</given_name>
<surname>Szalay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mostafa</given_name>
<surname>Shahin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tharmakulasingam</given_name>
<surname>Sirojan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Nan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Renata</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kirrie</given_name>
<surname>Ballard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beena</given_name>
<surname>Ahmed</surname>
</person_name>
					</contributors>
					<titles><title>AusKidTalk: Using Strategic Data Collection and Out-of-Domain Tools to Semi-Automate Novel Corpora Annotation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4268</first_page>
						<last_page>4272</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-539</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/szalay25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nithin</given_name>
<surname>Rao Koluguri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monica</given_name>
<surname>Sekoyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Zelenfroynd</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sasha</given_name>
<surname>Meister</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuoyang</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sofia</given_name>
<surname>Kostandian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>He</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolay</given_name>
<surname>Karpov#</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagadeesh</given_name>
<surname>Balam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vitaly</given_name>
<surname>Lavrukhin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Papi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Gaido</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessio</given_name>
<surname>Brutti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Granary: Speech Recognition and Translation Dataset in 25 European Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3923</first_page>
						<last_page>3927</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-540</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/raokoluguri25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zixiang</given_name>
<surname>Wan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guochang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifeng</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianqiang</given_name>
<surname>Wei</surname>
</person_name>
					</contributors>
					<titles><title>SpecTokenizer: A Lightweight Streaming Codec in the Compressed Spectrum Domain</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>599</first_page>
						<last_page>603</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-546</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoqin</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingguang</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaming</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiabei</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiwan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangyu</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Desheng</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinkang</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhui</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Qin</surname>
</person_name>
					</contributors>
					<titles><title>RA-CLAP: Relation-Augmented Emotional Speaking Style Contrastive Language-Audio Pretraining For Speech Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2995</first_page>
						<last_page>2999</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-548</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sun25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuxiang</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fan</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liyong</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heinrich</given_name>
<surname>Dinkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lichun</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junbo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Luan</surname>
</person_name>
					</contributors>
					<titles><title>GLCLAP: A Novel Contrastive Learning Pre-trained Model for Contextual Biasing in ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5173</first_page>
						<last_page>5177</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-550</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kong25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yike</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiming</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qinghua</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songjun</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Monotonic Attention for Robust Text-to-Speech Synthesis in Large Language Model Frameworks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2460</first_page>
						<last_page>2464</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-551</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junbo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heinrich</given_name>
<surname>Dinkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yadong</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenyu</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Si</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anbei</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Luan</surname>
</person_name>
					</contributors>
					<titles><title>X-ARES: A Comprehensive Framework for Assessing Audio Encoder Performance</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4868</first_page>
						<last_page>4872</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-552</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyun Joon</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeongmin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jin Sob</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeong Yeol</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung Won</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunwoo</given_name>
<surname>Song</surname>
</person_name>
					</contributors>
					<titles><title>RapFlow-TTS: Rapid and High-Fidelity Text-to-Speech with Improved Consistency Flow Matching</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2440</first_page>
						<last_page>2444</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-554</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/park25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashwin</given_name>
<surname>Ram</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marisol</given_name>
<surname>Muñoz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoi</given_name>
<surname>Gkalitsiou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros G.</given_name>
<surname>Dimakis</surname>
</person_name>
					</contributors>
					<titles><title>Bilingual Speakers Exhibit Cognitive Fatigue: A Speech Disfluencies Case Study on Research Talks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3808</first_page>
						<last_page>3812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-555</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ram25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pu</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatian</given_name>
<surname>Xi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4803</first_page>
						<last_page>4807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-559</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuanbo</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofen</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueru</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weibin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangmin</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>MMLoRA: Multitask Memory Parameter-Efficient Fine-Tuning for Multimodal SER</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4948</first_page>
						<last_page>4952</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-560</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hoyeon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sejung</given_name>
<surname>Son</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye-Eun</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong-Hwan</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Synthetic Data Generation for Phrase Break Prediction with Large Language Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>454</first_page>
						<last_page>458</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-564</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lee25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jihwan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kleanthis</given_name>
<surname>Avramidis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Pistrosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monica</given_name>
<surname>Gonzalez-Machorro</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoonjeong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis</given_name>
<surname>Goldstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory Feature Prediction from Surface EMG during Speech Production</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>320</first_page>
						<last_page>324</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-565</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lee25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiuwen</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bornali</given_name>
<surname>Phukon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonghwan</given_name>
<surname>Na</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ed</given_name>
<surname>Cutrell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyu J.</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pan-Pan</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aadhrik</given_name>
<surname>Kuila</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colin</given_name>
<surname>Lea</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bob</given_name>
<surname>MacDonald</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gautam</given_name>
<surname>Mantena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Venkatesh</given_name>
<surname>Ravichandran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leda</given_name>
<surname>Sari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Tomanek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang D.</given_name>
<surname>Yoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chris</given_name>
<surname>Zwilling</surname>
</person_name>
					</contributors>
					<titles><title>The Interspeech 2025 Speech Accessibility Project Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3269</first_page>
						<last_page>3273</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-566</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zheng25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yan</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visar</given_name>
<surname>Berisha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Liss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaitali</given_name>
<surname>Chakrabarti</surname>
</person_name>
					</contributors>
					<titles><title>Mitigating Overfitting During Speech Foundation Model Fine-tuning: Applications to Dysarthric Speech Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2133</first_page>
						<last_page>2137</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-567</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xiong25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenyang</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinfeng</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huiyan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manhong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yutao</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingyang</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>908</first_page>
						<last_page>912</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-568</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/le25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenhao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangjun</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haojie</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianrui</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yilu</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiharu</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiang</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the Power of Empirical Mode Decomposition for Sensing the Sound of Silence: A Pilot Study on Mice Autism Detection via Ultrasonic Vocalisation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1708</first_page>
						<last_page>1712</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-571</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wu25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Setoguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiko</given_name>
<surname>Arimoto</surname>
</person_name>
					</contributors>
					<titles><title>Assessment of the synthetic quality and controllability of laughing onset in speech-laugh synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2555</first_page>
						<last_page>2559</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-572</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/setoguchi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haiyun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofeng</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingran</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaoxun</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanyang</given_name>
<surname>Peng</surname>
</person_name>
					</contributors>
					<titles><title>VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5108</first_page>
						<last_page>5112</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-575</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jihyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Doyeon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyewon</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyoung</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonguk</given_name>
<surname>Yoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang Woo</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeongook</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoon-Young</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>Quadruple Path Modeling with Latent Feature Transfer for Permutation-free Continuous Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1438</first_page>
						<last_page>1442</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-576</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Onda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Imoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoru</given_name>
<surname>Fukayama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name>
					</contributors>
					<titles><title>Discrete Tokens Exhibit Interlanguage Speech Intelligibility Benefit: an Analytical Study Towards Accent-robust ASR Only with Native Speech Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>221</first_page>
						<last_page>225</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-577</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/onda25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jihyun</given_name>
<surname>Mun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhwa</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunhee</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Speech-Based Automatic Chronic Kidney Disease Diagnosis via Transformer Fusion of Glottal and Spectrogram Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5303</first_page>
						<last_page>5307</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-587</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mun25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yunqi C.</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dhruv</given_name>
<surname>Jagmohan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong Kit</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>C. T. Justine</given_name>
<surname>Hui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Hioka</surname>
</person_name>
					</contributors>
					<titles><title>Effect of Noise Floor in Room Impulse Response on Speech Perception Under Spherical Harmonics-based Spatial Sound Reproduction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>928</first_page>
						<last_page>932</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-588</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Onda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Imoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satoru</given_name>
<surname>Fukayama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name>
					</contributors>
					<titles><title>Prosodically Enhanced Foreign Accent Simulation by Discrete Token-based Resynthesis Only with Native Speech Corpora</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2195</first_page>
						<last_page>2199</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-590</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/onda25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaejun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyogu</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Vo-Ve: An Explainable Voice-Vector for Speaker Identity Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3988</first_page>
						<last_page>3992</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-591</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lee25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kentaro</given_name>
<surname>Onda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Kashiwagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emiru</given_name>
<surname>Tsunoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hayato</given_name>
<surname>Futami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Differentiable K-means for Fully-optimized Discrete Token-based ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1223</first_page>
						<last_page>1227</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-593</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/onda25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengjie</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rao</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefano</given_name>
<surname>Bannò</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kate M.</given_name>
<surname>Knill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark J.F.</given_name>
<surname>Gales</surname>
</person_name>
					</contributors>
					<titles><title>Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5083</first_page>
						<last_page>5087</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-594</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/qian25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kiyotada</given_name>
<surname>Mori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seiya</given_name>
<surname>Kawano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaoran</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos Toshinori</given_name>
<surname>Ishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angel García</given_name>
<surname>Contreras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichiro</given_name>
<surname>Yoshino</surname>
</person_name>
					</contributors>
					<titles><title>What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1753</first_page>
						<last_page>1757</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-595</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mori25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yejin</given_name>
<surname>Jeon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Solee</given_name>
<surname>Im</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youngjae</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gary Geunbae</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Facilitating Personalized TTS for Dysarthric Speakers Using Knowledge Anchoring and Curriculum Learning </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2108</first_page>
						<last_page>2112</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-596</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jeon25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vincent P.</given_name>
<surname>Martin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charles</given_name>
<surname>Brazier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maxime</given_name>
<surname>Amblard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michel</given_name>
<surname>Musiol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Luc</given_name>
<surname>Rouas</surname>
</person_name>
					</contributors>
					<titles><title>Network of acoustic characteristics for the automatic detection of suicide risk from speech. Contribution to the 2025 SpeechWellness challenge by the Semawave team</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>424</first_page>
						<last_page>428</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-599</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/martin25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kiyotada</given_name>
<surname>Mori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seiya</given_name>
<surname>Kawano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angel García</given_name>
<surname>Contreras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichiro</given_name>
<surname>Yoshino</surname>
</person_name>
					</contributors>
					<titles><title>Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3045</first_page>
						<last_page>3049</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-600</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mori25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuan-Kuei</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Azcarreta</given_name>
<surname>Ortiz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kashyap</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Buye</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jung-Suk</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanha</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashutosh</given_name>
<surname>Pandey</surname>
</person_name>
					</contributors>
					<titles><title>A Novel Deep Learning Framework for Efficient Multichannel Acoustic Feedback Control</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>788</first_page>
						<last_page>792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-605</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wu25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue Heng</given_name>
<surname>Yeo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jizhong</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>Visually-Adaptive Guided Robust Speech Recognition with Parameter-Efficient Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4938</first_page>
						<last_page>4942</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-606</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuyang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonghui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianbing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiqiang</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>CAGCRN: Real-Time Speech Enhancement with a Lightweight Model for Joint Acoustic Echo Cancellation and Noise Suppression</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>768</first_page>
						<last_page>772</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-608</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haopeng</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobuaki</given_name>
<surname>Minematsu</surname>
</person_name>
					</contributors>
					<titles><title>A Perception-Based L2 Speech Intelligibility Indicator: Leveraging a Rater’s Shadowing and Sequence-to-sequence Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2420</first_page>
						<last_page>2424</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-615</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/geng25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke-Han</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chun-Yi</given_name>
<surname>Kuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2078</first_page>
						<last_page>2082</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-619</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lu25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jin Sob</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun Joon</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wooseok</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung Won</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3713</first_page>
						<last_page>3717</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-628</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lishi</given_name>
<surname>Zuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Wai</given_name>
<surname>Mak</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Ordinal Information for Speech-based Depression Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>484</first_page>
						<last_page>488</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-638</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zuo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yaoxun</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwei</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hangting</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongzhi</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Luo</surname>
</person_name>
					</contributors>
					<titles><title>WAKE: Watermarking Audio with Key Enrichment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5093</first_page>
						<last_page>5097</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-642</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xu25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hojun</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunsoo</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziwon</given_name>
<surname>Hyung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungjun</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungjin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keunseok</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2220</first_page>
						<last_page>2224</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-643</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jin25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoxiang</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xun</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wangyou</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Ranking and Selection of Bias Words for Contextual Bias Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5183</first_page>
						<last_page>5187</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-646</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hou25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fei</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong-Qiu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Channel Acoustic Echo Cancellation Based on Direction-of-Arrival Estimation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>629</first_page>
						<last_page>633</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-647</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhao25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruoyi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name>
					</contributors>
					<titles><title>MIKU-PAL: An Automated and Standardized Multimodal Method for Speech Paralinguistic and Affect Labeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4308</first_page>
						<last_page>4312</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-648</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cheng25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Natsuo</given_name>
<surname>Yamashita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masaaki</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroaki</given_name>
<surname>Kokubo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yohei</given_name>
<surname>Kawaguchi</surname>
</person_name>
					</contributors>
					<titles><title>LLM-based Generative Error Correction for Rare Words with Synthetic Data and Phonetic Context</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3653</first_page>
						<last_page>3657</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-649</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yamashita25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ben</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangjie</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gang</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuqiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengling</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>StarGAN-Aug: A Cross-domain Fault Audio Generation Method for High-performance Fault Diagnosis of Power Transformers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3399</first_page>
						<last_page>3403</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-651</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/niu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuan</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huy Dat</given_name>
<surname>Tran</surname>
</person_name>
					</contributors>
					<titles><title>Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>753</first_page>
						<last_page>757</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-654</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/nguyen25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Wai</given_name>
<surname>Mak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan</given_name>
<surname>Rohdin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hynek</given_name>
<surname>Hermansky</surname>
</person_name>
					</contributors>
					<titles><title>Bayesian Learning for Domain-Invariant Speaker Verification and Anti-Spoofing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1123</first_page>
						<last_page>1127</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-655</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yizhong</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenxin</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qihang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bingsong</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingming</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>EEG-based Voice Conversion : Hearing the Voice of Your Brain</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4158</first_page>
						<last_page>4162</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-656</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/geng25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lingyun</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristian</given_name>
<surname>Tejedor-Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catia</given_name>
<surname>Cucchiarini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2850</first_page>
						<last_page>2854</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-658</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gao25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hien</given_name>
<surname>Ohnaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuma</given_name>
<surname>Shirahata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byeongseon</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name>
					</contributors>
					<titles><title>Grapheme-Coherent Phonemic and Prosodic Annotation of Speech by Implicit and Explicit Grapheme Conditioning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2525</first_page>
						<last_page>2529</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-661</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ohnaka25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi-Cheng</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huang-Cheng</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2053</first_page>
						<last_page>2057</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-662</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lin25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hannah</given_name>
<surname>White</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joshua</given_name>
<surname>Penney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felicity</given_name>
<surname>Cox</surname>
</person_name>
					</contributors>
					<titles><title>Variability in Intervocalic /t/ and Community Diversity in Australian English </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>121</first_page>
						<last_page>125</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-664</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/white25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Justin J. H.</given_name>
<surname>Lo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrycja</given_name>
<surname>Strycharczuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sam</given_name>
<surname>Kirkham</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory Strategy in Vowel Production as a Basis for Speaker Discrimination</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3504</first_page>
						<last_page>3508</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-666</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sam</given_name>
<surname>O'Connor Russell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naomi</given_name>
<surname>Harte</surname>
</person_name>
					</contributors>
					<titles><title>Visual Cues Support Robust Turn-taking Prediction in Noise</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1073</first_page>
						<last_page>1077</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-668</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/oconnorrussell25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nadav</given_name>
<surname>Har-Tuv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Or</given_name>
<surname>Tal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name>
					</contributors>
					<titles><title>PAST: Phonetic-Acoustic Speech Tokenizer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3509</first_page>
						<last_page>3513</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-669</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hartuv25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seungu</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungho</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juheon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyogu</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Few-step Adversarial Schrödinger Bridge for Generative Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2380</first_page>
						<last_page>2384</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-673</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/han25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thai-Binh</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc-Quan</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Waibel</surname>
</person_name>
					</contributors>
					<titles><title>Cocktail-Party Audio-Visual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1828</first_page>
						<last_page>1832</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-676</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/nguyen25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Xiang</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Cheng</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming-To</given_name>
<surname>Chuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia-Hung</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>I-Ning</given_name>
<surname>Tsai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pei Xing</given_name>
<surname>Kiew</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yueh-Hsuan</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chien-Feng</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo-Han</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenze</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>ToxicTone: A Mandarin Audio Dataset Annotated for Toxicity and Toxic Utterance Tonality</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4008</first_page>
						<last_page>4012</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-679</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/luo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shengkui</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zexu</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>ClearerVoice-Studio: Bridging Advanced Speech Processing Research and Practical Deployment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2980</first_page>
						<last_page>2984</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-680</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhao25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ünal Ege</given_name>
<surname>Gaznepoglu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Leschanowsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmad</given_name>
<surname>Aloradi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prachi</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Tenbrinck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emanuël A. P.</given_name>
<surname>Habets</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nils</given_name>
<surname>Peters</surname>
</person_name>
					</contributors>
					<titles><title>You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4238</first_page>
						<last_page>4242</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-681</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gaznepoglu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yunjae</given_name>
<surname>Nam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeong U</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiyeon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaemin</given_name>
<surname>Lim</surname>
</person_name>
					</contributors>
					<titles><title>Parameter-efficient Fine-tuning of Conformer-based Streaming Speech Recognition into Non-streaming Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4418</first_page>
						<last_page>4422</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-685</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/nam25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kalle</given_name>
<surname>Lahtinen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Einari</given_name>
<surname>Vaaras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liisa</given_name>
<surname>Mustanoja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Okko</given_name>
<surname>Räsänen</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3958</first_page>
						<last_page>3962</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-687</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lahtinen25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shulin</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>HWB-Net: A Novel High-Performance and Efficient Hybrid Waveform Bandwidth Extension Method</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4088</first_page>
						<last_page>4092</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-692</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter</given_name>
<surname>Vieting</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maximilian</given_name>
<surname>Kannen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benedikt</given_name>
<surname>Hilmes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Regularizing Learnable Feature Extraction for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4943</first_page>
						<last_page>4947</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-694</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/vieting25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dimitrios</given_name>
<surname>Damianos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Paraskevopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Potamianos</surname>
</person_name>
					</contributors>
					<titles><title>MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3863</first_page>
						<last_page>3867</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-695</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/damianos25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Julio Cesar</given_name>
<surname>Cavalcanti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Skantze</surname>
</person_name>
					</contributors>
					<titles><title>``Dyadosyncrasy'', Idiosyncrasy and Demographic Factors in Turn-Taking</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1093</first_page>
						<last_page>1097</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-697</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cavalcanti25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jia-Xin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Ming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiayang</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yin-Long</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiuyuan</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia-Hong</given_name>
<surname>Yuan</surname>
</person_name>
					</contributors>
					<titles><title>Decoding Speaker-Normalized Pitch from EEG for Mandarin Perception</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1043</first_page>
						<last_page>1047</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-700</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiaxi</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leyuan</given_name>
<surname>Qu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoxun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taihao</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Label Semantic-Driven Contrastive Learning for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4348</first_page>
						<last_page>4352</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-703</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hu25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Changfeng</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihao</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Differentiable Reward Optimization for LLM based TTS system</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2450</first_page>
						<last_page>2454</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-704</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gao25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qiongqiong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hardik B.</given_name>
<surname>Sailor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianchi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ai Ti</given_name>
<surname>Aw</surname>
</person_name>
					</contributors>
					<titles><title>Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3953</first_page>
						<last_page>3957</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-706</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Tienkamp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fleur</given_name>
<surname>van Ast</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roos</given_name>
<surname>van der Veen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Teja</given_name>
<surname>Rebernik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raoul</given_name>
<surname>Buurke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikki</given_name>
<surname>Hoekzema</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katharina</given_name>
<surname>Polsterer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hedwig</given_name>
<surname>Sekeres</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rob</given_name>
<surname>van Son</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martijn</given_name>
<surname>Wieling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Max</given_name>
<surname>Witjes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastiaan</given_name>
<surname>de Visscher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Defne</given_name>
<surname>Abur</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory clarity and variability before and after surgery for tongue cancer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3558</first_page>
						<last_page>3562</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-708</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tienkamp25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yafeng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chong</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiheng</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Han</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qian</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wen</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Pushing the Frontiers of Self-Distillation Prototypes Network with Dimension Regularization and Score Normalization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3688</first_page>
						<last_page>3692</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-715</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qian</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathilde</given_name>
<surname>Hutin</surname>
</person_name>
					</contributors>
					<titles><title>Evaluation of Three Automatic Alignment Tools for the Processing of Non-native French</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>81</first_page>
						<last_page>85</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-716</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhou25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Taewoo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guisik</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Choongsang</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Young Han</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Naturalness-Aware Curriculum Learning with Dynamic Temperature for Speech Deepfake Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5318</first_page>
						<last_page>5322</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-717</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Si</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyu</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junlan</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>Modeling Multi-Turn Spoken Language Understanding with Dynamic Graph Convolutional Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4113</first_page>
						<last_page>4117</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-718</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/huang25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Tseng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shucong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rogier C.</given_name>
<surname>van Dalen</surname>
</person_name>
					</contributors>
					<titles><title>Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4053</first_page>
						<last_page>4057</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-720</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/parcollet25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huimeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guinan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengxi</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>On-the-fly Routing for Zero-shot MoE Speaker Adaptation of Speech Foundation Models for Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4608</first_page>
						<last_page>4612</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-721</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hu25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kyowoon</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Artyom</given_name>
<surname>Stitsyuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gunu</given_name>
<surname>Jho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Inchul</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaesik</given_name>
<surname>Choi</surname>
</person_name>
					</contributors>
					<titles><title>Counterfactual Activation Editing for Post-hoc Prosody and Mispronunciation Correction in TTS Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>434</first_page>
						<last_page>438</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-723</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lee25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jihyun</given_name>
<surname>Mun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunhee</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minhwa</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>A Cascaded Multimodal Framework for Automatic Social Communication Severity Assessment in Children with Autism Spectrum Disorder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3055</first_page>
						<last_page>3059</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-726</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mun25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoming</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke-Yue</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taiping</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songjun</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shouhong</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>SonarGuard2: Ultrasonic Face Liveness Detection Based on Adaptive Doppler Effect Feature Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2520</first_page>
						<last_page>2524</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-728</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junyu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianrui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianwu</given_name>
<surname>Dang</surname>
</person_name>
					</contributors>
					<titles><title>ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5803</first_page>
						<last_page>5807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-730</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nick</given_name>
<surname>Rossenbach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benedikt</given_name>
<surname>Hilmes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leon</given_name>
<surname>Brackmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moritz</given_name>
<surname>Gunz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name>
					</contributors>
					<titles><title>Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2560</first_page>
						<last_page>2564</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-731</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/rossenbach25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaobin</given_name>
<surname>Rong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dahan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qinwen</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yushi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxiang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>TS-URGENet: A Three-stage Universal Robust and Generalizable Speech Enhancement Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>863</first_page>
						<last_page>867</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-734</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/rong25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chong-Xin</given_name>
<surname>Gan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhe</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zezhong</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zilong</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Wai</given_name>
<surname>Mak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>IDIR: Identifying and Distilling Informative Relations for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5758</first_page>
						<last_page>5762</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-736</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boyu</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruihao</given_name>
<surname>Jing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunyu</given_name>
<surname>Qiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianrui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao-Lei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuelong</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Augment Mandarin to Cantonese Speech Databases via Retrieval-Augmented Generation and Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4248</first_page>
						<last_page>4252</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-737</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sevada</given_name>
<surname>Hovsepyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew Magimai</given_name>
<surname>Doss</surname>
</person_name>
					</contributors>
					<titles><title>Speech power spectra: a window into neural oscillations in Parkinson's disease</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5278</first_page>
						<last_page>5282</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-738</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hovsepyan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yizhou</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Wen</given_name>
<surname>Chao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dianwen</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chongjia</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex Spoken Dialogue Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>176</first_page>
						<last_page>180</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-739</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/peng25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sofoklis</given_name>
<surname>Kakouros</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Impact of Word Informativeness on Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>349</first_page>
						<last_page>353</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-740</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kakouros25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaokang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingfeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yudong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Addressing Task Conflicts in Stuttering Detection via MMoE-Based Multi-Task Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>798</first_page>
						<last_page>802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-743</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mateusz</given_name>
<surname>Guzik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giulio</given_name>
<surname>Cengarle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Arteaga</surname>
</person_name>
					</contributors>
					<titles><title>Deep learning based spatial aliasing reduction in beamforming for audio capture</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2515</first_page>
						<last_page>2519</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-746</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/guzik25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fabian</given_name>
<surname>Ritter-Gutierrez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Cheng</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jui-Chiang</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeremy H.M</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nancy F.</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Distilling a speech and music encoder with task arithmetic</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3858</first_page>
						<last_page>3862</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-747</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/rittergutierrez25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaohuai</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuangqi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyu</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianjun</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanzeng</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>Multistage Universal Speech Enhancement System for URGENT Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>868</first_page>
						<last_page>872</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-749</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/le25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liang</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maoshen</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yonggang</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>Power Spectral Density Estimation for Acoustic Source Separation Using A Spherical Microphone Array</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1473</first_page>
						<last_page>1477</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-753</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tao25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoxun</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leyuan</given_name>
<surname>Qu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaxi</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taihao</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>EME-TTS: Unlocking the Emphasis and Emotion Link in Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4368</first_page>
						<last_page>4372</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-754</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi-Wen</given_name>
<surname>Chao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yizhou</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dianwen</given_name>
<surname>Ng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chongjia</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>A-SMiLE: Affective Sparse Mixture-of-Experts Adapter with Multi-Task Learning for Spoken Dialogue Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5458</first_page>
						<last_page>5462</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-756</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chao25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saskia</given_name>
<surname>Wepner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Eckert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gernot</given_name>
<surname>Kubin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Schuppler</surname>
</person_name>
					</contributors>
					<titles><title>What the Filler? Both ASR Systems and Humans Struggle More With Other Kinds of Disfluencies Than With Filler Particles</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2325</first_page>
						<last_page>2329</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-757</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wepner25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>João</given_name>
<surname>Menezes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aubin</given_name>
<surname>Mouras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arne-Lukas</given_name>
<surname>Fietkau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dani</given_name>
<surname>Kazzy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Silent Recognition of Phonemes Using Radar and Optopalatographic Silent Speech Interfaces</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5058</first_page>
						<last_page>5062</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-759</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/menezes25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Benedikt</given_name>
<surname>Hilmes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nick</given_name>
<surname>Rossenbach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing the Importance of Blank for CTC-Based Knowledge Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1998</first_page>
						<last_page>2002</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-760</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hilmes25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Injune</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jung-Min</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju Seok</given_name>
<surname>Ryu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyogu</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Voice-Based Dysphagia Detection: Leveraging Self-Supervised Speech Representation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5683</first_page>
						<last_page>5687</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-761</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hwang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christina</given_name>
<surname>Tånnander</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>House</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas</given_name>
<surname>Beskow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Edlund</surname>
</person_name>
					</contributors>
					<titles><title>Intrasentential English in Swedish TTS: perceived English-accentedness</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1638</first_page>
						<last_page>1642</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-762</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tannander25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seraphina</given_name>
<surname>Fong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Matassoni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessio</given_name>
<surname>Brutti</surname>
</person_name>
					</contributors>
					<titles><title>Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2003</first_page>
						<last_page>2007</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-764</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fong25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenyu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hangting</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haina</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>TVC-MusicGen: Time-Varying Structure Control for Background Music Generation via Self-Supervised Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1238</first_page>
						<last_page>1242</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-766</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peiran</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>EEG-based Speech Decoding Based on Multi-mode Joint Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5598</first_page>
						<last_page>5602</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-769</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Paierl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Hagmüller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Schuppler</surname>
</person_name>
					</contributors>
					<titles><title>Continuous prediction of backchannel timing for human-robot interaction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3020</first_page>
						<last_page>3024</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-770</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/paierl25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>James</given_name>
<surname>Taylor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wolfgang</given_name>
<surname>Mack</surname>
</person_name>
					</contributors>
					<titles><title>Improving Audio Classification by Transitioning from Zero- to Few-Shot </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2585</first_page>
						<last_page>2589</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-771</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/taylor25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lu</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junqi</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Renhua</given_name>
<surname>Peng</surname>
</person_name>
					</contributors>
					<titles><title>WTFormer: A Wavelet Conformer Network for MIMO Speech Enhancement with Spatial Cues Peservation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1188</first_page>
						<last_page>1192</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-773</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/han25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raphaël</given_name>
<surname>Bagat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Irina</given_name>
<surname>Illina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Vincent</surname>
</person_name>
					</contributors>
					<titles><title>Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1143</first_page>
						<last_page>1147</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-775</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bagat25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yihan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yichen</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijing</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaqi</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruihua</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>GALAXY: A Large-Scale Open-Domain Dataset for Multimodal Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>171</first_page>
						<last_page>175</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-776</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wu25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seohyun</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chitralekha</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michelle</given_name>
<surname>Kah Yian Kwan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhui</given_name>
<surname>Fung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander Wenjun</given_name>
<surname>Yip</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suranga</given_name>
<surname>Nanayakkara</surname>
</person_name>
					</contributors>
					<titles><title>Towards Temporally Explainable Dysarthric Speech Clarity Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2138</first_page>
						<last_page>2142</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-777</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/park25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sujoy</given_name>
<surname>Roychowdhury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ranjani</given_name>
<surname>H.G.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sumit</given_name>
<surname>Soman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nishtha</given_name>
<surname>Paul</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Subhadip</given_name>
<surname>Bandyopadhyay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddhanth</given_name>
<surname>Iyengar</surname>
</person_name>
					</contributors>
					<titles><title>Intelligibility of Text-to-Speech Systems for Mathematical Expressions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2280</first_page>
						<last_page>2284</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-779</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/roychowdhury25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruichen</given_name>
<surname>Zuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kong Aik</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zilong</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Wai</given_name>
<surname>Mak</surname>
</person_name>
					</contributors>
					<titles><title>The Sub-3Sec Problem: From Text-Independent to Text-Dependent Corpus</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4003</first_page>
						<last_page>4007</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-782</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zuo25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sofoklis</given_name>
<surname>Kakouros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Sounding Like a Winner? Prosodic Differences in Post-Match Interviews</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4188</first_page>
						<last_page>4192</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-783</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kakouros25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thiago Henrique Gomes</given_name>
<surname>Lobato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Magnus</given_name>
<surname>Schäfer</surname>
</person_name>
					</contributors>
					<titles><title>Gradual modeling of the Lombard effect by modifying speaker embeddings from a Text-To-Speech model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4148</first_page>
						<last_page>4152</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-787</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lobato25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Gimeno-Gómez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rubén</given_name>
<surname>Solera-Ureña</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Pompili</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos-D.</given_name>
<surname>Martínez-Hinarejos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rita</given_name>
<surname>Cardoso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Guimarães</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joaquim J.</given_name>
<surname>Ferreira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name>
					</contributors>
					<titles><title>On the Relevance of Clinical Assessment Tasks for the Automatic Detection of Parkinson’s Disease Medication State from Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5273</first_page>
						<last_page>5277</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-793</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gimenogomez25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kihyun</given_name>
<surname>Nam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jungwoo</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gangin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaeyoung</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ha-Jin</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>SEED: Speaker Embedding Enhancement Diffusion Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3718</first_page>
						<last_page>3722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-794</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/nam25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhihang</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tong</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rilin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengshi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Scaling beyond Denoising: Submitted System and Findings in URGENT Challenge 2025</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>873</first_page>
						<last_page>877</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-795</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sun25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Runduo</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanxin</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yihui</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zihan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukai</given_name>
<surname>Jv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-car Speech Separation with Distributed Heterogeneous Arrays</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4978</first_page>
						<last_page>4982</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-800</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/han25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emmy</given_name>
<surname>Postma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristian</given_name>
<surname>Tejedor-Garcia</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating the Effectiveness of Pre-Trained Audio Embeddings for Classification of Parkinson's Disease Speech Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4603</first_page>
						<last_page>4607</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-801</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/postma25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongtao</given_name>
<surname>Bao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Frequency-Domain Enhanced Extreme Bandwidth Extension Network with ICCRN for Superior Speech Quality</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4093</first_page>
						<last_page>4097</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-806</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bao25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ho-Young</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jae-Heung</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pil Moo</given_name>
<surname>Byun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Won-Gook</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Temp4Cap: Temporally-aligned Automated Audio Captioning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3135</first_page>
						<last_page>3139</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-808</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/choi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zehua</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaolou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lantian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>CNVSRC 2024: The Second Chinese Continuous Visual Speech Recognition Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2705</first_page>
						<last_page>2709</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-812</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Şeymanur</given_name>
<surname>Akti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuan-Nam</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Waibel</surname>
</person_name>
					</contributors>
					<titles><title>Towards Better Disentanglement in Non-Autoregressive Zero-Shot Expressive Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1358</first_page>
						<last_page>1362</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-815</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/akti25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingda</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name>
					</contributors>
					<titles><title>Bridging Speech and Singing: Multi-stage Speech-Prompted Singing Voice Conversion with Speaker Embedding Adaptation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1248</first_page>
						<last_page>1252</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-816</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Franzreb</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnab</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Polzehl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Möller</surname>
</person_name>
					</contributors>
					<titles><title>Private kNN-VC: Interpretable Anonymization of Converted Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3224</first_page>
						<last_page>3228</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-820</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/franzreb25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Edresson</given_name>
<surname>Casanova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paarth</given_name>
<surname>Neekhara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryan</given_name>
<surname>Langman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shehzeen</given_name>
<surname>Hussain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Subhankar</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuesong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ante</given_name>
<surname>Jukic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5028</first_page>
						<last_page>5032</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-827</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/casanova25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aditya Kamlesh</given_name>
<surname>Parikh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristian</given_name>
<surname>Tejedor-Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catia</given_name>
<surname>Cucchiarini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological Knowledge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5068</first_page>
						<last_page>5072</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-829</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/parikh25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shi-Xin</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang-Yeh</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Cheng</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huang-Cheng</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>136</first_page>
						<last_page>140</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-832</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fang25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jen-Tzung</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Po-Chun</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>CAPR: Confidence-Aware Prompt Refinement in Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3264</first_page>
						<last_page>3268</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-834</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chien25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kfir</given_name>
<surname>Cohen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lior</given_name>
<surname>Wolf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bracha</given_name>
<surname>Laufer-Goldshtein</surname>
</person_name>
					</contributors>
					<titles><title>Discovering Directions of Uncertainty in Speech Inpainting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4208</first_page>
						<last_page>4212</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-835</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cohen25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chih-Kai</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neo</given_name>
<surname>Ho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yen-Ting</given_name>
<surname>Piao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1788</first_page>
						<last_page>1792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-839</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Geonyoung</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Geonhee</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul Hongsuck</given_name>
<surname>Seo</surname>
</person_name>
					</contributors>
					<titles><title>DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4983</first_page>
						<last_page>4987</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-840</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lee25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nimrod</given_name>
<surname>Shabtay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zvi</given_name>
<surname>Kons</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Avihu</given_name>
<surname>Dekel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hagai</given_name>
<surname>Aronowitz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ron</given_name>
<surname>Hoory</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Assaf</given_name>
<surname>Arbelle</surname>
</person_name>
					</contributors>
					<titles><title>Spoken Question Answering for Visual Queries</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4823</first_page>
						<last_page>4827</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-843</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shabtay25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tzlil</given_name>
<surname>Avidan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bracha</given_name>
<surname>Laufer-Goldshtein</surname>
</person_name>
					</contributors>
					<titles><title>Deep-Simplex Multichannel Speech Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1463</first_page>
						<last_page>1467</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-844</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/avidan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruoxuan</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangjian</given_name>
<surname>Zeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingqiang</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>RuiChen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Le</given_name>
<surname>Ren</surname>
</person_name>
					</contributors>
					<titles><title>WhisperMSS: A Two-Stage Framework for Mandarin Singing Transcription and Segmentation Using Pretrained Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3110</first_page>
						<last_page>3114</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-847</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liang25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Frank</given_name>
<surname>Zalkow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paolo</given_name>
<surname>Sani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kishor</given_name>
<surname>Kayyar Lakshminarayana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emanuël A. P.</given_name>
<surname>Habets</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicola</given_name>
<surname>Pia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Dittmar</surname>
</person_name>
					</contributors>
					<titles><title>Bridging the Training–Inference Gap in TTS: Training Strategies for Robust Generative Postprocessing for Low-Resource Speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2470</first_page>
						<last_page>2474</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-854</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zalkow25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saisamarth Rajesh</given_name>
<surname>Phaye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milos</given_name>
<surname>Cernak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Harper</surname>
</person_name>
					</contributors>
					<titles><title>Model as Loss: A Self-Consistent Training Paradigm</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2370</first_page>
						<last_page>2374</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-855</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/phaye25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiale</given_name>
<surname>Ou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongying</given_name>
<surname>Zan</surname>
</person_name>
					</contributors>
					<titles><title>CMSP-ST: Cross-modal Mixup with Speech Purification for End-to-End Speech Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>16</first_page>
						<last_page>20</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-858</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ou25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenwei</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alif</given_name>
<surname>Silpachai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catia</given_name>
<surname>Cucchiarini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>Multitalker Babble in English Vowel Perception Training: A Comparison between Humans and Neural Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1278</first_page>
						<last_page>1282</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-859</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dong25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Charles</given_name>
<surname>McGhee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark J.F.</given_name>
<surname>Gales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kate M.</given_name>
<surname>Knill</surname>
</person_name>
					</contributors>
					<titles><title>Training Articulatory Inversion Models for Interspeaker Consistency</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5583</first_page>
						<last_page>5587</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-860</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mcghee25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuyang</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sami O.</given_name>
<surname>Simons</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visara</given_name>
<surname>Urovi</surname>
</person_name>
					</contributors>
					<titles><title>Developing a LeFF Transformer Model for Exacerbated Speech Detection in COPD and Asthma </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>993</first_page>
						<last_page>997</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-861</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yan25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhaoyang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>XiaoXiao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wangjie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longjie</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Diarization with Overlapping Community Detection Using Graph Attention Networks and Label Propagation Algorithm</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5213</first_page>
						<last_page>5217</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-862</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shuwen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingke</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingyi</given_name>
<surname>Luo</surname>
</person_name>
					</contributors>
					<titles><title>The Prosodic Characteristics of Standard Chinese Rhetorical Questions in Naturalistic Settings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5363</first_page>
						<last_page>5367</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-863</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juraj</given_name>
<surname>Šimko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Elie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alice</given_name>
<surname>Turk</surname>
</person_name>
					</contributors>
					<titles><title>Self-supervised Optimality-Guided Learning of Speech Articulation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1033</first_page>
						<last_page>1037</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-866</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/simko25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krishna</given_name>
<surname>Puvvada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elena</given_name>
<surname>Rastorgueva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhehuai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>He</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuoyang</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kunal</given_name>
<surname>Dhawan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hainan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagadeesh</given_name>
<surname>Balam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Word Level Timestamp Generation for Automatic Speech Recognition and Translation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2565</first_page>
						<last_page>2569</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-869</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hu25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>-</given_name>
<surname>Mansi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasios</given_name>
<surname>Lepipas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominika C</given_name>
<surname>Woszczyk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiying</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soteris</given_name>
<surname>Demetriou</surname>
</person_name>
					</contributors>
					<titles><title>Understanding Dementia Speech Alignment with Diffusion-Based Image Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1428</first_page>
						<last_page>1432</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-871</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mansi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ke</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ehsan</given_name>
<surname>Hosseini-Asl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Edresson</given_name>
<surname>Casanova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Subhankar</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Żelasko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhehuai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagadeesh</given_name>
<surname>Balam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2715</first_page>
						<last_page>2719</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-874</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hu25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhaoyang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haodong</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longjie</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>XiaoXiao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongxin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>Cross-attention and Self-attention for Audio-visual Speaker Diarization in MISP-Meeting Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1893</first_page>
						<last_page>1897</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-875</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haroun</given_name>
<surname>Elleuch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Salima</given_name>
<surname>Mdhaffar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fethi</given_name>
<surname>Bougares</surname>
</person_name>
					</contributors>
					<titles><title>ADI-20: Arabic Dialect Identification dataset and models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2775</first_page>
						<last_page>2779</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-884</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/elleuch25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emiliano</given_name>
<surname>Acevedo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martín</given_name>
<surname>Rocamora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Magdalena</given_name>
<surname>Fuentes</surname>
</person_name>
					</contributors>
					<titles><title>Domain Adaptation Method and Modality Gap Impact in Audio-Text Models for Prototypical Sound Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1328</first_page>
						<last_page>1332</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-886</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/acevedo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Mohammadamini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aghilas</given_name>
<surname>Sini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie</given_name>
<surname>Tahon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Laurent</surname>
</person_name>
					</contributors>
					<titles><title>Scaling pseudo-labeling data for end-to-end low-resource speech translation (the case of Kurdish language)</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>898</first_page>
						<last_page>902</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-887</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mohammadamini25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bornali</given_name>
<surname>Phukon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiuwen</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name>
					</contributors>
					<titles><title>Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5708</first_page>
						<last_page>5712</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-891</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/phukon25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Esther</given_name>
<surname>Janse</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Cooke</surname>
</person_name>
					</contributors>
					<titles><title>Prediction of listening effort ratings for habitual and clear-Lombard speech presented in noise</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1288</first_page>
						<last_page>1292</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-892</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/janse25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>De</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qilong</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Joint Rate Allocation and Sensor Selection for Speech Enhancement in Wireless Acoustic Sensor Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3838</first_page>
						<last_page>3842</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-893</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hu25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bongjun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arindam</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark C.</given_name>
<surname>Fuhs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Chowdhury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deblin</given_name>
<surname>Bagchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monika</given_name>
<surname>Woszczyna</surname>
</person_name>
					</contributors>
					<titles><title>A Hybrid Approach to Combining Role Diarization with ASR for Professional Conversations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5243</first_page>
						<last_page>5247</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-895</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoyang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuchen</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songting</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name>
					</contributors>
					<titles><title>From KAN to GR-KAN: Advancing Speech Enhancement with KAN-Based Methodology</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5153</first_page>
						<last_page>5157</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-896</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Loes</given_name>
<surname>van Bemmel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lauren G</given_name>
<surname>Reinders</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Folkert</given_name>
<surname>Brijker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bas</given_name>
<surname>Holverda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frits M.E.</given_name>
<surname>Franssen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanneke</given_name>
<surname>van Helvoort</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visara</given_name>
<surname>Urovi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marieke</given_name>
<surname>Spreeuwenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sami O.</given_name>
<surname>Simons</surname>
</person_name>
					</contributors>
					<titles><title>SPEAKtoCOPD: a flashmob study to collect COPD speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>988</first_page>
						<last_page>992</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-899</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/vanbemmel25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rong</given_name>
<surname>Chao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rauf</given_name>
<surname>Nasretdinov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Chiang Frank</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ante</given_name>
<surname>Jukic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szu-Wei</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>Universal Speech Enhancement with Regression and Generative Mamba</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>888</first_page>
						<last_page>892</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-900</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chao25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Harm</given_name>
<surname>Lameris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joakim</given_name>
<surname>Gustafsson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name>
					</contributors>
					<titles><title>VoiceQualityVC: A Voice Conversion System for Studying the Perceptual Effects of Voice Quality in Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2295</first_page>
						<last_page>2299</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-902</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lameris25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rogier C.</given_name>
<surname>van Dalen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shucong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Titouan</given_name>
<surname>Parcollet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sourav</given_name>
<surname>Bhattacharya</surname>
</person_name>
					</contributors>
					<titles><title>Robust Unsupervised Adaptation of a Speech Recogniser Using Entropy Minimisation and Speaker Codes</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4953</first_page>
						<last_page>4957</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-903</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/vandalen25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junsheng</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qintuya</given_name>
<surname>Si</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>De</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>D-GAT: Dual Graph Attention Network for Global HRTF Interpolation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2510</first_page>
						<last_page>2514</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-905</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hu25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minghui</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengpeng</given_name>
<surname>Ji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jialong</given_name>
<surname>Zuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xize</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenrui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoda</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruofan</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jieming</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhou</given_name>
<surname>Zhao</surname>
</person_name>
					</contributors>
					<titles><title>GTA: Towards Generative Text-To-Audio Retrieval via Multi-Scale Tokenizer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2650</first_page>
						<last_page>2654</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-908</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fang25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lauren G</given_name>
<surname>Reinders</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Loes</given_name>
<surname>van Bemmel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Mackay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Nobbs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frits M.E.</given_name>
<surname>Franssen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hester</given_name>
<surname>Gietema</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simona</given_name>
<surname>Schäfer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sami O.</given_name>
<surname>Simons</surname>
</person_name>
					</contributors>
					<titles><title>Effect of physical exercise on voice in people living with COPD</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1003</first_page>
						<last_page>1007</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-910</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/reinders25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eliathamby</given_name>
<surname>Ambikairajah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ting</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidhyasaharan</given_name>
<surname>Sethu</surname>
</person_name>
					</contributors>
					<titles><title>A Study of Speech Embedding Similarities Between Australian Aboriginal and High-Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1498</first_page>
						<last_page>1502</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-911</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ambikairajah25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenwei</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catia</given_name>
<surname>Cucchiarini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roeland</given_name>
<surname>van Hout</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Progress of CALL System Users on Accentedness and Comprehensibility: An Acoustic and ASR-Based Approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4438</first_page>
						<last_page>4442</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-914</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dong25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dennis</given_name>
<surname>Fucci</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Gaido</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matteo</given_name>
<surname>Negri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mauro</given_name>
<surname>Cettolo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luisa</given_name>
<surname>Bentivogli</surname>
</person_name>
					</contributors>
					<titles><title>Echoes of Phonetics:  Unveiling Relevant Acoustic Cues for ASR via Feature Attribution</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>206</first_page>
						<last_page>210</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-918</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fucci25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Avishai</given_name>
<surname>Weizman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yehuda</given_name>
<surname>Ben-Shimol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Itshak</given_name>
<surname>Lapidot</surname>
</person_name>
					</contributors>
					<titles><title>ASVspoof2019 vs. ASVspoof5: Assessment and Comparison</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4568</first_page>
						<last_page>4572</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-920</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/weizman25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haibin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sefik</given_name>
<surname>Emre Eskimez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>TS3-Codec: Transformer-Based Simple Streaming Single Codec</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>604</first_page>
						<last_page>608</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-921</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wu25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lilian von</given_name>
<surname>Bressensdorf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pia</given_name>
<surname>Greca</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Harrington</surname>
</person_name>
					</contributors>
					<titles><title>Agent-based modelling, sound change, and metaphony in Southern Italian varieties of Italo-Romance.</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2950</first_page>
						<last_page>2954</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-924</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bressensdorf25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lekshmi</given_name>
<surname>C R</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajeev</given_name>
<surname>Rajan</surname>
</person_name>
					</contributors>
					<titles><title>Focal Modulation Network: A Novel Solution for Polyphonic Music Instrument Recognition without Attention and Aggregation Strategy </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3095</first_page>
						<last_page>3099</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-930</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cr25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sophie</given_name>
<surname>Young</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fuxiang</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bahman</given_name>
<surname>Mirheidari</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Madhurananda</given_name>
<surname>Pahar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Reuber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heidi</given_name>
<surname>Christensen</surname>
</person_name>
					</contributors>
					<titles><title>Can Speech Accurately Detect Depression in Patients With Comorbid Dementia? An Approach for Mitigating Confounding Effects of Depression and Dementia</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>499</first_page>
						<last_page>503</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-933</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/young25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shiyao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaming</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiwan</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Qin</surname>
</person_name>
					</contributors>
					<titles><title>A Self-Training Approach for Whisper to Enhance Long Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3299</first_page>
						<last_page>3303</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-934</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jazmín</given_name>
<surname>Vidal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luciana</given_name>
<surname>Ferrer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan Esteban</given_name>
<surname>Kamienkowski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Riera</surname>
</person_name>
					</contributors>
					<titles><title>Improving Automatic Speech Recognition for Children's Reading Assessment with Disfluency-aware Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2855</first_page>
						<last_page>2859</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-936</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/vidal25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>You</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Baotong</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyao</given_name>
<surname>Duan</surname>
</person_name>
					</contributors>
					<titles><title>PartialEdit: Identifying Partial Deepfakes in the Era of Neural Speech Editing  </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5353</first_page>
						<last_page>5357</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-942</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Sawada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Miyazaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2575</first_page>
						<last_page>2579</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-943</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/he25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takanori</given_name>
<surname>Ashihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Matsuura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Semantic and Acoustic Token Variability Across Speech, Music, and Audio Domains</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>226</first_page>
						<last_page>230</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-945</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ashihara25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linda</given_name>
<surname>Bakkouche</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charles</given_name>
<surname>McGhee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily</given_name>
<surname>Lau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephanie</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinbing</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Madeleine</given_name>
<surname>Rees</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Alter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brechtje</given_name>
<surname>Post</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Schwarz</surname>
</person_name>
					</contributors>
					<titles><title>Finding the Human Voice in AI: Insights on the Perception of AI-Voice Clones from Naturalness and Similarity Ratings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2190</first_page>
						<last_page>2194</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-947</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bakkouche25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianhua</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tengfei</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhanglin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenming</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>PromptEVC: Controllable Emotional Voice Conversion with Natural Language Prompts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4588</first_page>
						<last_page>4592</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-948</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/qi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nikolay</given_name>
<surname>Karpov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sofia</given_name>
<surname>Kostandian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nune</given_name>
<surname>Tadevosyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexan</given_name>
<surname>Ayrapetyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrei</given_name>
<surname>Andrusenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ara</given_name>
<surname>Yeroyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mher</given_name>
<surname>Yerznkanyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vitaly</given_name>
<surname>Lavrukhin</surname>
</person_name>
					</contributors>
					<titles><title>From Scarcity to Sufficiency: Speech Recognition Pipeline for Zero-resource Language</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4303</first_page>
						<last_page>4307</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-950</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/karpov25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linda</given_name>
<surname>Bakkouche</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brechtje</given_name>
<surname>Post</surname>
</person_name>
					</contributors>
					<titles><title>Influence of Proficiency and L2 Experience on Dynamic Spectral Cue Utilization in L2 Vowel Perception and Production</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>101</first_page>
						<last_page>105</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-954</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bakkouche25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Bataev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrei</given_name>
<surname>Andrusenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lilit</given_name>
<surname>Grigoryan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksandr</given_name>
<surname>Laptev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vitaly</given_name>
<surname>Lavrukhin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>644</first_page>
						<last_page>648</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-955</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bataev25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Delphine</given_name>
<surname>Charuau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naomi</given_name>
<surname>Harte</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Dynamics of Hand Gestures and Pauses in Multiparty Interactions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3030</first_page>
						<last_page>3034</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-959</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/charuau25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingyi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicolas</given_name>
<surname>Audibert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaru</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martine</given_name>
<surname>Adda-Decker</surname>
</person_name>
					</contributors>
					<titles><title>Corpus-Based Insights into Mandarin Neutral Tone: Effects of Tonal Context and Structural Patterns in Spontaneous Speech </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4178</first_page>
						<last_page>4182</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-960</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sun25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Long</given_name>
<surname>Mai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Carson-Berndsen</surname>
</person_name>
					</contributors>
					<titles><title>Improving Linguistic Diversity of Large Language Models with Possibility Exploration Fine-Tuning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3254</first_page>
						<last_page>3258</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-962</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mai25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sofiane</given_name>
<surname>Azzouz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre-André</given_name>
<surname>Vuissoz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yves</given_name>
<surname>Laprie</surname>
</person_name>
					</contributors>
					<titles><title>Reconstruction of the Complete Vocal Tract Contour Through Acoustic to Articulatory Inversion Using Real-Time MRI Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>978</first_page>
						<last_page>982</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-963</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/azzouz25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Priyanka</given_name>
<surname>Kommagouni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pragya</given_name>
<surname>Khanna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vamshiraghusimha</given_name>
<surname>Narasinga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anirudh</given_name>
<surname>Bocha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anil Kumar</given_name>
<surname>Vuppala</surname>
</person_name>
					</contributors>
					<titles><title>Towards Classification of Typical and Atypical Disfluencies: A Self Supervised Representation Approach </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5293</first_page>
						<last_page>5297</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-964</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kommagouni25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anisia</given_name>
<surname>Popescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lori</given_name>
<surname>Lamel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Evrard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ioana</given_name>
<surname>Vasilescu</surname>
</person_name>
					</contributors>
					<titles><title>Tracking /r/ Deletion: Forced Alignment of Pronunciation Variants and Sociophonetic Insights into Post-Obstruent Final /r/ in French</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2945</first_page>
						<last_page>2949</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-967</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/popescu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yunzhuo</given_name>
<surname>Xiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyi</given_name>
<surname>Sun</surname>
</person_name>
					</contributors>
					<titles><title>Modeling Formant Dynamics in Mandarin /ai/: Effects of Speech Style and Speech Rate</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>369</first_page>
						<last_page>373</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-968</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xiang25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aviv</given_name>
<surname>Navon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aviv</given_name>
<surname>Shamsian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yael</given_name>
<surname>Segal-Feldman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neta</given_name>
<surname>Glazer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gil</given_name>
<surname>Hetz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Keshet</surname>
</person_name>
					</contributors>
					<titles><title>FlowTSE: Target Speaker Extraction with Flow Matching</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2965</first_page>
						<last_page>2969</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-970</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/navon25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alejandro</given_name>
<surname>Sosa Welford</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Pepino</surname>
</person_name>
					</contributors>
					<titles><title>A Dataset for Automatic Assessment of TTS Quality in Spanish</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4813</first_page>
						<last_page>4817</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-973</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sosawelford25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kwanghee</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masao</given_name>
<surname>Someki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emma</given_name>
<surname>Strubell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>On-device Streaming Discrete Speech Units</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4423</first_page>
						<last_page>4427</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-975</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/choi25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Agata</given_name>
<surname>Sage</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zuzanna</given_name>
<surname>Miodońska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michał</given_name>
<surname>Kręcichwost</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewa</given_name>
<surname>Kwaśniok</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paweł</given_name>
<surname>Badura</surname>
</person_name>
					</contributors>
					<titles><title>Visual features of the oral region in Polish sibilants produced by children with various sibilance patterns</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2033</first_page>
						<last_page>2037</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-979</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sage25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wen-Wei</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao-Wei</given_name>
<surname>Chi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kuan-Chen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ping-Cheng</given_name>
<surname>Yeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Te-hsin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chen-Yu</given_name>
<surname>Chiang</surname>
</person_name>
					</contributors>
					<titles><title>OMPAL: Bridging Speech and Learning with an Open-Source Mandarin Pronunciation Assessment Corpus for Global Learners</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2415</first_page>
						<last_page>2419</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-983</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hsieh25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna</given_name>
<surname>Leschanowsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kishor</given_name>
<surname>Kayyar Lakshminarayana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anjana</given_name>
<surname>Rajasekhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lyonel</given_name>
<surname>Behringer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ibrahim</given_name>
<surname>Kilinc</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guillaume</given_name>
<surname>Fuchs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emanuël A. P.</given_name>
<surname>Habets</surname>
</person_name>
					</contributors>
					<titles><title>Benchmarking Neural Speech Codec Intelligibility with SITool</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5488</first_page>
						<last_page>5492</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-984</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/leschanowsky25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arnav</given_name>
<surname>Rustagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Satvik</given_name>
<surname>Bajpai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nimrat</given_name>
<surname>Kaur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddharth</given_name>
<surname>Siddharth</surname>
</person_name>
					</contributors>
					<titles><title>Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2155</first_page>
						<last_page>2159</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-986</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/rustagi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pranjal</given_name>
<surname>Aggarwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ghritachi</given_name>
<surname>Mahajani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavan Kumar</given_name>
<surname>Malasani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vaibhav</given_name>
<surname>Jamadagni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caroline J.</given_name>
<surname>Wendt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ehsanul Haque</given_name>
<surname>Nirjhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Theodora</given_name>
<surname>Chaspari</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Reasoning Abilities of Large Language Models for Understanding Spoken Language in Interpersonal Interactions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4518</first_page>
						<last_page>4522</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-988</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/aggarwal25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryan</given_name>
<surname>Langman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuesong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paarth</given_name>
<surname>Neekhara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shehzeen</given_name>
<surname>Hussain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Edresson</given_name>
<surname>Casanova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evelina</given_name>
<surname>Bakhturina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>HiFiTTS-2: A Large-Scale High Bandwidth Speech Dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4778</first_page>
						<last_page>4782</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-989</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/langman25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aymen</given_name>
<surname>Bashir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haolan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amin</given_name>
<surname>Edraki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wai-Yip</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesper</given_name>
<surname>Jensen</surname>
</person_name>
					</contributors>
					<titles><title>Intelligibility Prediction for Time-Modified Speech Signals Using Spectro-Temporal Modulation Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5478</first_page>
						<last_page>5482</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-991</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bashir25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kyungguen</given_name>
<surname>Byun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Filos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erik</given_name>
<surname>Visser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunkuk</given_name>
<surname>Moon</surname>
</person_name>
					</contributors>
					<titles><title>Voice-ENHANCE: Speech Restoration using a Diffusion-based Voice Conversion Framework</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4068</first_page>
						<last_page>4072</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-998</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/byun25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saba</given_name>
<surname>Tabatabaee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>FT-Boosted SV: Towards Noise Robust Speaker Verification for English Speaking Classroom Environments </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2815</first_page>
						<last_page>2819</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1002</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tabatabaee25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pai</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dhruuv</given_name>
<surname>Agarwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kurt</given_name>
<surname>Partridge</surname>
</person_name>
					</contributors>
					<titles><title>LLM-Synth4KWS: Scalable Automatic Generation and Synthesis of Confusable Data for Custom Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2675</first_page>
						<last_page>2679</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1005</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhu25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziwei</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaan</given_name>
<surname>Donbekci</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Run</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Sasu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zehui</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name>
					</contributors>
					<titles><title>Learning More with Less: Self-Supervised Approaches forLow-Resource Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>151</first_page>
						<last_page>155</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1006</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gong25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chuan</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Verhulst</surname>
</person_name>
					</contributors>
					<titles><title>Individualized speech enhancement for hearing-impaired listeners</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3843</first_page>
						<last_page>3847</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1009</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wen25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siavash</given_name>
<surname>Shams</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Antonello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gavin</given_name>
<surname>Mischler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephan</given_name>
<surname>Bickel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashesh</given_name>
<surname>Mehta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nima</given_name>
<surname>Mesgarani</surname>
</person_name>
					</contributors>
					<titles><title>Neuro2Semantic: A Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2920</first_page>
						<last_page>2924</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1010</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shams25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Margot</given_name>
<surname>Masson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabelle</given_name>
<surname>Ferrané</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie</given_name>
<surname>Mauclair</surname>
</person_name>
					</contributors>
					<titles><title>Identification of Pathological Pronunciation Profiles in ASR Transcription Errors</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1863</first_page>
						<last_page>1867</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1011</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/masson25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aditya Kamlesh</given_name>
<surname>Parikh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristian</given_name>
<surname>Tejedor-Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catia</given_name>
<surname>Cucchiarini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helmer</given_name>
<surname>Strik</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Logit-Based GOP Scores for Mispronunciation Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2405</first_page>
						<last_page>2409</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1012</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/parikh25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chutong</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martijn</given_name>
<surname>Bartelds</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shih-Heng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsiu-Hsuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rafael</given_name>
<surname>Mosquera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Hincapie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Jurafsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonis</given_name>
<surname>Anastasopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karen</given_name>
<surname>Livescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2093</first_page>
						<last_page>2097</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1013</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pei-Chin</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yih-Liang</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc-Son</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tai-Shih</given_name>
<surname>Chi</surname>
</person_name>
					</contributors>
					<titles><title>Tonality-Based Accompaniment-Guided Automatic Singing Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3085</first_page>
						<last_page>3089</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1015</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hsieh25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lea</given_name>
<surname>Fischbach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akbar</given_name>
<surname>Karimi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caroline</given_name>
<surname>Kleen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfred</given_name>
<surname>Lameli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucie</given_name>
<surname>Flek</surname>
</person_name>
					</contributors>
					<titles><title>Improving Low-Resource Dialect Classification Using Retrieval-based Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2780</first_page>
						<last_page>2784</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1017</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fischbach25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Natarajan</given_name>
<surname>Balaji Shankar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zilai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaiyuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>CHSER: A Dataset and Case Study on Generative Speech Error Correction for Child ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2895</first_page>
						<last_page>2899</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1019</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/balajishankar25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Porteš</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleš</given_name>
<surname>Horák</surname>
</person_name>
					</contributors>
					<titles><title>Learning Optimal Prosody Embedding Codebook based on F0 and Energy</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4728</first_page>
						<last_page>4732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1020</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/portes25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yike</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iran R.</given_name>
<surname>Roman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Poeppel</surname>
</person_name>
					</contributors>
					<titles><title>Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>216</first_page>
						<last_page>220</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1021</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chang25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarina</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekaterina</given_name>
<surname>Kolos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc Thang</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>First Steps Towards Voice Anonymization for Code-Switching Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5123</first_page>
						<last_page>5127</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1027</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/meyer25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Polychronia</given_name>
<surname>Christodoulidou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Tanner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jane</given_name>
<surname>Stuart-Smith</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>McAuliffe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mridhula</given_name>
<surname>Murali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amy</given_name>
<surname>Smith</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lauren</given_name>
<surname>Taylor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joanne</given_name>
<surname>Cleland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anja</given_name>
<surname>Kuschmann</surname>
</person_name>
					</contributors>
					<titles><title>A semi-automatic pipeline for transcribing and segmenting child speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4278</first_page>
						<last_page>4282</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1030</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/christodoulidou25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Griffin</given_name>
<surname>Smith</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dianna</given_name>
<surname>Yee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer King</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leah</given_name>
<surname>Findlater</surname>
</person_name>
					</contributors>
					<titles><title>Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1943</first_page>
						<last_page>1947</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1031</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/smith25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Ning</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andros</given_name>
<surname>Tjandra</surname>
</person_name>
					</contributors>
					<titles><title>Audiobox TTA-RAG: Improving Zero-Shot and Few-Shot Text-To-Audio with Retrieval-Augmented Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1243</first_page>
						<last_page>1247</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1032</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Noe</given_name>
<surname>Berger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name>
					</contributors>
					<titles><title>Non-Standard Accent TTS Support via Large Multi-Accent Frontend Pronunciation Knowledge Transfer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2530</first_page>
						<last_page>2534</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1034</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/berger25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lucía</given_name>
<surname>Gómez-Zaragozá</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Marín-Morales</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mariano</given_name>
<surname>Alcañiz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Soleymani</surname>
</person_name>
					</contributors>
					<titles><title>Speech and Text Foundation Models for Depression Detection: Cross-Task and Cross-Language Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5253</first_page>
						<last_page>5257</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1035</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gomezzaragoza25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Julian</given_name>
<surname>Zapata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lara</given_name>
<surname>Hanna</surname>
</person_name>
					</contributors>
					<titles><title>Text Entry for All: Towards Speech-based Multimodal Interaction for Inclusion, Accessibility and the Preservation of the World’s Linguistic Heritage </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1818</first_page>
						<last_page>1822</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1036</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zapata25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kun</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siva</given_name>
<surname>Penke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivasa</given_name>
<surname>Algubelli</surname>
</person_name>
					</contributors>
					<titles><title>VoiceNet: Multilingual On-Device Phoneme-To-Audio Alignment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2260</first_page>
						<last_page>2264</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1037</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jin25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Harry</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kurt</given_name>
<surname>Partridge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pai</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Neng</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyun Jin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dhruuv</given_name>
<surname>Agarwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quan</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>GraphemeAug: A Systematic Approach to Synthesized Hard Negative Keyword Spotting Examples</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2680</first_page>
						<last_page>2684</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1038</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haley</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dani</given_name>
<surname>Byrd</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khalil</given_name>
<surname>Iskarous</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis</given_name>
<surname>Goldstein</surname>
</person_name>
					</contributors>
					<titles><title>Instantaneous changes in acoustic signals reflect syllable progression and cross-linguistic syllable variation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>96</first_page>
						<last_page>100</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1040</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hsu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyo Jin</given_name>
<surname>Jon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longbin</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyuntaek</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyunseo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donghun</given_name>
<surname>Min</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eun Yi</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>MATER: Multi-level Acoustic and Textual Emotion Representation for Interpretable Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4673</first_page>
						<last_page>4677</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1041</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jon25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>C. T. Justine</given_name>
<surname>Hui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jenice</given_name>
<surname>Kuzhikombil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabella</given_name>
<surname>Shields</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiraia</given_name>
<surname>Haami-Wells</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine I.</given_name>
<surname>Watson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter [J.]</given_name>
<surname>Keegan</surname>
</person_name>
					</contributors>
					<titles><title>Perception of Long and Short Vowel Contrast in Te Reo Māori in Clean and Everyday Listening Environments</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2305</first_page>
						<last_page>2309</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1043</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hui25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Ong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sean</given_name>
<surname>Robertson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leo</given_name>
<surname>Peckham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alba</given_name>
<surname>Jorquera Jimenez de Aberasturi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paula</given_name>
<surname>Arkhangorodsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Huo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aman</given_name>
<surname>Sakhardande</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hallap</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naomi</given_name>
<surname>Nagy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewan</given_name>
<surname>Dunbar</surname>
</person_name>
					</contributors>
					<titles><title>The Faetar Speech Recognition Benchmark</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4028</first_page>
						<last_page>4032</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1045</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ong25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tara</given_name>
<surname>McAllister</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Collin</given_name>
<surname>Eagen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Shan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Traver</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daphna</given_name>
<surname>Harel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tae Hong</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vesna</given_name>
<surname>Novak</surname>
</person_name>
					</contributors>
					<titles><title>Web-Based Application for Real-Time Biofeedback of Vocal Resonance in Gender-Affirming Voice Training: Design and Usability Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>664</first_page>
						<last_page>668</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1050</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mcallister25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Komatsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hokuto</given_name>
<surname>Munakata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuchi</given_name>
<surname>Ishikawa</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Unlabeled Audio for Audio-Text Contrastive Learning via Audio-Composed Text Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2600</first_page>
						<last_page>2604</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1053</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/komatsu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuchi</given_name>
<surname>Ishikawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Nakada</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hokuto</given_name>
<surname>Munakata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazuhiro</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Komatsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshimitsu</given_name>
<surname>Aoki</surname>
</person_name>
					</contributors>
					<titles><title>Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2605</first_page>
						<last_page>2609</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1054</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ishikawa25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wooil</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bongsu</given_name>
<surname>Jung</surname>
</person_name>
					</contributors>
					<titles><title>DLF-EEND: Dynamic Layer Fusion for End-to-End Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1688</first_page>
						<last_page>1692</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1059</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad</given_name>
<surname>Shakeel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yui</given_name>
<surname>Sudo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinchuan</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chyi-Jiunn</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2225</first_page>
						<last_page>2229</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1062</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/peng25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiacheng</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanfu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Gao</surname>
</person_name>
					</contributors>
					<titles><title>CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization via Soft Prompt Tuning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4498</first_page>
						<last_page>4502</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1064</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tünde</given_name>
<surname>Szalay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Proctor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amelia</given_name>
<surname>Gully</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tharinda</given_name>
<surname>Piyadasa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Craig</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Waddington</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naeim</given_name>
<surname>Sanaei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheryl</given_name>
<surname>Foster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kirrie</given_name>
<surname>Ballard</surname>
</person_name>
					</contributors>
					<titles><title>Lateral Channel Formation in Australian English /l/: Insights from Magnetic Resonance Imaging</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3489</first_page>
						<last_page>3493</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1065</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/szalay25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wanli</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Ragni</surname>
</person_name>
					</contributors>
					<titles><title>Score-Based Training for Energy-Based TTS Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5528</first_page>
						<last_page>5532</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1066</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sun25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiabo</given_name>
<surname>Jing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijian</given_name>
<surname>Ou</surname>
</person_name>
					</contributors>
					<titles><title>A Joint Network for Singing Melody Extraction from Polyphonic Music with Attention Aggregation and Self-Consistency Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3100</first_page>
						<last_page>3104</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1070</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jing25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Taesoo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongsik</given_name>
<surname>Jo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyunmin</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taehwan</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4828</first_page>
						<last_page>4832</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1075</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongbin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Om</given_name>
<surname>Thakkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhradeep</given_name>
<surname>Thakurta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Differentially Private Parameter-Efficient Fine-tuning for Large ASR Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1523</first_page>
						<last_page>1527</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1076</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Mena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pol</given_name>
<surname>Serra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jacobo</given_name>
<surname>Romero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abir</given_name>
<surname>Messaoudi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose</given_name>
<surname>Giraldo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carme</given_name>
<surname>Armentano-Oller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rodolfo</given_name>
<surname>Zevallos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Meza</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Hernando</surname>
</person_name>
					</contributors>
					<titles><title>Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4298</first_page>
						<last_page>4302</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1078</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mena25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takafumi</given_name>
<surname>Moriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masato</given_name>
<surname>Mimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiyoaki</given_name>
<surname>Matsui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Matsuura</surname>
</person_name>
					</contributors>
					<titles><title>Attention-Free Dual-Mode ASR with Latency-Controlled Selective State Spaces</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3588</first_page>
						<last_page>3592</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1079</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/moriya25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaoquan</given_name>
<surname>Ke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Wai</given_name>
<surname>Mak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Optimizing Pause Context in Fine-Tuning Pre-trained Large Language Models for Dementia Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1408</first_page>
						<last_page>1412</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1080</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ke25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Desheng</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Xiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhui</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinkang</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Normalization and Content Restoration for Zero-Shot Voice Conversion with Attention-Enhanced Discriminator</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1403</first_page>
						<last_page>1407</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1081</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hu25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thanathai</given_name>
<surname>Lertpetchpun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tiantian</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dani</given_name>
<surname>Byrd</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Developing a High-performance Framework for Speech Emotion Recognition in Naturalistic Conditions Challenge for Emotional Attribute Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4648</first_page>
						<last_page>4652</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1082</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lertpetchpun25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joun Yeop</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sangjun</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byoung Jin</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hyun</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min-Kyung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoon-Young</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Streaming TTS Acoustic Model with Depthwise RVQ Decoding Strategies in a Mamba Framework</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5513</first_page>
						<last_page>5517</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1084</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lee25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yongjie</given_name>
<surname>Si</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanxiong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiaxin</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qianhua</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Il-Youp</given_name>
<surname>Kwak</surname>
</person_name>
					</contributors>
					<titles><title>Fully Few-shot Class-incremental Audio Classification Using Multi-level Embedding Extractor and Ridge Regression Classifier</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1318</first_page>
						<last_page>1322</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1085</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/si25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anfeng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tiantian</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>So Hyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Somer</given_name>
<surname>Bishop</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lord</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Large Language Models based ASR Error Correction for Child Conversations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2840</first_page>
						<last_page>2844</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1088</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xu25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Sheng</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ching-Yu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsing-Hang</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya-Tse</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo-Hao</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Defend for Self-Vocoding: A Novel Enhanced Decoder Network for Watermark Recovery</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5098</first_page>
						<last_page>5102</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1091</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lin25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wen</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuechen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>From Sharpness to Better Generalization for Speech Deepfake Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5338</first_page>
						<last_page>5342</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1095</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/huang25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenguang</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaqian</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fulin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoxue</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yao</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yingying</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shilei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junlan</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>Privacy-Preserving Speaker Verification via End-to-End Secure Representation Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1508</first_page>
						<last_page>1512</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1096</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hu25j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tadashi</given_name>
<surname>Ogura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuma</given_name>
<surname>Okamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yamato</given_name>
<surname>Ohtani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erica</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>GST-BERT-TTS: Prosody Prediction Without Accentual Labels For Multi-Speaker TTS Using BERT With Global Style Tokens</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>444</first_page>
						<last_page>448</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1098</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ogura25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junyu</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanxiong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haolin</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Infant Cry Emotion Recognition Using Improved ECAPA-TDNN with Multi-scale Feature Fusion and Attention Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4478</first_page>
						<last_page>4482</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1100</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhou25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hsing-Hang</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun-Shao</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ching-Chin</given_name>
<surname>Sung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>ZSDEVC: Zero-Shot Diffusion-based Emotional Voice Conversion with Disentangled Mechanism</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4398</first_page>
						<last_page>4402</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1101</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chou25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maliha</given_name>
<surname>Jahan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yinglun</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyam</given_name>
<surname>Mazumdar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zsuzsanna</given_name>
<surname>Fagyal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Thebaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesus</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano Moro</given_name>
<surname>Velazquez</surname>
</person_name>
					</contributors>
					<titles><title>FaiST: A Benchmark Dataset for Fairness in Speech Technology</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1343</first_page>
						<last_page>1347</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1102</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jahan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bhasi</given_name>
<surname>K.C.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajeev</given_name>
<surname>Rajan</surname>
</person_name>
					</contributors>
					<titles><title>A Siamese Network-Based Framework for Voice Mimicry Proficiency Assessment Using X-Vector Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1668</first_page>
						<last_page>1672</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1103</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kc25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chris</given_name>
<surname>Emezue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>The NaijaVoices Community</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Busayo</given_name>
<surname>Awobade</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abraham Toluwase</given_name>
<surname>Owodunni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Handel</given_name>
<surname>Emezue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gloria Monica Tobechukwu</given_name>
<surname>Emezue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nefertiti Nneoma</given_name>
<surname>Emezue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sewade</given_name>
<surname>Ogun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bunmi</given_name>
<surname>Akinremi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David Ifeoluwa</given_name>
<surname>Adelani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chris</given_name>
<surname>Pal</surname>
</person_name>
					</contributors>
					<titles><title>The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1338</first_page>
						<last_page>1342</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1104</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/emezue25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Boya</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wentao</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>FFD: Fine-Finger Diffusion Model for Music to Fine-grained Finger Dance Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>186</first_page>
						<last_page>190</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1105</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dong25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiwei</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenpeng</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hankun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5018</first_page>
						<last_page>5022</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1106</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/guo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minh</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debjyoti</given_name>
<surname>Paul</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yutong</given_name>
<surname>Pang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laxmi</given_name>
<surname>Pandey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinxi</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuedong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Lei</surname>
</person_name>
					</contributors>
					<titles><title>R2S: Real-to-Synthetic Representation Learning for Training Speech Recognition Models on Synthetic Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3194</first_page>
						<last_page>3198</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1109</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tran25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fengyuan</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian C. J.</given_name>
<surname>Moore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huiyong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengshi</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>L3C-DeepMFC: Low-Latency Low-Complexity Deep Marginal Feedback Cancellation with Closed-Loop Fine Tuning for Hearing Aids</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>838</first_page>
						<last_page>842</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1111</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hao25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youjun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoning</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guinan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengxi</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huimeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Towards LLM-Empowered Fine-Grained Speech Descriptors for Explainable Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4633</first_page>
						<last_page>4637</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1112</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhichao</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yueteng</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Songjun</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiulin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qun</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>MPE-TTS: Customized Emotion Zero-Shot Text-To-Speech Using Multi-Modal Prompt</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4403</first_page>
						<last_page>4407</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1115</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wu25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuto</given_name>
<surname>Kondo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirokazu</given_name>
<surname>Kameoka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kou</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuhiro</given_name>
<surname>Kaneko</surname>
</person_name>
					</contributors>
					<titles><title>JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking Styles</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4783</first_page>
						<last_page>4787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1116</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kondo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muhammad Yeza</given_name>
<surname>Baihaqi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angel García</given_name>
<surname>Contreras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seiya</given_name>
<surname>Kawano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichiro</given_name>
<surname>Yoshino</surname>
</person_name>
					</contributors>
					<titles><title>Rapport-Building Dialogue Strategies for Deeper Connection: Integrating Proactive Behavior, Personalization, and Aizuchi Backchannels</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1083</first_page>
						<last_page>1087</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1117</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/baihaqi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaichen</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinpeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Whisper-Based Multilingual Alzheimer's Disease Detection and Improvements for Low-Resource Language</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>549</first_page>
						<last_page>553</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1118</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jia25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuchen</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenxing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rilin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Video-to-Audio Generation with Fine-grained Temporal Semantics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4223</first_page>
						<last_page>4227</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1119</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hu25k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anqi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-yin</given_name>
<surname>Hsu</surname>
</person_name>
					</contributors>
					<titles><title>When focus shapes the flow: prosodic restructuring in Mandarin complex nominals</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>344</first_page>
						<last_page>348</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1121</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xu25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masaya</given_name>
<surname>Kawamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Hasumi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuma</given_name>
<surname>Shirahata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name>
					</contributors>
					<titles><title>BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5538</first_page>
						<last_page>5542</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1122</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kawamura25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Genzo</given_name>
<surname>Miyahara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsuneo</given_name>
<surname>Kato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akihiro</given_name>
<surname>Tamura</surname>
</person_name>
					</contributors>
					<titles><title>Stuttering Detection Based on Self-Attention Weights of Temporal Acoustic Vector Sequence</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5298</first_page>
						<last_page>5302</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1124</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/miyahara25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhe-chen</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bharath</given_name>
<surname>Chandrasekaran</surname>
</person_name>
					</contributors>
					<titles><title>Extended High-frequency Cues to Phoneme Recognition: Insights from ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1038</first_page>
						<last_page>1042</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1125</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/guo25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingjing</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zijian</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Zeyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eugen</given_name>
<surname>Beck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title>Dynamic Acoustic Model Architecture Optimization in Training for ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3603</first_page>
						<last_page>3607</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1126</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xu25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bence Mark</given_name>
<surname>Halpern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Tienkamp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Teja</given_name>
<surname>Rebernik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rob J.J.H.</given_name>
<surname>van Son</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martijn</given_name>
<surname>Wieling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Defne</given_name>
<surname>Abur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Relationship between objective and subjective perceptual measures of speech in individuals with head and neck cancer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3733</first_page>
						<last_page>3737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1127</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/halpern25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Fei</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>Universal Preference-Score-based Pairwise Speech Quality Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2345</first_page>
						<last_page>2349</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1131</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shi25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaya</given_name>
<surname>Narain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vasudha</given_name>
<surname>Kowtha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Colin</given_name>
<surname>Lea</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lauren</given_name>
<surname>Tooley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dianna</given_name>
<surname>Yee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikramjit</given_name>
<surname>Mitra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zifang</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miquel</given_name>
<surname>Espi Marques</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Avendano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shirley</given_name>
<surname>Ren</surname>
</person_name>
					</contributors>
					<titles><title>Voice Quality Dimensions as Interpretable Primitives for Speaking Style for Atypical Speech and Affect</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4628</first_page>
						<last_page>4632</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1133</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/narain25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shumeng</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangfan</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>Age-related changes in multisensory integration of emotions in an audiovisual face-prosody-semantics Stroop task</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3344</first_page>
						<last_page>3348</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1136</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lin25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiarui</given_name>
<surname>Hai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenxing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mounya</given_name>
<surname>Elhilali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>EzAudio: Enhancing Text-to-Audio Generation with Efficient Diffusion Transformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4233</first_page>
						<last_page>4237</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1137</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hai25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuchen</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yucong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Pre-trained models on Ultrasound Modeling for Mice Autism Detection with Uniform Filter Bank and Attentive Scoring</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1713</first_page>
						<last_page>1717</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1139</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/song25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prabhav</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesus</given_name>
<surname>Villalba</surname>
</person_name>
					</contributors>
					<titles><title>EmoJudge: LLM Based Post-Hoc Refinement for Multimodal Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4703</first_page>
						<last_page>4707</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1141</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/singh25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dengjian</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianghan</given_name>
<surname>Hai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sijia</given_name>
<surname>Liao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue Ivan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kainam Thomas</given_name>
<surname>Wong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiujuan</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Detection of UAV Abnormality Using One Ground-Based Acoustic Vector Sensor</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3394</first_page>
						<last_page>3398</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1142</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhou25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Han</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rohan Kumar</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jisheng</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haohe</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenwu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark D</given_name>
<surname>Plumbley</surname>
</person_name>
					</contributors>
					<titles><title>EnvSDD: Benchmarking Environmental Sound Deepfake Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>201</first_page>
						<last_page>205</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1143</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yin25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minu</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kangwook</given_name>
<surname>Jang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoirin</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>ParaNoise-SV: Integrated Approach for Noise-Robust Speaker Verification with Parallel Joint Learning of Speech Enhancement and Noise Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1103</first_page>
						<last_page>1107</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1145</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuya</given_name>
<surname>Hasumi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name>
					</contributors>
					<titles><title>DnR-nonverbal: Cinematic Audio Source Separation DatasetContaining Non-Verbal Sounds</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4993</first_page>
						<last_page>4997</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1148</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hasumi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Samuel J.</given_name>
<surname>Broughton</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lahiru</given_name>
<surname>Samarakoon</surname>
</person_name>
					</contributors>
					<titles><title>Pushing the Limits of End-to-End Diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5218</first_page>
						<last_page>5222</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1150</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/broughton25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yubin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prakash</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziwei</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haley</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krishna</given_name>
<surname>Nayak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis</given_name>
<surname>Goldstein</surname>
</person_name>
					</contributors>
					<titles><title>Co-registration of real-time MRI and respiration for speech research </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>983</first_page>
						<last_page>987</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1151</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tao</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guinan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Regularized Federated Learning for Privacy-Preserving Dysarthric and Elderly Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2103</first_page>
						<last_page>2107</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1152</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhong25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinxin</given_name>
<surname>Ji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiying</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gang</given_name>
<surname>Peng</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Features of Mandarin Tone Production in Noise: A Comparison Between Chinese Native Speakers and Korean L2 Learners</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4448</first_page>
						<last_page>4452</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1159</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ji25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Long-Khanh</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thanh V. T.</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minh-Tan</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Van</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>RESOUND: Speech Reconstruction from Silent Videos via Acoustic-Semantic Decomposed Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5613</first_page>
						<last_page>5617</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1160</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pham25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Itay</given_name>
<surname>Ben-Dom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine I.</given_name>
<surname>Watson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clare M.</given_name>
<surname>McCann</surname>
</person_name>
					</contributors>
					<titles><title>Introducing EMOPARKNZ: the Emotional Speech Database from New Zealand English Speakers with Parkinson’s Disease</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1873</first_page>
						<last_page>1877</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1162</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bendom25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tiantian</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thanathai</given_name>
<surname>Lertpetchpun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dani</given_name>
<surname>Byrd</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Developing a Top-tier Framework in Naturalistic Conditions Challenge for Categorized Emotion Prediction: From Speech Foundation Models and Learning Objective to Data Augmentation and Engineering Choices</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4643</first_page>
						<last_page>4647</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1163</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/feng25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aswin Shanmugam</given_name>
<surname>Subramanian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harveen</given_name>
<surname>Chadha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikas</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shubham</given_name>
<surname>Bansal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rupeshkumar</given_name>
<surname>Mehta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Length Aware Speech Translation for Video Dubbing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>6</first_page>
						<last_page>10</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1166</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/subramanian25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zixia</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ronny</given_name>
<surname>Ibrahim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joshua</given_name>
<surname>Penney</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Felicity</given_name>
<surname>Cox</surname>
</person_name>
					</contributors>
					<titles><title>Creaky Voice Facilitates More Efficient Phonological Processing of Mandarin Tone 3</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1303</first_page>
						<last_page>1307</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1168</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fan25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaobin</given_name>
<surname>Rong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianchi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhibin</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>A Lightweight Hybrid Dual Channel Speech Enhancement System under Low-SNR Conditions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1178</first_page>
						<last_page>1182</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1171</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fengming</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chien-Jer</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monica</given_name>
<surname>Nesbitt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuju</given_name>
<surname>Shi</surname>
</person_name>
					</contributors>
					<titles><title>Neutral Tone Variation in Beijing Mandarin: Is Neutral Tone Toneless?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>694</first_page>
						<last_page>698</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1173</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dong25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingting</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keyi</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinran</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Su-Jing</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Synthetic Dysarthric Speech: A Supplement, Not a Substitute for Authentic Data in Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2755</first_page>
						<last_page>2759</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1174</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chidambar</given_name>
<surname>B</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanumanth Rao</given_name>
<surname>Naidu</surname>
</person_name>
					</contributors>
					<titles><title>Structured Codebook Based Hierarchical Framework for DNN for Computationally Efficient Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>76</first_page>
						<last_page>80</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1176</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/b25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pengjie</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xueliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong-Qiu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>ARiSE: Auto-Regressive Multi-Channel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1183</first_page>
						<last_page>1187</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1178</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shen25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Patawee</given_name>
<surname>Prakrankamanant</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekapol</given_name>
<surname>Chuangsuwanich</surname>
</person_name>
					</contributors>
					<titles><title>Explainable Depression Detection using Masked Hard Instance Mining</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>474</first_page>
						<last_page>478</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1181</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/prakrankamanant25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masakazu</given_name>
<surname>Inoue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Motoshige</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenichi</given_name>
<surname>Tomeoka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathania</given_name>
<surname>Nah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eri</given_name>
<surname>Hatakeyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Arulkumaran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilya</given_name>
<surname>Horiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuntaro</given_name>
<surname>Sasai</surname>
</person_name>
					</contributors>
					<titles><title>A Silent Speech Decoding System from EEG and EMG with Heterogenous Electrode Configurations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5603</first_page>
						<last_page>5607</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1183</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/inoue25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinchuan</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shikhar</given_name>
<surname>Bharadwaj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takashi</given_name>
<surname>Maekaku</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Shinohara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keita</given_name>
<surname>Goto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huck</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>OpusLM: A Family of Open Unified Speech Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3259</first_page>
						<last_page>3263</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1184</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tian25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tong</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoke</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Lv</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cunhang</given_name>
<surname>Fan</surname>
</person_name>
					</contributors>
					<titles><title>SSF-DST: A Spectro-Spatial Features Enhanced Deep Spatiotemporal Network for EEG-Based Auditory Attention Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1048</first_page>
						<last_page>1052</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1187</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhu25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gaoyang</given_name>
<surname>Dong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhicheng</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ping</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minghui</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Adaptive Differential Denoising for Respiratory Sounds Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1008</first_page>
						<last_page>1012</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1190</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dong25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kenichi</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shota</given_name>
<surname>Horiguchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Ijima</surname>
</person_name>
					</contributors>
					<titles><title>Voice Impression Control in Zero-Shot TTS</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4363</first_page>
						<last_page>4367</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1192</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fujita25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masao</given_name>
<surname>Someki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shikhar</given_name>
<surname>Bharadwaj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atharva Anand</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chyi-Jiunn</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinchuan</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Markus</given_name>
<surname>Müller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nathan</given_name>
<surname>Susanj</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Context-Driven Dynamic Pruning for Large Speech Foundation Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1993</first_page>
						<last_page>1997</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1193</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/someki25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shreeram Suresh</given_name>
<surname>Chandra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Goncalves</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junchen</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name>
					</contributors>
					<titles><title>EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3000</first_page>
						<last_page>3004</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1198</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chandra25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaowen</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yao</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Self-Improvement for Audio Large Language Model using Unlabeled Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>46</first_page>
						<last_page>50</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1200</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peidong</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiyu</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Disentangling Dual-Encoder Masked Autoencoder for Respiratory Sound Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1013</first_page>
						<last_page>1017</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1209</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wei25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaosu</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>BoWen</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaowei</given_name>
<surname>Yi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yun</given_name>
<surname>Cao</surname>
</person_name>
					</contributors>
					<titles><title>DiffEmotionVC: A Dual-Granularity Disentangled Diffusion Framework for Any-to-Any Emotional Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4393</first_page>
						<last_page>4397</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1210</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/su25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Littell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aidan</given_name>
<surname>Pine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robbie</given_name>
<surname>Jimerson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gilles</given_name>
<surname>Boulianne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vishwa</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rolando</given_name>
<surname>Coto-Solano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Kazantseva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Tessier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Delaney</given_name>
<surname>Lothian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akwiratékha'</given_name>
<surname>Martin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>Joanis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuel</given_name>
<surname>Larkin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roland</given_name>
<surname>Kuhn</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Speech Foundation Models for Automatic Speech Recognition in the Low-Resource Kanyen’kéha Language</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2063</first_page>
						<last_page>2067</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1215</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/geng25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tiantian</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anfeng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Somer</given_name>
<surname>Bishop</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>Egocentric Speaker Classification in Child-Adult Dyadic Interactions: From Sensing to Computational Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2835</first_page>
						<last_page>2839</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1216</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/feng25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiheng</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Temporal organization of prenuclear glides in Hefei Mandarin</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4768</first_page>
						<last_page>4772</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1219</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ankit</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Munir</given_name>
<surname>Georges</surname>
</person_name>
					</contributors>
					<titles><title>DRI-GAN: A Novel Dual Real Input GAN with Triplet Loss for Cross-Lingual and Noisy SLU</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4118</first_page>
						<last_page>4122</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1220</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kumar25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chanwoo</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna Seo Gyeong</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunghye</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chanwoo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Reasoning-Based Approach with Chain-of-Thought for Alzheimer’s Detection Using Speech and Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2185</first_page>
						<last_page>2189</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1226</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/park25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Debarpan</given_name>
<surname>Bhattacharya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Apoorva</given_name>
<surname>Kulkarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2068</first_page>
						<last_page>2072</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1228</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bhattacharya25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yihan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leying</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>E2E-BPVC: End-to-End Background-Preserving Voice Conversion via In-Context Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1378</first_page>
						<last_page>1382</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1229</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongfei</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sidi</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gang</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Gao</surname>
</person_name>
					</contributors>
					<titles><title>EAA: Emotion-Aware Audio Large Language Models with Dual Cross-Attention and Context-Aware Instruction Tuning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5433</first_page>
						<last_page>5437</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1232</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/du25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minji</given_name>
<surname>Ryu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hyeon</given_name>
<surname>Hur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung Heuk</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gahgene</given_name>
<surname>Gweon</surname>
</person_name>
					</contributors>
					<titles><title>Pitch Contour Model (PCM) with Transformer Cross-Attention for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4353</first_page>
						<last_page>4357</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1233</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ryu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aurosweta</given_name>
<surname>Mahapatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ismail R.</given_name>
<surname>Ulgen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abinay</given_name>
<surname>Reddy Naini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name>
					</contributors>
					<titles><title>Can Emotion Fool Anti-spoofing?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5628</first_page>
						<last_page>5632</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1234</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mahapatra25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeongsoo</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhikang</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hoon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chunhui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Accelerating Diffusion-based Text-to-Speech Model Trainingwith Dual Modality Alignment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3459</first_page>
						<last_page>3463</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1236</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/choi25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mengxue</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianxin</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiewen</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Pitch Target Realization in Putonghua Tone Production of Children from Dialect-Speaking Regions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4458</first_page>
						<last_page>4462</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1237</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cao25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Prabash Reddy</given_name>
<surname>Male</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swayambhu Nath</given_name>
<surname>Ray</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harish</given_name>
<surname>Arsikere</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akshat</given_name>
<surname>Jaiswal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prakhar</given_name>
<surname>Swarup</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prantik</given_name>
<surname>Sen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debmalya</given_name>
<surname>Chakrabarty</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K V Vijay</given_name>
<surname>Girish</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikhil</given_name>
<surname>Bhave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frederick</given_name>
<surname>Weber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sambuddha</given_name>
<surname>Bhattacharya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sri</given_name>
<surname>Garimella</surname>
</person_name>
					</contributors>
					<titles><title>DuRep: Dual-Mode Speech Representation Learning via ASR-Aware Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5808</first_page>
						<last_page>5812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1242</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/male25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuta</given_name>
<surname>Hirano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name>
					</contributors>
					<titles><title>SC-SOT: Conditioning the Decoder on Diarized Speaker Information for End-to-End Overlapped Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4923</first_page>
						<last_page>4927</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1243</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hirano25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sneha</given_name>
<surname>Raman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preeti</given_name>
<surname>Rao</surname>
</person_name>
					</contributors>
					<titles><title>Oral Reading Errors by Grade 3 Children in Indian Schools: A Hindi-English Perspective</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2860</first_page>
						<last_page>2864</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1245</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/raman25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wangyou</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Saijo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuele</given_name>
<surname>Cornell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Scheibler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenda</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoheng</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Sach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yihui</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Fingscheidt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Lessons Learned from the URGENT 2024 Speech Enhancement Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>853</first_page>
						<last_page>857</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1246</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yunkee</given_name>
<surname>Chae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eunsik</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suntae</given_name>
<surname>Hwang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seungryeol</given_name>
<surname>Paik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyogu</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1273</first_page>
						<last_page>1277</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1247</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chae25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yunkee</given_name>
<surname>Chae</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyogu</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Towards Bitrate-Efficient and Noise-Robust Speech Coding with Variable Bitrate RVQ</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>609</first_page>
						<last_page>613</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1253</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chae25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>King Yiu</given_name>
<surname>Suen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rudolf</given_name>
<surname>Chow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert Y.S.</given_name>
<surname>Lam</surname>
</person_name>
					</contributors>
					<titles><title>Cantonese Punctuation Restoration using LLM Annotated Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1953</first_page>
						<last_page>1957</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1254</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/suen25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Potsawee</given_name>
<surname>Manakul</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangzhi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Warit</given_name>
<surname>Sirichotedumrong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kasima</given_name>
<surname>Tharnpipitchai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kunat</given_name>
<surname>Pipatanakul</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2083</first_page>
						<last_page>2087</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1258</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/manakul25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ming</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cancan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Channel Sequence-to-Sequence Neural Diarization: Experimental Results for The MISP 2025 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1898</first_page>
						<last_page>1902</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1262</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cheng25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chengxi</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengzhe</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yicong</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiankun</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guinan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youjun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huimeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoning</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>MOPSA: Mixture of Prompt-Experts Based Speaker Adaptation for Elderly Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4933</first_page>
						<last_page>4937</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1263</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/deng25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Klein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemlata</given_name>
<surname>Tak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elie</given_name>
<surname>Khoury</surname>
</person_name>
					</contributors>
					<titles><title>Open-Set Source Tracing of Audio Deepfake Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1578</first_page>
						<last_page>1582</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1269</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/klein25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xingwei</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heinrich</given_name>
<surname>Dinkel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yadong</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linzhang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junbo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Luan</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Speech Enhancement via Embeddings from Pre-trained Generative Audioencoders</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4848</first_page>
						<last_page>4852</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1270</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sun25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingyu</given_name>
<surname>Cui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiawen</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shujie</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoqing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiliang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1203</first_page>
						<last_page>1207</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1280</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cui25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Drishya</given_name>
<surname>Uniyal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinayak</given_name>
<surname>Abrol</surname>
</person_name>
					</contributors>
					<titles><title>From Pretraining to Performance: Benchmarking Self-Supervised Speech Models for Interspeech-25 SER Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4638</first_page>
						<last_page>4642</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1283</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/uniyal25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Longbin</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donghun</given_name>
<surname>Min</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jung Eun</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eun Yi</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Contrastive Learning-based Syllable-Level Mispronunciation Detection and Diagnosis for Speech Audiometry</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>828</first_page>
						<last_page>832</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1285</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jin25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xingyuan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kenny</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengyue</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Dog2vec: Self-Supervised Pre-Training for Canine Vocal Representation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1698</first_page>
						<last_page>1702</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1287</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hanglei</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiwei</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Hao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5003</first_page>
						<last_page>5007</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1289</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chin Yuen</given_name>
<surname>Kwok</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia Qi</given_name>
<surname>Yip</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>659</first_page>
						<last_page>663</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1290</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kwok25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alan</given_name>
<surname>Dao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dinh Bach</given_name>
<surname>Vu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huy Hoang</given_name>
<surname>Ha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuan Le Duc</given_name>
<surname>Anh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shreyas</given_name>
<surname>Gopal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue Heng</given_name>
<surname>Yeo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Warren Keng Hoong</given_name>
<surname>Low</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eng Siong</given_name>
<surname>Chng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia Qi</given_name>
<surname>Yip</surname>
</person_name>
					</contributors>
					<titles><title>Speechless: Speech Instruction Training Without Speech for Low Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3239</first_page>
						<last_page>3243</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1292</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dao25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Miika</given_name>
<surname>Toikkanen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>June-Woo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1023</first_page>
						<last_page>1027</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1295</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/toikkanen25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuanjun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>I-Ming</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiawei</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haibin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hung-yi</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jyh-Shing Roger</given_name>
<surname>Jang</surname>
</person_name>
					</contributors>
					<titles><title>Codec-Based Deepfake Source Tracing via Neural Audio Codec Taxonomy</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1538</first_page>
						<last_page>1542</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1297</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chikara</given_name>
<surname>Maeda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muhammad</given_name>
<surname>Shakeel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yui</given_name>
<surname>Sudo</surname>
</person_name>
					</contributors>
					<titles><title>Joint Target-Speaker ASR and Activity Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1683</first_page>
						<last_page>1687</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1299</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/maeda25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Nakagome</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Hentschel</surname>
</person_name>
					</contributors>
					<titles><title>WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard CTC-based Keyword Spotting and Inter-layer Biasing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5178</first_page>
						<last_page>5182</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1300</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/nakagome25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dashanka</given_name>
<surname>Da Silva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurav</given_name>
<surname>Pahuja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>NeuroSpex+: Dual-Task Training of Neuro-Guided Speaker Extraction with Speech Envelope and Waveform</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5568</first_page>
						<last_page>5572</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1304</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dasilva25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vishal</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinayak</given_name>
<surname>Abrol</surname>
</person_name>
					</contributors>
					<titles><title>ArticulateX: End-to-End Monolingual Speech Translation in Articulator Space</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>11</first_page>
						<last_page>15</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1305</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kumar25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoya</given_name>
<surname>Mizumoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Kojima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianbo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yui</given_name>
<surname>Sudo</surname>
</person_name>
					</contributors>
					<titles><title>AC/DC: LLM-based Audio Comprehension via Dialogue Continuation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2610</first_page>
						<last_page>2614</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1308</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fujita25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ye-Eun</given_name>
<surname>Ko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mun-Hak</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong-Hyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Improving Generalization of End-to-End ASR through Diversity and Independence Regularization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3578</first_page>
						<last_page>3582</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1309</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ko25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seyun</given_name>
<surname>Ahn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pil Moo</given_name>
<surname>Byun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Won-Gook</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Optimizing CLAP Reward with LLM Feedback for Semantically Aligned and Diverse Automated Audio Captioning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3140</first_page>
						<last_page>3144</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1313</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ahn25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoxu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiheng</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gang</given_name>
<surname>Qiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengteng</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Biao</given_name>
<surname>Tian</surname>
</person_name>
					</contributors>
					<titles><title>FLASepformer: Efficient Speech Separation with Gated Focused Linear Attention Transformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1468</first_page>
						<last_page>1472</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1315</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yoshinori</given_name>
<surname>Fukunaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryota</given_name>
<surname>Nishimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kengo</given_name>
<surname>Ohta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Norihide</given_name>
<surname>Kitaoka</surname>
</person_name>
					</contributors>
					<titles><title>Backchannel prediction for natural spoken dialog systems  using general speaker and listener information </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1078</first_page>
						<last_page>1082</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1316</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fukunaga25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bunlong</given_name>
<surname>Lay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rostilav</given_name>
<surname>Makarov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>Diffusion Buffer: Online Diffusion-based Speech Enhancement with Sub-Second Latency</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>793</first_page>
						<last_page>797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1317</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lay25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenxuan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5053</first_page>
						<last_page>5057</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1321</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wu25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu-Ying</given_name>
<surname>Chuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng-Fu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Tonal Variation and Word Meaning in Taiwanese</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4183</first_page>
						<last_page>4187</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1325</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chuang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juan</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shi-Fu</given_name>
<surname>XIong</surname>
</person_name>
					</contributors>
					<titles><title>A Study of Real-world Audio-Visual Corpus Design and Production: A Perspective from MISP Challenges</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3903</first_page>
						<last_page>3907</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1329</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mohammed</given_name>
<surname>Al-Radhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Géza</given_name>
<surname>Németh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Branislav</given_name>
<surname>Gerazov</surname>
</person_name>
					</contributors>
					<titles><title>MiSTR: Multi-Modal iEEG-to-Speech Synthesis with Transformer-Based Prosody Prediction and Neural Phase Reconstruction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2930</first_page>
						<last_page>2934</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1334</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/alradhi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoxiao</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>AS</given_name>
<surname>Madhukumar</surname>
</person_name>
					</contributors>
					<titles><title>LSPnet: an ultra-low bitrate hybrid neural codec</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>614</first_page>
						<last_page>618</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1335</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Long-Vu</given_name>
<surname>Hoang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuan</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huy Dat</given_name>
<surname>Tran</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2625</first_page>
						<last_page>2629</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1336</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hoang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takatomo</given_name>
<surname>Kano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Fukuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Pick and Summarize: Integrating Extractive and Abstractive Speech Summarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>281</first_page>
						<last_page>285</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1341</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kano25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jie</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wang</given_name>
<surname>Xiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cunhang</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Lv</surname>
</person_name>
					</contributors>
					<titles><title>REB-former: RWKV-enhanced E-branchformer for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3868</first_page>
						<last_page>3872</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1343</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/song25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ki-Joong</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun-Ho</given_name>
<surname>So</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sang-Hoon</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Parameter-Efficient Fine-Tuning for Low-Resource Text-to-Speech via Cross-Lingual Continual Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1613</first_page>
						<last_page>1617</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1344</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kwon25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Soo-Whan</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min-Seok</given_name>
<surname>Choi</surname>
</person_name>
					</contributors>
					<titles><title>Listen through the Sound: Generative Speech Restoration Leveraging Acoustic Context Representation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4843</first_page>
						<last_page>4847</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1352</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chung25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Myeonghoon</given_name>
<surname>Ryu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongseok</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suji</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Han</given_name>
<surname>Park</surname>
</person_name>
					</contributors>
					<titles><title>Unified Microphone Conversion: Many-to-Many Device Mapping via Feature-wise Linear Modulation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1333</first_page>
						<last_page>1337</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1356</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ryu25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jing-Tong</given_name>
<surname>Tzeng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bo-Hao</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ya-Tse</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsing-Hang</given_name>
<surname>Chou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi-Chun</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Lessons Learnt: Revisit Key Training Strategies for Effective Speech Emotion Recognition in the Wild</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4708</first_page>
						<last_page>4712</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1357</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tzeng25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bowen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nur Afiqah Abdul</given_name>
<surname>Latiff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justin</given_name>
<surname>Kan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rong</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donny</given_name>
<surname>Soh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoxiao</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name>
					</contributors>
					<titles><title>Automated evaluation of children's speech fluency for low-resource languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1948</first_page>
						<last_page>1952</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1358</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Masato</given_name>
<surname>Mimura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaeyoung</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Switch Conformer with Universal Phonetic Experts for Multilingual ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1128</first_page>
						<last_page>1132</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1359</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mimura25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Panda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunil Kumar</given_name>
<surname>Kopparapu</surname>
</person_name>
					</contributors>
					<titles><title>EmbedAug: An Augmentation Scheme for End-to-End Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3429</first_page>
						<last_page>3433</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1360</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/panda25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoki</given_name>
<surname>Hojo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryoichi</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chihiro</given_name>
<surname>Sugiyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nobukazu</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kanji</given_name>
<surname>Nohara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kazunori</given_name>
<surname>Nozaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuya</given_name>
<surname>Takiguchi</surname>
</person_name>
					</contributors>
					<titles><title>Revisiting WFST-based Hybrid Japanese Speech Recognition System for Individuals with Organic Speech Disorders</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1878</first_page>
						<last_page>1882</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1362</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hojo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Saijo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wangyou</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuele</given_name>
<surname>Cornell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Scheibler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenda</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaoheng</given_name>
<surname>Ni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anurag</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Sach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yihui</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Fingscheidt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Interspeech 2025 URGENT Speech Enhancement Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>858</first_page>
						<last_page>862</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1363</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/saijo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zijing</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liang</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jichen</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>VS-Singer: Vision-Guided Stereo Singing Voice Synthesis with Consistency Schrödinger Bridge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1258</first_page>
						<last_page>1262</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1364</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhao25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Byeong Hyeon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyungseob</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Inseon</given_name>
<surname>Jang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>Towards an Ultra-Low-Delay Neural Audio Coding with Computational Efficiency</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>594</first_page>
						<last_page>598</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1369</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Woongjib</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Byeong Hyeon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyungseob</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Inseon</given_name>
<surname>Jang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong-Goo</given_name>
<surname>Kang</surname>
</person_name>
					</contributors>
					<titles><title>Neural Spectral Band Generation for Audio Coding </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>624</first_page>
						<last_page>628</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1370</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/choi25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zexu</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shengkui</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tingting</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukun</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bin</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1933</first_page>
						<last_page>1937</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1371</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pan25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>June-Woo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wonkyo</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haram</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sung-Hoon</given_name>
<surname>Yoon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dae-Jin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong-Ho</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sang-Yeol</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chan-Mo</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Language-Agnostic Suicidal Risk Detection Using Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>419</first_page>
						<last_page>423</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1372</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shanshan</given_name>
<surname>Xiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hankiz</given_name>
<surname>Yilahun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Askar</given_name>
<surname>Hamdulla</surname>
</person_name>
					</contributors>
					<titles><title>Speech Mutil-label Emotion Recognition Using Asymmetric Class Loss Function Based on Effective Samples</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4483</first_page>
						<last_page>4487</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1373</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xiang25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiahong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiwen</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianheng</given_name>
<surname>Zhuo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenda</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liliang</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Multilingual ASR Finetuning via LoRA Language Experts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1138</first_page>
						<last_page>1142</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1374</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nhan</given_name>
<surname>Phan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kuronen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Kautonen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Riikka</given_name>
<surname>Ullakonoja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>von Zansen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaroslav</given_name>
<surname>Getman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekaterina</given_name>
<surname>Voskoboinik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tamás</given_name>
<surname>Grósz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>Mispronunciation Detection Without L2 Pronunciation Dataset in Low-Resource Setting: A Case Study in Finland Swedish</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2435</first_page>
						<last_page>2439</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1375</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/phan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingping</given_name>
<surname>Nie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tien Dung</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karan</given_name>
<surname>Thakkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vasudha</given_name>
<surname>Kowtha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jon</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Avendano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erdrin</given_name>
<surname>Azemi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikramjit</given_name>
<surname>Mitra</surname>
</person_name>
					</contributors>
					<titles><title>Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2018</first_page>
						<last_page>2022</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1376</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/nie25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katsuhiko</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Miyazaki</surname>
</person_name>
					</contributors>
					<titles><title>Non-Intrusive Binaural Speech Intelligibility Prediction Using Mamba for Hearing-Impaired Listeners</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5463</first_page>
						<last_page>5467</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1377</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yamamoto25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zijian</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minh-Nghia</given_name>
<surname>Phan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ralf</given_name>
<surname>Schlüter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hermann</given_name>
<surname>Ney</surname>
</person_name>
					</contributors>
					<titles><title> Label-Context-Dependent Internal Language Model Estimation for CTC</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5193</first_page>
						<last_page>5197</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1378</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuochao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>D</given_name>
<surname>Shin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hakan</given_name>
<surname>Erdogan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sinan</given_name>
<surname>Hersek</surname>
</person_name>
					</contributors>
					<titles><title>SoundSculpt: Direction and Semantics Driven Ambisonic Target Sound Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>943</first_page>
						<last_page>947</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1379</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Honghong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fanqin</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rong</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Speech Emotion Recognition with Multi-Task Learning and Dynamic Feature Fusion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4713</first_page>
						<last_page>4717</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1380</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peter</given_name>
<surname>Birkholz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Schäfer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Häsner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jihyeon</given_name>
<surname>Yun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iris</given_name>
<surname>Kruppke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rémi</given_name>
<surname>Blandin</surname>
</person_name>
					</contributors>
					<titles><title>Influence of wall coverings of 3D-printed vocal tract models on measured transfer functions </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3479</first_page>
						<last_page>3483</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1381</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/birkholz25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Longjie</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shenghui</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the MISP-Meeting Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1883</first_page>
						<last_page>1887</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1382</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/luo25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lilit</given_name>
<surname>Grigoryan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Bataev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrei</given_name>
<surname>Andrusenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hainan</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vitaly</given_name>
<surname>Lavrukhin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Pushing the Limits of Beam Search Decoding  for Transducer-based ASR models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>649</first_page>
						<last_page>653</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1388</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/grigoryan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Deok-Hyeon</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyung-Seok</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung-Bin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong-Whan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>EmoSphere-SER: Enhancing Speech Emotion Recognition Through Spherical Representation with Auxiliary Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4653</first_page>
						<last_page>4657</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1391</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cho25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Deok-Hyeon</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyung-Seok</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung-Bin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong-Whan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>DiEmo-TTS: Disentangled Emotion Representations via Self-Supervised Distillation for Cross-Speaker Emotion Transfer in Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4373</first_page>
						<last_page>4377</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1394</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cho25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fengyun</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Zou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ning</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaojun</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xiao</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Serialized Output Training for Multi-Talker ASR with Soft Monotonic Alignment and Utterance-level Timestamp</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1603</first_page>
						<last_page>1607</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1396</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joon-Seung</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong-Min</given_name>
<surname>Byun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyung-Seok</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong-Whan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>VibE-SVC: Vibrato Extraction with High-frequency F0 Contour for Singing Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1233</first_page>
						<last_page>1237</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1397</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/choi25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shunsuke</given_name>
<surname>Mitsumori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Kashiwagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keitaro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeo</given_name>
<surname>Morishima</surname>
</person_name>
					</contributors>
					<titles><title>Cross-lingual Data Selection Using Clip-level Acoustic Similarity for Enhancing Low-resource Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3314</first_page>
						<last_page>3318</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1399</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mitsumori25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Owais Mujtaba</given_name>
<surname>Khanday</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo Rodríguez San</given_name>
<surname>Esteban</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zubair Ahmad</given_name>
<surname>Lone</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Ouellet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose A.</given_name>
<surname>Gonzalez-Lopez</surname>
</person_name>
					</contributors>
					<titles><title>Recreating Neural Activity During Speech Production with Language and Speech Model Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5553</first_page>
						<last_page>5557</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1400</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/khanday25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dariia</given_name>
<surname>Puhach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amir H.</given_name>
<surname>Payberah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name>
					</contributors>
					<titles><title>Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2058</first_page>
						<last_page>2062</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1402</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/puhach25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuan-Nam</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc-Quan</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Şeymanur</given_name>
<surname>Akti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Waibel</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4163</first_page>
						<last_page>4167</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1403</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/nguyen25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhou</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huijun</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Hybrid Expert Knowledge and Self-Supervised Learning for Diagnostic Modeling of Adductor Spasmodic and Primary Myotonic Dysphonia</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3543</first_page>
						<last_page>3547</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1406</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/du25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yao</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui-Chen</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui-Peng</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao-Hang</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>Vision-Integrated High-Quality Neural Speech Coding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>619</first_page>
						<last_page>623</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1409</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/guo25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tuan Dat</given_name>
<surname>Phuong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long-Vu</given_name>
<surname>Hoang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huy Dat</given_name>
<surname>Tran</surname>
</person_name>
					</contributors>
					<titles><title>Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5633</first_page>
						<last_page>5637</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1411</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/phuong25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiangzhu</given_name>
<surname>Kong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijian</given_name>
<surname>Ou</surname>
</person_name>
					</contributors>
					<titles><title>Lightweight and Robust Multi-Channel End-to-End Speech Recognition with Spherical Harmonic Transform</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3439</first_page>
						<last_page>3443</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1415</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kong25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chia-Hua</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wanying</given_name>
<surname>Ge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>A Comparative Study on Proactive and Passive Detection  of Deepfake Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5328</first_page>
						<last_page>5332</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1419</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wu25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaoxiang</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shogo</given_name>
<surname>Seki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroaki</given_name>
<surname>Kudo</surname>
</person_name>
					</contributors>
					<titles><title>First Analyze Then Enhance: A Task-Aware System for Speech Separation, Denoising, and Dereverberation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3848</first_page>
						<last_page>3852</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1424</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaocan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiwei</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guibin</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenhao</given_name>
<surname>Jing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tieran</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Knowledge Distillation Method for Pruned RNN-T Models via Pruning Bounds Sharing and Losses Confusion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3608</first_page>
						<last_page>3612</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1425</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Timothy Shin Heng</given_name>
<surname>Mak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>King Yiu</given_name>
<surname>Suen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert Y.S.</given_name>
<surname>Lam</surname>
</person_name>
					</contributors>
					<titles><title>Speech-guided Grapheme-to-Phoneme Conversion for Cantonese Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2535</first_page>
						<last_page>2539</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1428</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mak25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Attia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dorottya</given_name>
<surname>Demszky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>From Weak Labels to Strong Results: Utilizing 5,000 Hours of Noisy Classroom Transcripts with Minimal Accurate Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3678</first_page>
						<last_page>3682</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1432</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/attia25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaohan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingfeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Who, When, and What: Leveraging the ``Three Ws'' Concept for Emotion Recognition in Conversation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1763</first_page>
						<last_page>1767</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1433</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shi25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ishan D.</given_name>
<surname>Biyani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nirmesh J.</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ashishkumar P.</given_name>
<surname>Gudmalwar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pankaj</given_name>
<surname>Wasnik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajiv R.</given_name>
<surname>Shah</surname>
</person_name>
					</contributors>
					<titles><title>REWIND: Speech Time Reversal for Enhancing Speaker Representations in Diffusion-based Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1368</first_page>
						<last_page>1372</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1434</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/biyani25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cheng Hung</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Yasuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akifumi</given_name>
<surname>Yoshimoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Unifying Listener Scoring Scales: Comparison Learning Framework for Speech Quality Assessment and Continuous Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5428</first_page>
						<last_page>5432</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1435</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hu25l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jesujoba O.</given_name>
<surname>Alabi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuechen</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dietrich</given_name>
<surname>Klakow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>AfriHuBERT: A self-supervised speech representation model for African languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4023</first_page>
						<last_page>4027</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1437</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/alabi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lauren</given_name>
<surname>White</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewan</given_name>
<surname>Carr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Judith</given_name>
<surname>Dineley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catarina</given_name>
<surname>Botelho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pauline</given_name>
<surname>Conde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Faith</given_name>
<surname>Matcham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carolin</given_name>
<surname>Oetzmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amos</given_name>
<surname>Folarin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Fairs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Agnes</given_name>
<surname>Norbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefano</given_name>
<surname>Goria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinivasan</given_name>
<surname>Vairavan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Til</given_name>
<surname>Wykes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Dobson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vaibhav</given_name>
<surname>Naraya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Hotopf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name>
					</contributors>
					<titles><title>Speech Reference Intervals: An Assessment of Feasibility in Depression Symptom Severity Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>459</first_page>
						<last_page>463</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1438</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/white25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaohan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingfeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-Aware Multi-Task Learning for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4333</first_page>
						<last_page>4337</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1439</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shi25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Youqiang</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiping</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yueteng</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yike</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>FreeCodec: A Disentangled Neural Speech Codec with Fewer Tokens</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4878</first_page>
						<last_page>4882</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1440</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zheng25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Neta</given_name>
<surname>Glazer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Chernin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Idan</given_name>
<surname>Achituve</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Gannot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ethan</given_name>
<surname>Fetaya</surname>
</person_name>
					</contributors>
					<titles><title>Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2240</first_page>
						<last_page>2244</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1442</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/glazer25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Frederik</given_name>
<surname>Rautenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fritz</given_name>
<surname>Seebauer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jana</given_name>
<surname>Wiechmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Kuhlmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petra</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name>
					</contributors>
					<titles><title>Synthesizing Speech with Selected Perceptual Voice Qualities – A Case Study with Creaky Voice</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1633</first_page>
						<last_page>1637</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1443</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/rautenberg25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiaohan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyi</given_name>
<surname>Mi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingfeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Advancing Emotion Recognition via Ensemble Learning: Integrating Speech, Context, and Text Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4693</first_page>
						<last_page>4697</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1445</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shi25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vahid</given_name>
<surname>Ahmadi Kalkhorani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Buye</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>DeLiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Online AV-CrossNet: a Causal and Efficient Audiovisual System for Speech Enhancement and Target Speaker Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2985</first_page>
						<last_page>2989</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1448</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yu25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nan</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lirong</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name>
					</contributors>
					<titles><title>An Effective Anomalous Sound Detection Method Based on Global and Local Attribute Mining</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2620</first_page>
						<last_page>2624</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1449</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jiang25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Terashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuma</given_name>
<surname>Shirahata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masaya</given_name>
<surname>Kawamura</surname>
</person_name>
					</contributors>
					<titles><title>SLASH: Self-Supervised Speech Pitch Estimation Leveraging DSP-derived Absolute Pitch</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1733</first_page>
						<last_page>1737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1453</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/terashima25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Man</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yixin</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Niels</given_name>
<surname>Schiller</surname>
</person_name>
					</contributors>
					<titles><title>Semantic Processing During Spoken Word Production by Children with Cochlear Implants</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>843</first_page>
						<last_page>847</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1455</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shanshan</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dianlong</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tian</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>SCD-Conformer: Semantic Content Disentanglement for Text-Independent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3628</first_page>
						<last_page>3632</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1456</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yao25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yujie</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bing</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Mel-McNet: A Mel-Scale Framework for Online Multichannel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1173</first_page>
						<last_page>1177</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1462</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengyang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pascal</given_name>
<surname>Reichert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Graave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick</given_name>
<surname>Blumenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Fingscheidt</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Noise-Robust Hybrid Audiovisual Encoder  with Joint Distillation and Pruning for Audiovisual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1833</first_page>
						<last_page>1837</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1464</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Neeraj</given_name>
<surname>Agrawal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Ganapathy</surname>
</person_name>
					</contributors>
					<titles><title>Spoken Language Understanding on Unseen Tasks With In-Context Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4103</first_page>
						<last_page>4107</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1467</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/agrawal25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhe</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wen-Chin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoxiao</given_name>
<surname>Miao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junichi</given_name>
<surname>Yamagishi</surname>
</person_name>
					</contributors>
					<titles><title>Mitigating Language Mismatch in SSL-Based Speaker Anonymization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5133</first_page>
						<last_page>5137</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1469</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mandar</given_name>
<surname>Gogate</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kia</given_name>
<surname>Dashtipour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amir</given_name>
<surname>Hussain</surname>
</person_name>
					</contributors>
					<titles><title>Towards Personalised Audio Visual Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4853</first_page>
						<last_page>4857</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1471</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gogate25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Iddo</given_name>
<surname>Yosha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dorin</given_name>
<surname>Shteyman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yossi</given_name>
<surname>Adi</surname>
</person_name>
					</contributors>
					<titles><title>WhiStress: Enriching Transcriptions with Sentence Stress Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4718</first_page>
						<last_page>4722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1475</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yosha25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Se-Ha</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tae-Gyeong</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang-Jae</given_name>
<surname>Chun</surname>
</person_name>
					</contributors>
					<titles><title>Mamba-based Hybrid Model for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5163</first_page>
						<last_page>5167</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1476</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seung Gyu</given_name>
<surname>Jeong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong Eun</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Patient-Aware Feature Alignment for Robust Lung Sound Classification: Cohesion-Separation and Global Alignment Losses</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1018</first_page>
						<last_page>1022</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1477</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jeong25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kang</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fangkun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaodong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengshi</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>LightL2S: Ultra-Low Complexity Lip-to-Speech Synthesis for Multi-Speaker Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3783</first_page>
						<last_page>3787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1478</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liang25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gilly</given_name>
<surname>Marchini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeremy</given_name>
<surname>Steffman</surname>
</person_name>
					</contributors>
					<titles><title>Data-driven approaches to pitch modelling in two Mexican Spanish ethnolects: K-means Clustering &amp; GAMMs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2940</first_page>
						<last_page>2944</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1479</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/marchini25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Luong</given_name>
<surname>Ho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khanh</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinh</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bao</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tan</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Duc</given_name>
<surname>Chau</surname>
</person_name>
					</contributors>
					<titles><title>Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4433</first_page>
						<last_page>4437</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1480</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ho25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiao</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Long</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hong</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>404</first_page>
						<last_page>408</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1483</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gao25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaito</given_name>
<surname>Takahashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keigo</given_name>
<surname>Hojo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Toshimitsu</given_name>
<surname>Sakai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yukoh</given_name>
<surname>Wakabayashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Norihide</given_name>
<surname>Kitaoka</surname>
</person_name>
					</contributors>
					<titles><title>Fine-tuning Parakeet-TDT for Dysarthric Speech Recognition in the Speech Accessibility Project Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3304</first_page>
						<last_page>3308</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1484</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/takahashi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rongshuai</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debasish Ray</given_name>
<surname>Mohapatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sidney</given_name>
<surname>Fels</surname>
</person_name>
					</contributors>
					<titles><title>2D Immersed Boundary Method in Vocal Tract Acoustics: An Eulerian–Lagrangian Model for Simulation of Diphthongs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>973</first_page>
						<last_page>977</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1486</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wu25j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aarish Shah</given_name>
<surname>Mohsin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammad</given_name>
<surname>Nadeem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shahab Saquib</given_name>
<surname>Sohail</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tughrul</given_name>
<surname>Arsalan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mandar</given_name>
<surname>Gogate</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nasir</given_name>
<surname>Saleem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amir</given_name>
<surname>Hussain</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Gender Bias in Text-to-Audio Generation Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3369</first_page>
						<last_page>3373</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1488</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mohsin25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Viola</given_name>
<surname>Negroni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Davide</given_name>
<surname>Salvi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paolo</given_name>
<surname>Bestagini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefano</given_name>
<surname>Tubaro</surname>
</person_name>
					</contributors>
					<titles><title> Source Verification for Speech Deepfakes </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1548</first_page>
						<last_page>1552</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1490</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/negroni25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Changin</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sungjun</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wonjong</given_name>
<surname>Rhee</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Retrieval-Augmented Audio Captioning with Generation-Assisted Multimodal Querying and Progressive Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2655</first_page>
						<last_page>2659</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1493</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/choi25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shumin</given_name>
<surname>Que</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anton</given_name>
<surname>Ragni</surname>
</person_name>
					</contributors>
					<titles><title>VisualSpeech: Enhancing Prosody Modeling in TTS Using Video</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3778</first_page>
						<last_page>3782</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1494</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/que25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Filippo</given_name>
<surname>Villani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wai-Yip</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng-Hua</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Østergaard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesper</given_name>
<surname>Jensen</surname>
</person_name>
					</contributors>
					<titles><title>Analysis and Extension of a Near-End Listening Enhancement Method Based on Long-Term Fractile Noise Statistics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>783</first_page>
						<last_page>787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1496</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/villani25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yassine</given_name>
<surname>El Kheir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Omnia</given_name>
<surname>Ibrahim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amit</given_name>
<surname>Meghanani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nada</given_name>
<surname>Almarwani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hawau</given_name>
<surname>Toyin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sadeen</given_name>
<surname>Alharbi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Modar</given_name>
<surname>Alfadly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lamya</given_name>
<surname>Alkanhal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ibrahim</given_name>
<surname>Selim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shehab</given_name>
<surname>Elbatal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Salima</given_name>
<surname>Mdhaffar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yasser</given_name>
<surname>Hifny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mostafa</given_name>
<surname>Shahin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Ali</surname>
</person_name>
					</contributors>
					<titles><title>Towards a Unified Benchmark for Arabic Pronunciation Assessment: Qur’anic Recitation as Case Study</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2410</first_page>
						<last_page>2414</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1497</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/elkheir25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chengyuan</given_name>
<surname>Qin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenmeng</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maoshen</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changchun</given_name>
<surname>Bao</surname>
</person_name>
					</contributors>
					<titles><title>Speech Enhancement with Dual-path Multi-Channel Linear Prediction Filter and Multi-norm Beamforming</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1198</first_page>
						<last_page>1202</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1502</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/qin25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Te</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Min</given_name>
<surname>Bi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saierdaer</given_name>
<surname>Yusuyin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijian</given_name>
<surname>Ou</surname>
</person_name>
					</contributors>
					<titles><title>LLM-based phoneme-to-grapheme for phoneme-based speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>559</first_page>
						<last_page>563</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1503</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ma25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hamid</given_name>
<surname>Mojarad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Tang</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Speech Recognition of African American English: Lexical and Contextual Effects</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3883</first_page>
						<last_page>3887</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1511</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mojarad25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qing</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lirong</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name>
					</contributors>
					<titles><title>A Domain Robust Pre-Training Method with Local Prototypes for Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3693</first_page>
						<last_page>3697</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1512</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gu25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Longjie</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name>
					</contributors>
					<titles><title>SuPseudo: A Pseudo-supervised Learning Method for Neural Speech Enhancement in Far-field Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3404</first_page>
						<last_page>3408</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1513</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/luo25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Chochlakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Turab</given_name>
<surname>Iqbal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Woo Hyun</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaocheng</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>Modality-Agnostic Multimodal Emotion Recognition using a Contrastive Masked Autoencoder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3005</first_page>
						<last_page>3009</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1514</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chochlakis25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jakob</given_name>
<surname>Kienegger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>Steering Deep Non-Linear Spatially Selective Filters for Weakly Guided Extraction of Moving Speakers in Dynamic Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2990</first_page>
						<last_page>2994</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1515</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kienegger25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoran</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ningning</given_name>
<surname>Pan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gongping</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>TTMBA: Towards Text To Multiple Sources Binaural Audio Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4228</first_page>
						<last_page>4232</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1516</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/he25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jesuraj</given_name>
<surname>Bandekar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Acoustic-to-Articulatory Inversion with Multi-Target Pretraining for Low-Resource Settings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5588</first_page>
						<last_page>5592</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1519</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bandekar25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziqian</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianjun</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinfa</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>U-SAM: An Audio Language Model for Unified Speech, Audio, and Music Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2720</first_page>
						<last_page>2724</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1524</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Marianne</given_name>
<surname>de Heer Kloots</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hosein</given_name>
<surname>Mohebbi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charlotte</given_name>
<surname>Pouw</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaofei</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Willem</given_name>
<surname>Zuidema</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martijn</given_name>
<surname>Bentum</surname>
</person_name>
					</contributors>
					<titles><title>What do self-supervised speech models know about Dutch?  Analyzing advantages of language-specific pre-training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>256</first_page>
						<last_page>260</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1526</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/deheerkloots25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yigitcan</given_name>
<surname>Özer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Woosung</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joan</given_name>
<surname>Serrà</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mayank Kumar</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Hsiang</given_name>
<surname>Liao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Mitsufuji</surname>
</person_name>
					</contributors>
					<titles><title>A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5113</first_page>
						<last_page>5117</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1530</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ozer25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wangjin</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianjiao</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenglin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Simple and Effective Content Encoder for Singing Voice Conversion via SSL-Embedding Dimension Reduction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1268</first_page>
						<last_page>1272</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1531</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhou25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lena-Marie</given_name>
<surname>Huttner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeppe H.</given_name>
<surname>Christensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gitte</given_name>
<surname>Keidser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>May</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Torsten</given_name>
<surname>Dau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sergi</given_name>
<surname>Rotger-Griful</surname>
</person_name>
					</contributors>
					<titles><title>Does effortful speech production indicate communication difficulty caused by noise and hearing aid support?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1088</first_page>
						<last_page>1092</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1535</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/huttner25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anuprabha</given_name>
<surname>M</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Krishna</given_name>
<surname>Gurugubelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anil Kumar</given_name>
<surname>Vuppala</surname>
</person_name>
					</contributors>
					<titles><title>Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2750</first_page>
						<last_page>2754</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1536</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/m25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Neil</given_name>
<surname>Shah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shirish</given_name>
<surname>Karande</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vineet</given_name>
<surname>Gandhi</surname>
</person_name>
					</contributors>
					<titles><title>NAM-to-Speech Conversion with Multitask-Enhanced Autoregressive Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5608</first_page>
						<last_page>5612</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1537</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shah25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fengjin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yadong</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meng</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Luan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4593</first_page>
						<last_page>4597</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1538</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yinfeng</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huiyan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenyang</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manhong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yutao</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingyang</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4408</first_page>
						<last_page>4412</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1541</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xia25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yixuan</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc Thang</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>Layer-Wise Decision Fusion for Fake Audio Detection Using XLS-R</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5618</first_page>
						<last_page>5622</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1543</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xiao25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingchen</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinfa</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengyou</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bingshen</given_name>
<surname>Mu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junhui</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Danming</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Weakly Supervised Data Refinement and Flexible Sequence Compression for Efficient Thai LLM-based ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>748</first_page>
						<last_page>752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1548</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shao25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xugang</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hisashi</given_name>
<surname>Kawai</surname>
</person_name>
					</contributors>
					<titles><title>Cross-modal Knowledge Transfer Learning as Graph Matching Based on Optimal Transport for ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3339</first_page>
						<last_page>3343</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1549</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lu25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hawau</given_name>
<surname>Toyin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rufael</given_name>
<surname>Marew</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Humaid</given_name>
<surname>Alblooshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samar M.</given_name>
<surname>Magdy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanan</given_name>
<surname>Aldarmaki</surname>
</person_name>
					</contributors>
					<titles><title>ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4808</first_page>
						<last_page>4812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1550</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/toyin25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kangqi</given_name>
<surname>Jing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenbin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Gao</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End DOA-Guided Speech Extraction in Noisy Multi-Talker Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1443</first_page>
						<last_page>1447</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1552</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jing25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Moreno</given_name>
<surname>La Quatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alkis</given_name>
<surname>Koudounas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Valerio Mario</given_name>
<surname>Salerno</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Generative Error Correction for Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3284</first_page>
						<last_page>3288</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1553</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/laquatra25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wang</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Archontis</given_name>
<surname>Politis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuomas</given_name>
<surname>Virtanen</surname>
</person_name>
					</contributors>
					<titles><title>Inter-Speaker Relative Cues for Text-Guided Target Speech Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1918</first_page>
						<last_page>1922</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1554</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dai25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Arnab</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yassine</given_name>
<surname>El Kheir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Franzreb</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Herzig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tim</given_name>
<surname>Polzehl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Möller</surname>
</person_name>
					</contributors>
					<titles><title>Generalizable Audio Spoofing Detection using Non-Semantic Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4553</first_page>
						<last_page>4557</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1555</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/das25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Klejch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Lamb</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>A Practitioner’s Guide to Building ASR Models for Low-Resource Languages: A Case Study on Scottish Gaelic</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>728</first_page>
						<last_page>732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1557</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/klejch25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thai-Binh</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thi Van</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quoc Truong</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chi Mai</given_name>
<surname>Luong</surname>
</person_name>
					</contributors>
					<titles><title>ViCocktail: Automated Multi-Modal Data Collection for Vietnamese Audio-Visual Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>166</first_page>
						<last_page>170</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1559</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/nguyen25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sreekanth</given_name>
<surname>Sankala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Venkatesh</given_name>
<surname>Parvathala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ramesh</given_name>
<surname>Gundluru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K. Sri Rama</given_name>
<surname>Murty</surname>
</person_name>
					</contributors>
					<titles><title>Adversarial Attacks on Text-dependent Speaker Verification System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4558</first_page>
						<last_page>4562</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1560</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sankala25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tieran</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guibin</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongjun</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Dual Orthogonality Sub-center Loss for Enhanced Anomalous Sound Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3374</first_page>
						<last_page>3378</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1563</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yin-Long</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanchao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liu</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia-Xin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Ming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Ang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan-Han</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia-Hong</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Cascaded Binary Classification and Multimodal Fusion for Dementia Detection through Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>544</first_page>
						<last_page>548</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1564</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Harish</given_name>
<surname>Battula</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gauri</given_name>
<surname>Deshpande</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yagna</given_name>
<surname>Gudipalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sachin</given_name>
<surname>Patel</surname>
</person_name>
					</contributors>
					<titles><title>Heart Rate as a Proxy Measure to Assess Human Confidence in Spoken Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2013</first_page>
						<last_page>2017</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1570</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/battula25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinfu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziteng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengqiang</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feiran</given_name>
<surname>Yang</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Echo Path Priors for Enhanced Stereo Acoustic Echo Cancellation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>773</first_page>
						<last_page>777</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1572</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daiki</given_name>
<surname>Takeuchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Binh Thien</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masahiro</given_name>
<surname>Yasuda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yasunori</given_name>
<surname>Ohishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daisuke</given_name>
<surname>Niizumi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noboru</given_name>
<surname>Harada</surname>
</person_name>
					</contributors>
					<titles><title>CLAP-ART: Automated Audio Captioning with Semantic-rich Audio Representation Tokenizer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3130</first_page>
						<last_page>3134</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1573</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/takeuchi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sirui</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongjie</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>SpeechRefiner: Towards Perceptual Quality Refinement for Front-End Algorithms</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3828</first_page>
						<last_page>3832</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1581</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiqing</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guibin</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tieran</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongjun</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Adaptive Across-Subcenter Representation Learning for Imbalanced Anomalous Sound Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3379</first_page>
						<last_page>3383</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1584</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Le</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenxing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruibo</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dong</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Mitigating Audiovisual Mismatch in Visual-Guide Audio Captioning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>161</first_page>
						<last_page>165</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1593</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xu25j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingru</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmei</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qianhua</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanxiong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peirong</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongqiang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huijia</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiqiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Generalizable Audio Deepfake Detection via Hierarchical Structure Learning and Feature Whitening in Poincaré sphere</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2255</first_page>
						<last_page>2259</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1594</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hayato</given_name>
<surname>Futami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emiru</given_name>
<surname>Tsunoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Kashiwagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Ito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hassan</given_name>
<surname>Shahmohammadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>36</first_page>
						<last_page>40</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1595</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/futami25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuqiu</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongjie</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yudong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuzhi</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongfeng</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nan</given_name>
<surname>Yan</surname>
</person_name>
					</contributors>
					<titles><title>Emotion-Guided Graph Attention Networks for Speech-Based Depression Detection under Emotion-Inducting Tasks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>469</first_page>
						<last_page>473</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1597</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhou25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yin-Long</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rui</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia-Xin</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Ming</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jia-Hong</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name>
					</contributors>
					<titles><title>Beyond Manual Transcripts: The Potential of Automated Speech Recognition Errors in Improving Alzheimer’s Disease Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5678</first_page>
						<last_page>5682</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1598</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haoshuai</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Changgeng</given_name>
<surname>Mo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boxuan</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linkai</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shan Xiang</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5468</first_page>
						<last_page>5472</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1599</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhou25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jin Gyo</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong Eun</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>SIDC-KWS: Efficient Spiking Inception-Dilated Conformer with Self-Attention for Keyword Spotting</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2665</first_page>
						<last_page>2669</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1607</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lim25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Muyeol</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>HyunJung</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yohan</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeonguk</given_name>
<surname>Bang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minkyu</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seonhui</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung</given_name>
<surname>Yun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donghyun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minsoo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>SangHun</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Bidirectional Spoken-Written Text Conversion with Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5088</first_page>
						<last_page>5092</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1610</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/choi25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alkis</given_name>
<surname>Koudounas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moreno</given_name>
<surname>La Quatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eliana</given_name>
<surname>Pastor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elena</given_name>
<surname>Baralis</surname>
</person_name>
					</contributors>
					<titles><title>“KAN you hear me?” Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4123</first_page>
						<last_page>4127</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1612</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/koudounas25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yulu</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingyue</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qisheng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jianqiao</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kele</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yong</given_name>
<surname>Dou</surname>
</person_name>
					</contributors>
					<titles><title>Multi-view Fusion and Parameter Perturbation for Few-Shot Class-Incremental Audio Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1313</first_page>
						<last_page>1317</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1613</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fang25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kamo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tsubasa</given_name>
<surname>Ochiai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Delcroix</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomohiro</given_name>
<surname>Nakatani</surname>
</person_name>
					</contributors>
					<titles><title>MOVER: Combining Multiple Meeting Recognition Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3424</first_page>
						<last_page>3428</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1614</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kamo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aleksandr</given_name>
<surname>Kutsakov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandr</given_name>
<surname>Maximenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgii</given_name>
<surname>Gospodinov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pavel</given_name>
<surname>Bogomolov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fyodor</given_name>
<surname>Minkin</surname>
</person_name>
					</contributors>
					<titles><title>GigaAM: Efficient Self-Supervised Learner for Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1213</first_page>
						<last_page>1217</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1616</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kutsakov25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Proctor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tünde</given_name>
<surname>Szalay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tharinda</given_name>
<surname>Piyadasa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Craig</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naeim</given_name>
<surname>Sanaei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amelia</given_name>
<surname>Gully</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Waddington</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheryl</given_name>
<surname>Foster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kirrie</given_name>
<surname>Ballard</surname>
</person_name>
					</contributors>
					<titles><title>Rhotic Articulation in Australian English: Insights from MRI</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3499</first_page>
						<last_page>3503</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1619</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/proctor25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rodolfo</given_name>
<surname>Zevallos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martí</given_name>
<surname>Cortada Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Solito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Mena</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Peiró-Lilja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Hernando</surname>
</person_name>
					</contributors>
					<titles><title>Assessing the Performance and Efficiency of Mamba ASR in Low-Resource Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5198</first_page>
						<last_page>5202</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1624</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zevallos25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gwangyeol</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junhyeok</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seoryeong</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jimin</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jehyuk</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Mimic Blocker: Self-Supervised Adversarial Training for Voice Conversion Defense with Pretrained Feature Extractors</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1663</first_page>
						<last_page>1667</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1625</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yu25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xiyuan</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bruce Xiao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meiling</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuming</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shekhar</given_name>
<surname>Nayak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Coler</surname>
</person_name>
					</contributors>
					<titles><title>A Multimodal Chinese Dataset for Cross-lingual Sarcasm Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3968</first_page>
						<last_page>3972</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1632</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gao25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zheyuan</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Decoding Listener's Identity: Person Identification from EEG Signals Using a Lightweight Spiking Transformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5548</first_page>
						<last_page>5552</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1637</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lin25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingyuan</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhipeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuaiqi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofen</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangmin</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>EATS-Speech: Emotion-Adaptive Transformation and Priority Synthesis for Zero-Shot Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4358</first_page>
						<last_page>4362</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1638</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xing25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maxime</given_name>
<surname>Jacquelin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maëva</given_name>
<surname>Garnier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laurent</given_name>
<surname>Girin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rémy</given_name>
<surname>Vincent</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivier</given_name>
<surname>Perrotin</surname>
</person_name>
					</contributors>
					<titles><title>LombardTokenizer: Disentanglement and Control of Vocal Effort in a Neural Speech Codec</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5778</first_page>
						<last_page>5782</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1639</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jacquelin25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chunhui</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xue</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liming</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junkwang</given_name>
<surname>Oh</surname>
</person_name>
					</contributors>
					<titles><title>Robust Neural Codec Language Modeling with Phoneme Position Prediction for Zero-Shot TTS</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2475</first_page>
						<last_page>2479</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1641</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lu25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomoya</given_name>
<surname>Yoshinaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiaki</given_name>
<surname>Bando</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keitaro</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keisuke</given_name>
<surname>Imoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masaki</given_name>
<surname>Onishi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shigeo</given_name>
<surname>Morishima</surname>
</person_name>
					</contributors>
					<titles><title>Training Onset-and-Offset-Aware Sound Event Detection  on a Heterogeneous Dataset via Probabilistic Sequential Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1308</first_page>
						<last_page>1312</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1642</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yoshinaga25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rishabh</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>MLNS</given_name>
<surname>Karthik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yughendaran</given_name>
<surname>P</surname>
</person_name>
					</contributors>
					<titles><title>Low Complex IIR Adaptive Hear-Through Ambient Filtering for Overcoming Practical Constraints in Earbuds</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3115</first_page>
						<last_page>3119</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1646</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gupta25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tharinda</given_name>
<surname>Piyadasa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joan</given_name>
<surname>Glaunès</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amelia</given_name>
<surname>Gully</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Proctor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kirrie</given_name>
<surname>Ballard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tünde</given_name>
<surname>Szalay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naeim</given_name>
<surname>Sanaei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheryl</given_name>
<surname>Foster</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Waddington</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Craig</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Constrained LDDMM for Dynamic Vocal Tract Morphing: Integrating Volumetric and Real-Time MRI</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>968</first_page>
						<last_page>972</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1650</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/piyadasa25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongfei</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yufeng</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuelong</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Selective Invocation for Multilingual ASR: A Cost-effective Approach Adapting to Speech Recognition Difficulty</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2580</first_page>
						<last_page>2584</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1652</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xue25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Noumida</given_name>
<surname>A</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajeev</given_name>
<surname>Rajan</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Avian Biphonic Vocalization Using Computational Modelling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1693</first_page>
						<last_page>1697</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1658</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/a25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuyun</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujia</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiahao</given_name>
<surname>Luo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenming</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheng</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuan</given_name>
<surname>Zong</surname>
</person_name>
					</contributors>
					<titles><title>Interactive Fusion of Multi-View Speech Embeddings via Pretrained Large-Scale Speech Models for Speech Emotional Attribute Prediction in Naturalistic Conditions </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4688</first_page>
						<last_page>4692</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1662</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Cord-Landwehr</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Gburrek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Deegen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name>
					</contributors>
					<titles><title>Spatio-Spectral Diarization of Meetings by Combining TDOA-based Segmentation and Speaker Embedding-based Clustering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5223</first_page>
						<last_page>5227</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1663</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cordlandwehr25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amit</given_name>
<surname>Sofer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoav</given_name>
<surname>Goldman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shlomo E.</given_name>
<surname>Chazan</surname>
</person_name>
					</contributors>
					<titles><title>Pull It Together: Reducing the Modality Gap in Contrastive Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>196</first_page>
						<last_page>200</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1664</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sofer25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siyi</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanmin</given_name>
<surname>Qian</surname>
</person_name>
					</contributors>
					<titles><title>Lightweight Front-end Enhancement for Robust ASR via Frame Resampling and Sub-Band Pruning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3409</first_page>
						<last_page>3413</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1668</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhao25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianyi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongjie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lv</given_name>
<surname>Hang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jian</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhennan</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongxiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>584</first_page>
						<last_page>588</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1669</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xu25k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyu</given_name>
<surname>Xiong</surname>
</person_name>
					</contributors>
					<titles><title>The Role of Syntactic Structures in Shaping Directionality in Trisyllabic Tone Sandhi: Evidence from Tianjin Mandarin</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>699</first_page>
						<last_page>703</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1673</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lu25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingyi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bowei</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martine</given_name>
<surname>Adda-Decker</surname>
</person_name>
					</contributors>
					<titles><title>Apical vs. Regular Vowel Duration: A Corpus-based Analysis of Contextual Influences in Standard Mandarin</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4753</first_page>
						<last_page>4757</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1674</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sun25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yizhi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luyuan</given_name>
<surname>Geng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengru</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Sentence-Final Particles in Mandarin Child-Directed Speech: Frequency and Impact on Speech Rate</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3803</first_page>
						<last_page>3807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1675</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wei</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Iuliia</given_name>
<surname>Zaitova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name>
					</contributors>
					<titles><title>The Effect of Word Predictability on Spoken Cross-Language Intelligibility</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3798</first_page>
						<last_page>3802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1676</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xue25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xueru</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyuan</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofen</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhipeng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangmin</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>SA-RAS: Speaker-Aware Style Retrieval Augmented Generation for Expressive Zero-Shot Text-to-Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4388</first_page>
						<last_page>4392</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1684</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Longhao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangze</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongfei</given_name>
<surname>Xue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Delayed-KD: Delayed Knowledge Distillation based CTC for Low-Latency Streaming ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4413</first_page>
						<last_page>4417</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1691</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyung Kyu</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hak Gu</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven 3D Facial Animation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3763</first_page>
						<last_page>3767</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1692</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nathalie</given_name>
<surname>Vauquier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brij Mohan Lal</given_name>
<surname>Srivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seyed Ahmad</given_name>
<surname>Hosseini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Vincent</surname>
</person_name>
					</contributors>
					<titles><title>Legally validated evaluation framework for voice anonymization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3229</first_page>
						<last_page>3233</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1699</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/vauquier25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Enes</given_name>
<surname>Ugan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc-Quan</given_name>
<surname>Pham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Waibel</surname>
</person_name>
					</contributors>
					<titles><title>Weight Factorization and Centralization for Continual Learning in Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2200</first_page>
						<last_page>2204</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1701</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ugan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hoan My</given_name>
<surname>Tran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Damien</given_name>
<surname>Lolive</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Guennec</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aghilas</given_name>
<surname>Sini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnaud</given_name>
<surname>Delhay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierre-François</given_name>
<surname>Marteau</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging SSL Speech Features and Mamba for Enhanced DeepFake Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5323</first_page>
						<last_page>5327</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1703</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tran25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abdul</given_name>
<surname>Hannan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessio</given_name>
<surname>Brutti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shah</given_name>
<surname>Nawaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mubashir</given_name>
<surname>Noman</surname>
</person_name>
					</contributors>
					<titles><title>An Effective Training Framework for Light-Weight Automatic Speech Recognition Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3613</first_page>
						<last_page>3617</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1704</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hannan25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tianyi</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohuai</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenzhi</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianjun</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanzeng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>CBA-Whisper: Curriculum Learning-Based AdaLoRA Fine-Tuning on Whisper for Low-Resource Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3309</first_page>
						<last_page>3313</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1705</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tan25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nune</given_name>
<surname>Tadevosyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikolay</given_name>
<surname>Karpov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrei</given_name>
<surname>Andrusenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vitaly</given_name>
<surname>Lavrukhin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ante</given_name>
<surname>Jukic</surname>
</person_name>
					</contributors>
					<titles><title> Unified Semi-Supervised Pipeline for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3184</first_page>
						<last_page>3188</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1706</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tadevosyan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jeena</given_name>
<surname>Prakash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Blessingh</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kadri</given_name>
<surname>Hacioglu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bidisha</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sindhuja</given_name>
<surname>Gopalan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Malolan</given_name>
<surname>Chetlur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shankar</given_name>
<surname>Venkatesan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name>
					</contributors>
					<titles><title>Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>579</first_page>
						<last_page>583</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1707</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/prakash25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuxi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yikang</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qishan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiromitsu</given_name>
<surname>Nishizaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>VCapAV: A Video-Caption Based Audio-Visual Deepfake Detection Dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3908</first_page>
						<last_page>3912</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1713</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Allan</given_name>
<surname>Vurma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Einar</given_name>
<surname>Meister</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lya</given_name>
<surname>Meister</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaan</given_name>
<surname>Ross</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marju</given_name>
<surname>Raju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Veeda</given_name>
<surname>Kala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuuri</given_name>
<surname>Dede</surname>
</person_name>
					</contributors>
					<titles><title>The Role of Voiced Consonant Duration in Sung Vowel-Consonant and Consonant-Vowel Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>958</first_page>
						<last_page>962</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1716</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/vurma25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zeyan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianchi</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ronghui</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Lu</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Self-Supervised Learning Based Speaker Diarization for MISP 2025 AVSD Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1903</first_page>
						<last_page>1907</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1717</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/song25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junhui</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanhui</given_name>
<surname>Tu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Ma</surname>
</person_name>
					</contributors>
					<titles><title>TA-RIR: Topology-Aware Neural Modeling of Acoustic Propagation for Room Impulse Response Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2485</first_page>
						<last_page>2489</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1718</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhao25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kübra</given_name>
<surname>Bodur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Corinne</given_name>
<surname>Fredouille</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christine</given_name>
<surname>Meunier</surname>
</person_name>
					</contributors>
					<titles><title>Speech Reduction in French: The Relationship Between Vowel Space and Articulation Dynamics</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>384</first_page>
						<last_page>388</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1720</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bodur25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yonghyeon</given_name>
<surname>Jun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beomjun</given_name>
<surname>Woo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myeonghun</given_name>
<surname>Jeong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Namsoo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>SNR-Aligned Consistent Diffusion for Adaptive Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4073</first_page>
						<last_page>4077</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1721</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jun25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rini</given_name>
<surname>Sharon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hema A.</given_name>
<surname>Murthy</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Syllabic Recognition via Speech-EEG Phase Analysis and Non-Activity State Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2910</first_page>
						<last_page>2914</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1725</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sharon25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Péter</given_name>
<surname>Mihajlik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Máté Soma</given_name>
<surname>Kádár</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>László</given_name>
<surname>Tóth</surname>
</person_name>
					</contributors>
					<titles><title>Voice Reconstruction through Large-Scale TTS Models: Comparing Zero-Shot and Fine-tuning Approaches to Personalise TTS in Assistive Communication</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2735</first_page>
						<last_page>2739</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1726</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/szekely25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kristin</given_name>
<surname>Teplansky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emily</given_name>
<surname>Rangel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mimi</given_name>
<surname>LaValley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinuk</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beiming</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory Vowel Distinctiveness in Spanish</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5593</first_page>
						<last_page>5597</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1727</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/teplansky25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Danilo</given_name>
<surname>de Oliveira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julius</given_name>
<surname>Richter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Marie</given_name>
<surname>Lemercier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Welker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timo</given_name>
<surname>Gerkmann</surname>
</person_name>
					</contributors>
					<titles><title>Non-intrusive Speech Quality Assessment with Diffusion Models Trained on Clean Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2330</first_page>
						<last_page>2334</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1728</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/deoliveira25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paul</given_name>
<surname>McGuire</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kye</given_name>
<surname>Shibata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thanh Viet</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng-fan</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yueh-chin</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Supralaryngeal Kinematics of Implosives in Central Vietnamese: An EMA Study</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3484</first_page>
						<last_page>3488</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1733</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mcguire25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yawei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiaoling</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junyao</given_name>
<surname>Hu</surname>
</person_name>
					</contributors>
					<titles><title>Anomalous Sound Detection Based Feature Fusion and Dual-path Non-linear Independent Components Estimation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2615</first_page>
						<last_page>2619</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1734</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziqian</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zikai</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinfa</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yike</given_name>
<surname>Zhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mingshuai</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Longshuai</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chao</given_name>
<surname>Weng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4858</first_page>
						<last_page>4862</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1745</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuhiro</given_name>
<surname>Kaneko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirokazu</given_name>
<surname>Kameoka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kou</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuto</given_name>
<surname>Kondo</surname>
</person_name>
					</contributors>
					<titles><title>FasterVoiceGrad: Faster One-step Diffusion-Based Voice Conversion with Adversarial Diffusion Conversion Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4598</first_page>
						<last_page>4602</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1747</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kaneko25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rory</given_name>
<surname>Turnbull</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elisa</given_name>
<surname>Kiefer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Peperkamp</surname>
</person_name>
					</contributors>
					<titles><title>Does English fish sound like French fiche? Perceptual similarity judgments versus acoustic similarity</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4443</first_page>
						<last_page>4447</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1748</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/turnbull25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongyu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Selective Channel Attention based Target Speaker Voice Activity Detection for Speaker Diarization under AD-HOC Microphone Array Settings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5228</first_page>
						<last_page>5232</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1749</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mélen</given_name>
<surname>Guillaume</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anahita</given_name>
<surname>Basirat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Diard</surname>
</person_name>
					</contributors>
					<titles><title>Theoretical proposal for a unified Bayesian model of adaptation in non-interactive and interactive speech production</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1028</first_page>
						<last_page>1032</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1751</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/guillaume25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhen-Hua</given_name>
<surname>Ling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Gu</surname>
</person_name>
					</contributors>
					<titles><title>LIST: Language-Independent Speech Token for Multilingual Speech Synthesis with Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1623</first_page>
						<last_page>1627</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1752</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryandhimas E.</given_name>
<surname>Zezario</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato M.</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fei</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>Feature Importance across Domains for Improving Non-Intrusive Speech Intelligibility Prediction in Hearing Aids</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5473</first_page>
						<last_page>5477</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1756</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zezario25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dimitrios</given_name>
<surname>Koutsianos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stavros</given_name>
<surname>Zacharopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannis</given_name>
<surname>Panagakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Themos</given_name>
<surname>Stafylakis</surname>
</person_name>
					</contributors>
					<titles><title>Synthetic Speech Source Tracing using Metric Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1558</first_page>
						<last_page>1562</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1757</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/koutsianos25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jose</given_name>
<surname>Giraldo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Peiró-Lilja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carme</given_name>
<surname>Armentano-Oller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rodolfo</given_name>
<surname>Zevallos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristina</given_name>
<surname>España-Bonet</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Speech Enhancement Performance Across Demographics and Language</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1353</first_page>
						<last_page>1357</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1760</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/giraldo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Takuhiro</given_name>
<surname>Kaneko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirokazu</given_name>
<surname>Kameoka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kou</given_name>
<surname>Tanaka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuto</given_name>
<surname>Kondo</surname>
</person_name>
					</contributors>
					<titles><title>Vocoder-Projected Feature Discriminator</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4898</first_page>
						<last_page>4902</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1763</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kaneko25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuzhu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Archontis</given_name>
<surname>Politis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Konstantinos</given_name>
<surname>Drossos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tuomas</given_name>
<surname>Virtanen</surname>
</person_name>
					</contributors>
					<titles><title>Attractor-Based Speech Separation of Multiple Utterances  by Unknown Number of Speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1453</first_page>
						<last_page>1457</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1764</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xueyuan</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongchao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenxuan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minglin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>DiffDSR: Dysarthric Speech Reconstruction Using Latent Diffusion Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2113</first_page>
						<last_page>2117</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1770</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Livia</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Figueroa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Skantze</surname>
</person_name>
					</contributors>
					<titles><title>Representation of Perceived Prosodic Similarity of Conversational Feedback</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>374</first_page>
						<last_page>378</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1771</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/qian25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kyeongman</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seongho</given_name>
<surname>Joo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyomin</given_name>
<surname>Jung</surname>
</person_name>
					</contributors>
					<titles><title>MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices of  Multiple Speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5048</first_page>
						<last_page>5052</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1773</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/park25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maya</given_name>
<surname>Dewhurst</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jack</given_name>
<surname>Collins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justin J. H.</given_name>
<surname>Lo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roy</given_name>
<surname>Alderton</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sam</given_name>
<surname>Kirkham</surname>
</person_name>
					</contributors>
					<titles><title>Nosey: Open-Source Hardware for Acoustic Nasalance</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2265</first_page>
						<last_page>2269</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1775</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dewhurst25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Liming</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongchao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianwei</given_name>
<surname>Zhuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxin</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuehan</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuexian</given_name>
<surname>Zou</surname>
</person_name>
					</contributors>
					<titles><title>SpeechSEC: A Unified Multi-Task Framework for Speech Synthesis, Editing, and Continuation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3464</first_page>
						<last_page>3468</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1776</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liang25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhaoqing</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoning</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xurong</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengrui</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tianzi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xunying</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Unfolding A Few Structures for The Many: Memory-Efficient Compression of Conformer and Speech Foundation Models </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1978</first_page>
						<last_page>1982</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1777</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhaokai</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1653</first_page>
						<last_page>1657</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1778</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sun25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pengyu</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenhao</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaidi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peijie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>ReFlow-VC: Zero-shot Voice Conversion Based on Rectified Flow and Speaker Feature Optimization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1388</first_page>
						<last_page>1392</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1779</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ren25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhennan</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaixun</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Linju</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name>
					</contributors>
					<titles><title>Contextualized Automatic Speech Recognition with Dynamic Vocabulary Prediction and Activation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3174</first_page>
						<last_page>3178</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1785</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lin25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel-José</given_name>
<surname>Alcala Padilla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nils L.</given_name>
<surname>Westhausen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swati</given_name>
<surname>Vivekananthan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd T.</given_name>
<surname>Meyer</surname>
</person_name>
					</contributors>
					<titles><title>Location-Aware Target Speaker Extraction for Hearing Aids</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2975</first_page>
						<last_page>2979</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1787</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/alcalapadilla25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bubai</given_name>
<surname>Maji</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monorama</given_name>
<surname>Swain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shazia</given_name>
<surname>Nasreen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Debabrata</given_name>
<surname>Majumdar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajlakshmi</given_name>
<surname>Guha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aurobinda</given_name>
<surname>Routray</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anders</given_name>
<surname>Søgaard</surname>
</person_name>
					</contributors>
					<titles><title>A Study on The Impact of Foundation Models on Automatic Depression Detection from Speech Signals</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5258</first_page>
						<last_page>5262</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1789</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/maji25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rao</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mengjie</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyuan</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefano</given_name>
<surname>Bannò</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kate M.</given_name>
<surname>Knill</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark J.F.</given_name>
<surname>Gales</surname>
</person_name>
					</contributors>
					<titles><title>Assessment of L2 Oral Proficiency using Speech Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5078</first_page>
						<last_page>5082</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1793</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ma25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanel</given_name>
<surname>Alumäe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Artem</given_name>
<surname>Fedorchenko</surname>
</person_name>
					</contributors>
					<titles><title>TalTech Systems for the Interspeech 2025 ML-SUPERB 2.0 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2098</first_page>
						<last_page>2102</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1797</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/alumae25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lishan</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yaolin</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoying</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Lexical competition in the process of Cantonese tone merging: Diverse Impact Mechanisms Across Different Individuals and Tone Pairs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>709</first_page>
						<last_page>713</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1798</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chanho</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oscar</given_name>
<surname>Saz</surname>
</person_name>
					</contributors>
					<titles><title>Character Error Rate Estimation for Semi-Supervised Training of Speech Recognition for Arabic Dialects</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3179</first_page>
						<last_page>3183</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1799</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/park25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>D. Fortuné</given_name>
<surname>Kponou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Salima</given_name>
<surname>Mdhaffar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fréjus A. A.</given_name>
<surname>Laleye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eugène C.</given_name>
<surname>Ezin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name>
					</contributors>
					<titles><title>Extending the Fongbe to French Speech Translation Corpus:  resources, models and benchmark</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4533</first_page>
						<last_page>4537</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1801</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kponou25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Terry Yi</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esther</given_name>
<surname>Janse</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cristian</given_name>
<surname>Tejedor-Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis ten</given_name>
<surname>Bosch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martha</given_name>
<surname>Larson</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3738</first_page>
						<last_page>3742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1805</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhong25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Joonas</given_name>
<surname>Kalda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clément</given_name>
<surname>Pagés</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanel</given_name>
<surname>Alumäe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hervé</given_name>
<surname>Bredin</surname>
</person_name>
					</contributors>
					<titles><title>Diarization-Guided Multi-Speaker Embeddings</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5233</first_page>
						<last_page>5237</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1807</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kalda25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Badr M.</given_name>
<surname>Abdullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Baas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd</given_name>
<surname>Möbius</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dietrich</given_name>
<surname>Klakow</surname>
</person_name>
					</contributors>
					<titles><title>Voice Conversion Improves Cross-Domain Robustness  for Spoken Arabic Dialect Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2790</first_page>
						<last_page>2794</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1809</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/abdullah25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jing</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng-fan</given_name>
<surname>Hsieh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yueh-chin</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory variations in Apical Vowels in Southwestern Mandarin</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3494</first_page>
						<last_page>3498</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1810</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/huang25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Reo</given_name>
<surname>Yoneyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Masaya</given_name>
<surname>Kawamura</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Terashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichi</given_name>
<surname>Yamamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>Comparative Analysis of Fast and High-Fidelity Neural Vocoders for Low-Latency Streaming Synthesis in Resource-Constrained Environments</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4888</first_page>
						<last_page>4892</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1819</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yoneyama25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Frédéric</given_name>
<surname>Berthommier</surname>
</person_name>
					</contributors>
					<titles><title>Articulatory modeling of the S-shaped F2 trajectories observed in Öhman's spectrographic analysis of VCV syllables</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>953</first_page>
						<last_page>957</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1820</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/berthommier25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Guitao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinming</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guilin</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tongtong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gholamreza</given_name>
<surname>Haffari</surname>
</person_name>
					</contributors>
					<titles><title>Continual Speech Learning with Fused Speech Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1793</first_page>
						<last_page>1797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1823</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Julian</given_name>
<surname>Linke</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jana</given_name>
<surname>Winkler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Barbara</given_name>
<surname>Schuppler</surname>
</person_name>
					</contributors>
					<titles><title>Context is all you need? Low-resource conversational ASR profits from context, coming from the same or from the other speaker</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3199</first_page>
						<last_page>3203</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1824</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/linke25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Othman</given_name>
<surname>Istaiteh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Salima</given_name>
<surname>Mdhaffar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannick</given_name>
<surname>Estève</surname>
</person_name>
					</contributors>
					<titles><title>Beyond Similarity Scoring: Detecting Entailment and Contradiction in Multilingual and Multimodal Contexts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>286</first_page>
						<last_page>290</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1825</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/istaiteh25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Magoshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinsuke</given_name>
<surname>Sakai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaeyoung</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Multi-lingual and Zero-Shot Speech Recognition by Incorporating Classification of Language-Independent Articulatory Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>91</first_page>
						<last_page>95</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1827</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/magoshi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Kanamori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Okamoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taisei</given_name>
<surname>Takano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuki</given_name>
<surname>Saito</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroshi</given_name>
<surname>Saruwatari</surname>
</person_name>
					</contributors>
					<titles><title>RELATE: Subjective evaluation dataset for automatic evaluation of relevance between text and audio </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3155</first_page>
						<last_page>3159</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1830</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kanamori25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhijie</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Feng</surname>
</person_name>
					</contributors>
					<titles><title>Acoustic Representation and Realization of Weak Elements Subcategories: In the Case of Tianjin Mandarin</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>704</first_page>
						<last_page>708</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1835</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pouya</given_name>
<surname>Mehralian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Geographic Metadata for Dialect-Aware Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1153</first_page>
						<last_page>1157</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1839</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mehralian25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tahitoa</given_name>
<surname>Leygue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Astrid</given_name>
<surname>Sabourin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christian</given_name>
<surname>Bolzmacher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sylvain</given_name>
<surname>Bouchigny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margarita</given_name>
<surname>Anastassova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quoc-Cuong</given_name>
<surname>Pham</surname>
</person_name>
					</contributors>
					<titles><title>Explainable Speech Emotion Recognition Through Attentive Pooling: Insights from Attention-Based Temporal Localization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4658</first_page>
						<last_page>4662</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1841</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/leygue25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aijun</given_name>
<surname>LI</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiwei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Zhou</surname>
</person_name>
					</contributors>
					<titles><title>The Development of Speech Rhythm in Putonghua-Learning Preschool Children in South Xinjiang Uyghur Autonomous Region of China</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4463</first_page>
						<last_page>4467</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1847</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Suqi</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheqi</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yongyi</given_name>
<surname>Zang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yin</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qiuqiang</given_name>
<surname>Kong</surname>
</person_name>
					</contributors>
					<titles><title>DiffStereo: End-to-End Mono-to-Stereo Audio Generation with Diffusion Transformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3150</first_page>
						<last_page>3154</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1850</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sven</given_name>
<surname>Franz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Grewe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd T.</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jörg</given_name>
<surname>Bitzer</surname>
</person_name>
					</contributors>
					<titles><title>Influence of Room Acoustics on Objective Voice Assessment Methods in the Context of Speech and Language Therapy </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5308</first_page>
						<last_page>5312</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1851</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/franz25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>William N.</given_name>
<surname>Havard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Renauld</given_name>
<surname>Govain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benjamin</given_name>
<surname>Lecouteux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Schang</surname>
</person_name>
					</contributors>
					<titles><title>Self-Supervised Models of Speech Processing for Haitian Creole</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4018</first_page>
						<last_page>4022</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1852</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/havard25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Satu</given_name>
<surname>Hopponen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandre</given_name>
<surname>Nikolaev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rosa</given_name>
<surname>González Hautamäki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lauri</given_name>
<surname>Tavi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Einar</given_name>
<surname>Meister</surname>
</person_name>
					</contributors>
					<titles><title>FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>364</first_page>
						<last_page>368</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1853</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hopponen25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mingxi</given_name>
<surname>LU</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ran</given_name>
<surname>Tao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yujia</given_name>
<surname>Tian</surname>
</person_name>
					</contributors>
					<titles><title>Talker Normalization in Chinese Bilinguals: A Comparative Study</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2315</first_page>
						<last_page>2319</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1857</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lu25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gustavo</given_name>
<surname>Silveira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aviad</given_name>
<surname>Albert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martine</given_name>
<surname>Grice</surname>
</person_name>
					</contributors>
					<titles><title>Probing Prosodic Differences Between Two Regional Varieties of Brazilian Portuguese</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2935</first_page>
						<last_page>2939</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1863</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/silveira25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhe</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Man-Wai</given_name>
<surname>Mak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jen-Tzung</given_name>
<surname>Chien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mert</given_name>
<surname>Pilanci</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zezhong</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>Disentangling Speaker and Content in Pre-trained Speech Models with Latent Diffusion for Robust Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1108</first_page>
						<last_page>1112</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1865</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25z_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alkis</given_name>
<surname>Koudounas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moreno</given_name>
<surname>La Quatra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriele</given_name>
<surname>Ciravegna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Fantini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erika</given_name>
<surname>Crosetti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giovanni</given_name>
<surname>Succo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tania</given_name>
<surname>Cerquitelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elena</given_name>
<surname>Baralis</surname>
</person_name>
					</contributors>
					<titles><title>MVP: Multi-source Voice Pathology detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3548</first_page>
						<last_page>3552</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1868</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/koudounas25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bowei</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Buech</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne</given_name>
<surname>Hermes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Giavazzi</surname>
</person_name>
					</contributors>
					<titles><title>Lexical stress affects lenition: The case of Italian palato-alveolar affricates</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>354</first_page>
						<last_page>358</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1869</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shao25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zexin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenhan</given_name>
<surname>Yao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinsu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fen</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiping</given_name>
<surname>Wen</surname>
</person_name>
					</contributors>
					<titles><title>LRBA: Stealthy Backdoor Attacks on Speech Classification via Latent Rearrangement in VITS</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5653</first_page>
						<last_page>5657</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1872</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25aa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zilong</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoxue</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinyang</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaitao</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jue</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Can AI Understand Mandarin Speech Prosody?  A Framework and Benchmark Showcase</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5378</first_page>
						<last_page>5382</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1873</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abhijit</given_name>
<surname>Sinha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hemant</given_name>
<surname>Kumar Kathania</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mikko</given_name>
<surname>Kurimo</surname>
</person_name>
					</contributors>
					<titles><title>Beyond Traditional Speech Modifications : Utilizing Self Supervised Features for Enhanced Zero-Shot Children ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1963</first_page>
						<last_page>1967</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1874</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sinha25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongli</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ayiduosi</given_name>
<surname>Tuohan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yizhou</given_name>
<surname>Peng</surname>
</person_name>
					</contributors>
					<titles><title>Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1133</first_page>
						<last_page>1137</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1875</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fengyue Lisa</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Kuo</surname>
</person_name>
					</contributors>
					<titles><title> The Role of Contextual Variation in Learning Cantonese Tones from Naturalistic Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4453</first_page>
						<last_page>4457</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1876</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhao25j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ka Ki</given_name>
<surname>SO</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenzi</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grace Wenling</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peggy</given_name>
<surname>Mok</surname>
</person_name>
					</contributors>
					<titles><title>Performance of Montreal Forced Aligner on Cantonese Spontaneous Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5398</first_page>
						<last_page>5402</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1882</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/so25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christophe</given_name>
<surname>Van Gysel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maggie</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lyan</given_name>
<surname>Verwimp</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caglar</given_name>
<surname>Tirkaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Bertola</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihong</given_name>
<surname>Lei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youssef</given_name>
<surname>Oualil</surname>
</person_name>
					</contributors>
					<titles><title>Phonetically-Augmented Discriminative Rescoring for Voice Search Error Correction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3673</first_page>
						<last_page>3677</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1885</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/vangysel25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yuhang</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>He</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xingchen</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zihan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuiyuan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hongxiao</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaoji</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Bu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>AISHELL-5: The First Open-Source In-Car Multi-Channel Multi-Speaker Speech Dataset for Automatic Speech Diarization and Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5493</first_page>
						<last_page>5497</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1886</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dai25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saurav</given_name>
<surname>Pahuja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Ivucic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siqi</given_name>
<surname>Cai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dashanka</given_name>
<surname>Da Silva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>GTAnet: Geometry-Guided Temporal Attention for EEG-Based Sound Source Tracking in Cocktail Party Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5543</first_page>
						<last_page>5547</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1887</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pahuja25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sanberk</given_name>
<surname>Serbest</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tijana</given_name>
<surname>Stojkovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milos</given_name>
<surname>Cernak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Harper</surname>
</person_name>
					</contributors>
					<titles><title>DeepFilterGAN: A Full-band Real-time Speech Enhancement System with GAN-based Stochastic Regeneration</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>878</first_page>
						<last_page>882</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1889</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/serbest25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karen</given_name>
<surname>Rosero</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ali N</given_name>
<surname>Salman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shreeram</given_name>
<surname>Chandra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cortney Van’t</given_name>
<surname>Slot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alex</given_name>
<surname>Kane</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rami R</given_name>
<surname>Hallac</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Advancing Pediatric ASR: The Role of Voice Generation in Disordered Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2890</first_page>
						<last_page>2894</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1890</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/rosero25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Cathal</given_name>
<surname>Ó Faoláin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Hines</surname>
</person_name>
					</contributors>
					<titles><title>Attention Models and Auditory Transduction Features for Noise Robustness</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3434</first_page>
						<last_page>3438</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1892</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ofaolain25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Truong</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minh-Phuong</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Le -Minh</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>PruneSLU: Efficient On-device Spoken Language Understanding through Vocabulary and Structural Pruning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1738</first_page>
						<last_page>1742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1893</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/do25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ticho</given_name>
<surname>Urai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pachara</given_name>
<surname>Boonsarngsuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ekapol</given_name>
<surname>Chuangsuwanich</surname>
</person_name>
					</contributors>
					<titles><title>Thai Speech Spoofing Detection Dataset with Variations in Speaking Styles</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5643</first_page>
						<last_page>5647</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1895</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/urai25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Raymond</given_name>
<surname>Grossman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taejin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kunal</given_name>
<surname>Dhawan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Titus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sophia</given_name>
<surname>Zhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yulia</given_name>
<surname>Shchadilova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagadeesh</given_name>
<surname>Balam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4048</first_page>
						<last_page>4052</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1896</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/grossman25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karumannil</given_name>
<surname>Mohamed Ismail Yasar Arafath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohammed Abeer</given_name>
<surname>K. C.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aurobinda</given_name>
<surname>Routray</surname>
</person_name>
					</contributors>
					<titles><title>A Naturally Elicited Multimodal Stress Database and Speech Breathing Based Stress Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4523</first_page>
						<last_page>4527</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1902</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mohamedismailyasararafath25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Buech</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne</given_name>
<surname>Hermes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rachid</given_name>
<surname>Ridouane</surname>
</person_name>
					</contributors>
					<titles><title>Equivalence and differences: Formant patterns of labialization and pharyngealization in Tashlhiyt</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4763</first_page>
						<last_page>4767</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1905</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/buech25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gaofei</given_name>
<surname>Shen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hosein</given_name>
<surname>Mohebbi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arianna</given_name>
<surname>Bisazza</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Afra</given_name>
<surname>Alishahi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grzegorz</given_name>
<surname>Chrupala</surname>
</person_name>
					</contributors>
					<titles><title>On the reliability of feature attribution methods for speech classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>266</first_page>
						<last_page>270</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1911</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shen25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Ick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gordon</given_name>
<surname>Wichern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiki</given_name>
<surname>Masuyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François G.</given_name>
<surname>Germain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Le Roux</surname>
</person_name>
					</contributors>
					<titles><title>Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>933</first_page>
						<last_page>937</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1912</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ick25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oli Danyi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>The role of audio-visual integration in the time course of phonetic encoding in self-supervised speech models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2730</first_page>
						<last_page>2734</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1913</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Scheck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tom</given_name>
<surname>Dombeck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Wand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>DiffMV-ETS: Diffusion-based Multi-Voice Electromyography-to-Speech Conversion using Speaker-Independent Speech Training Targets</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5573</first_page>
						<last_page>5577</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1914</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/scheck25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jialong</given_name>
<surname>Mai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofen</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weidong</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanbo</given_name>
<surname>Fang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangmin</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>AA-SLLM: An Acoustically Augmented Speech Large Language Model for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4328</first_page>
						<last_page>4332</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1915</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mai25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ana</given_name>
<surname>Valente</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rufael</given_name>
<surname>Marew</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hawau</given_name>
<surname>Toyin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hamdan</given_name>
<surname>Al-Ali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anelise</given_name>
<surname>Bohnen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Inma</given_name>
<surname>Becerra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elsa</given_name>
<surname>Soares</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gonçalo</given_name>
<surname>Leal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanan</given_name>
<surname>Aldarmaki</surname>
</person_name>
					</contributors>
					<titles><title>Clinical Annotations for Automatic Stuttering Severity Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4318</first_page>
						<last_page>4322</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1916</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/valente25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Sasu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalie</given_name>
<surname>Schluter</surname>
</person_name>
					</contributors>
					<titles><title>Pitch Accent Detection improves Pretrained Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4733</first_page>
						<last_page>4737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1918</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sasu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Antonios</given_name>
<surname>Alexos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raghuveer</given_name>
<surname>Peri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sai Muralidhar</given_name>
<surname>Jayanthi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Metehan</given_name>
<surname>Cekic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Vishnubhotla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kyu J.</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Ronanki</surname>
</person_name>
					</contributors>
					<titles><title>Defending Speech-enabled LLMs Against Adversarial Jailbreak Threats</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2048</first_page>
						<last_page>2052</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1921</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/alexos25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Louise</given_name>
<surname>Coppieters de Gibson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip N.</given_name>
<surname>Garner</surname>
</person_name>
					</contributors>
					<titles><title>Exploring auditory feedback mechanisms in speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4743</first_page>
						<last_page>4747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1924</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/coppietersdegibson25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Francesco</given_name>
<surname>Pierotti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Bandini</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Assessment of Speech Impairment in Amyotrophic Lateral Sclerosis Using Audio-Visual and Machine Learning Approaches</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3743</first_page>
						<last_page>3747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1931</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pierotti25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yen</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sharon</given_name>
<surname>Goldwater</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Tang</surname>
</person_name>
					</contributors>
					<titles><title>Effective Context in Neural Speech Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>246</first_page>
						<last_page>250</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1932</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/meng25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Junjie</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zihao</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chaofan</given_name>
<surname>Ding</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yunming</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yihan</given_name>
<surname>Fan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lei</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinhan</given_name>
<surname>Di</surname>
</person_name>
					</contributors>
					<titles><title>MM-MovieDubber: Towards Multi-Modal Learning for Multi-Modal Movie Dubbing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3758</first_page>
						<last_page>3762</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1933</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zheng25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Maison</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Soulas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie-Jean</given_name>
<surname>Meurs</surname>
</person_name>
					</contributors>
					<titles><title>CEREALES  : a new dataset of Quebec French accented speech with applications to speech recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4058</first_page>
						<last_page>4062</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1934</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/maison25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna</given_name>
<surname>Havras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Mendes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helena</given_name>
<surname>Moniz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gueorgui</given_name>
<surname>Hristovsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>João</given_name>
<surname>Miranda</surname>
</person_name>
					</contributors>
					<titles><title>Exploratory Study of Filled Pauses in Ukrainian Language: Phonetic Properties of Filled Pauses</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4193</first_page>
						<last_page>4197</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1938</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/havras25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Paul</given_name>
<surname>Mayer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Lux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandro</given_name>
<surname>Pérez-González-de-Martos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angelina</given_name>
<surname>Elizarova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lindsey</given_name>
<surname>Vanderlyn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dirk</given_name>
<surname>Väth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ngoc Thang</given_name>
<surname>Vu</surname>
</person_name>
					</contributors>
					<titles><title>Investigating Stochastic Methods for Prosody Modeling in Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>439</first_page>
						<last_page>443</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1940</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mayer25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Louis</given_name>
<surname>Lalay</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathieu</given_name>
<surname>Fontaine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Roland</given_name>
<surname>Badeau</surname>
</person_name>
					</contributors>
					<titles><title>Unified Variational and Physics-aware Model for Room Impulse Response Estimation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3818</first_page>
						<last_page>3822</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1942</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lalay25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Theo</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Madurya</given_name>
<surname>Suresh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne</given_name>
<surname>Warluamont</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kasia</given_name>
<surname>Hitczenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandrina</given_name>
<surname>Cristia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Margaret</given_name>
<surname>Cychosz</surname>
</person_name>
					</contributors>
					<titles><title>Employing self-supervised learning models for cross-linguistic child speech maturity classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2825</first_page>
						<last_page>2829</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1946</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25r_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Joubaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julien</given_name>
<surname>Hauret</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Véronique</given_name>
<surname>Zimpfer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éric</given_name>
<surname>Bavu</surname>
</person_name>
					</contributors>
					<titles><title>French Listening Tests for the Assessment of Intelligibility, Quality, and Identity of Body-Conducted Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5483</first_page>
						<last_page>5487</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1947</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/joubaud25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>De</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuyao</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanrong</given_name>
<surname>He</surname>
</person_name>
					</contributors>
					<titles><title>Joint Reference Microphone Selection and Filter Order Determination in Multi-channel Active Noise Control</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2500</first_page>
						<last_page>2504</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1948</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hu25m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michaela</given_name>
<surname>Watkins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rasmus</given_name>
<surname>Puggaard-Rode</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paul</given_name>
<surname>Boersma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silke</given_name>
<surname>Hamann</surname>
</person_name>
					</contributors>
					<titles><title>Robustness of F0 Ratio as a Diagnostic: Comparing Creaky Voice in Danish and Seoul Korean</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4203</first_page>
						<last_page>4207</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1949</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/watkins25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yixuan</given_name>
<surname>Hou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heyang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuhao</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyang</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ronghua</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qunshan</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanfeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Wang</surname>
</person_name>
					</contributors>
					<titles><title>SOVA-Bench: Benchmarking the Speech Conversation Ability for LLM-based Voice Assistant</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5713</first_page>
						<last_page>5717</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1950</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hou25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Felix</given_name>
<surname>Burkhardt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Schrüfer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Uwe</given_name>
<surname>Reichel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hagen</given_name>
<surname>Wierstorf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Derington</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Eyben</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>EmoDB 2.0: A Database of Emotional Speech in a World that is not Black or White but Grey</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4488</first_page>
						<last_page>4492</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1951</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/burkhardt25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gerard I.</given_name>
<surname>Gállego</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oriol</given_name>
<surname>Pareras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martí</given_name>
<surname>Cortada Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Takanori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Javier</given_name>
<surname>Hernando</surname>
</person_name>
					</contributors>
					<titles><title>Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing Cross-Lingual Transfer in Low-Resource Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>31</first_page>
						<last_page>35</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1954</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gallego25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Huy Ba</given_name>
<surname>Do</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vy Le-Phuong</given_name>
<surname>Huynh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luan Thanh</given_name>
<surname>Nguyen</surname>
</person_name>
					</contributors>
					<titles><title>ViToSA: Audio-Based Toxic Spans Detection on Vietnamese Speech Utterances</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4013</first_page>
						<last_page>4017</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1958</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/do25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hye-jin</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Uni-VERSA: Versatile Speech Assessment with a Unified Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1798</first_page>
						<last_page>1802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1960</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shi25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Sasu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Benedict</given_name>
<surname>Quartey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kweku Andoh</given_name>
<surname>Yamoah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalie</given_name>
<surname>Schluter</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1958</first_page>
						<last_page>1962</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1961</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sasu25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tarek</given_name>
<surname>Kunze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marianne</given_name>
<surname>Métais</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hadrien</given_name>
<surname>Titeux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Elbert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joseph</given_name>
<surname>Coffey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandrina</given_name>
<surname>Cristia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Lavechin</surname>
</person_name>
					</contributors>
					<titles><title>Challenges in Automated Processing of Speech from Child Wearables:  The Case of Voice Type Classifier</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2845</first_page>
						<last_page>2849</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1962</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kunze25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carole</given_name>
<surname>Millot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clara</given_name>
<surname>Ponchard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cédric</given_name>
<surname>Gendrot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-François</given_name>
<surname>Bonastre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Orane</given_name>
<surname>Dufour</surname>
</person_name>
					</contributors>
					<titles><title>Using gender, phonation and age to interpret automatically discovered speech attributes for explainable speaker recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3638</first_page>
						<last_page>3642</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1963</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/millot25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ebru</given_name>
<surname>Arisoy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Merve</given_name>
<surname>Unlu Menevse</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusufcan</given_name>
<surname>Manav</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arzucan</given_name>
<surname>Ozgur</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Large Language Models in Data Generation for Low-Resource Scenarios: A Case Study on Question Answering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1773</first_page>
						<last_page>1777</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1965</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/arisoy25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vishwanath</given_name>
<surname>Pratap Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md</given_name>
<surname>Sahidullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name>
					</contributors>
					<titles><title>Causal Structure Discovery for Error Diagnostics of Children's ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2900</first_page>
						<last_page>2904</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1967</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pratapsingh25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qingkun</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saturnino</given_name>
<surname>Luz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sofia</given_name>
<surname>de la Fuente Garcia</surname>
</person_name>
					</contributors>
					<titles><title>An interpretable speech foundation model for depression detection by revealing prediction-relevant acoustic features from long speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5248</first_page>
						<last_page>5252</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1968</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/deng25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abinay Reddy</given_name>
<surname>Naini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Goncalves</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ali N.</given_name>
<surname>Salman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pravin</given_name>
<surname>Mote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ismail R.</given_name>
<surname>Ulgen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Thebaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano Moro</given_name>
<surname>Velazquez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leibny Paola</given_name>
<surname>Garcia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Berrak</given_name>
<surname>Sisman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>The Interspeech 2025 Challenge on Speech Emotion Recognition in Naturalistic Conditions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4668</first_page>
						<last_page>4672</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1972</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/naini25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dana</given_name>
<surname>Serditova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jochen</given_name>
<surname>Steffens</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Speech Recognition Biases in Newcastle English: an Error Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3204</first_page>
						<last_page>3208</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1973</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/serditova25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Luca</given_name>
<surname>Ducceschi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Greta H.</given_name>
<surname>Franzini</surname>
</person_name>
					</contributors>
					<titles><title>Speech transcription from South Tyrolean Dialect to Standard German with Whisper</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1</first_page>
						<last_page>5</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1976</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ducceschi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wen-Chin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erica</given_name>
<surname>Cooper</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>SHEET: A Multi-purpose Open-source Speech Human Evaluation Estimation Toolkit</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2355</first_page>
						<last_page>2359</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1977</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/huang25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jialong</given_name>
<surname>Mai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofen</given_name>
<surname>Xing</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yangbiao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiangmin</given_name>
<surname>Xu</surname>
</person_name>
					</contributors>
					<titles><title>Chain-of-Thought Distillation with Fine-Grained Acoustic Cues for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5438</first_page>
						<last_page>5442</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1979</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mai25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Weilinghoff</surname>
</person_name>
					</contributors>
					<titles><title>Transcribing Diverse Voices: Using Whisper for ICE corpora</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3359</first_page>
						<last_page>3363</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1980</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/weilinghoff25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harsh</given_name>
<surname>Deshpande</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmy</given_name>
<surname>Phung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmad</given_name>
<surname>Emami</surname>
</person_name>
					</contributors>
					<titles><title>An Exploratory Framework for LLM-assisted Human Annotation of Speech Datasets</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4253</first_page>
						<last_page>4257</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1983</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/johnson25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nathan</given_name>
<surname>Griot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Driss</given_name>
<surname>Matrouf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raphael</given_name>
<surname>Blouet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-François</given_name>
<surname>Bonastre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ana</given_name>
<surname>Mantecon</surname>
</person_name>
					</contributors>
					<titles><title>Unified Text and Speaker Verification using SSL model for Text-Dependent Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1648</first_page>
						<last_page>1652</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1984</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/griot25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saravanakumar</given_name>
<surname>Duraisamy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maurice</given_name>
<surname>Rekrut</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luis A.</given_name>
<surname>Leiva</surname>
</person_name>
					</contributors>
					<titles><title>Functional Connectivity and Hilbert-Based Features for Covert Speech EEG Variability Analysis and Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2915</first_page>
						<last_page>2919</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1986</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/duraisamy25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Loann</given_name>
<surname>Peurey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marvin</given_name>
<surname>Lavechin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tarek</given_name>
<surname>Kunze</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manel</given_name>
<surname>Khentout</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Gautheron</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alejandrina</given_name>
<surname>Cristia</surname>
</person_name>
					</contributors>
					<titles><title>Fifteen Years of Child-Centered Long-Form Recordings: Promises, Resources, and Remaining Challenges to Validity</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3948</first_page>
						<last_page>3952</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1987</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/peurey25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Venkatesh</given_name>
<surname>Parvathala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ramesh</given_name>
<surname>Gundluru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sreekanth</given_name>
<surname>Sankala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K. Sri Rama</given_name>
<surname>Murty</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Bispectral Features for Single-Channel Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2385</first_page>
						<last_page>2389</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1988</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/parvathala25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ambika</given_name>
<surname>Kirkland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Edlund</surname>
</person_name>
					</contributors>
					<titles><title>Who knows best? Effects of speech disfluencies on incentivized decision-making</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4508</first_page>
						<last_page>4512</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1990</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kirkland25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shafique</given_name>
<surname>Ahmed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryandhimas E.</given_name>
<surname>Zezario</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nasir</given_name>
<surname>Saleem</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amir</given_name>
<surname>Hussain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hsin-Min</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Tsao</surname>
</person_name>
					</contributors>
					<titles><title>A Study on Speech Assessment with Visual Cues</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5418</first_page>
						<last_page>5422</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1992</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ahmed25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiankun</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingwei</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengxi</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Defending Unauthorized Voice Cloning with Watermark-Aware Codecs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1573</first_page>
						<last_page>1577</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1993</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhao25k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Aboeitta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Sharshar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youssef</given_name>
<surname>Nafea</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shady</given_name>
<surname>Shehata</surname>
</person_name>
					</contributors>
					<titles><title>Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking Self-Supervised and Generative Approaches </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2123</first_page>
						<last_page>2127</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1994</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/aboeitta25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Darline Monika</given_name>
<surname>Marx</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Matassoni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessio</given_name>
<surname>Brutti</surname>
</person_name>
					</contributors>
					<titles><title>Automatic detection of speech sound disorders in German-speaking children: augmenting the data with typically developed speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2875</first_page>
						<last_page>2879</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1996</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/marx25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yun</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eesung</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vijendra</given_name>
<surname>Raj Apsingekar</surname>
</person_name>
					</contributors>
					<titles><title>Enhanced Hybrid Transducer and Attention Encoder Decoder with Text Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2725</first_page>
						<last_page>2729</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1997</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Claudia</given_name>
<surname>Montero-Ramírez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alba</given_name>
<surname>Martínez-Serrano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jorge</given_name>
<surname>Garcelán-Gómez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francisco J.</given_name>
<surname>Valverde-Albacete</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carmen</given_name>
<surname>Peláez-Moreno</surname>
</person_name>
					</contributors>
					<titles><title>Beyond Conventional Metrics: using Entropic Triangles to Explain Balancing Methods in Acoustic Scene Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1323</first_page>
						<last_page>1327</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1998</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/monteroramirez25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shilpa</given_name>
<surname>Chandra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akansha</given_name>
<surname>Tyagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shiven</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Padmanabhan</given_name>
<surname>Rajan</surname>
</person_name>
					</contributors>
					<titles><title>Beyond Attacks: Advancing Fake Speech Detection with Attack-Agnostic Methods</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4563</first_page>
						<last_page>4567</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-1999</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chandra25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Johnny</given_name>
<surname>Tam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christine</given_name>
<surname>Weaver</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Watts</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddharthan</given_name>
<surname>Chandran</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suvankar</given_name>
<surname>Pal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>Rowling Speech Consortium</surname>
</person_name>
					</contributors>
					<titles><title>Anne Rowling Neurological Speech Corpus: clinically annotated longitudinal dataset for developing speech biomarkers in neurodegenerative disorders</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5693</first_page>
						<last_page>5697</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2000</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tam25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pierre</given_name>
<surname>Falez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tony</given_name>
<surname>Marteau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Damien</given_name>
<surname>Lolive</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arnaud</given_name>
<surname>Delhay</surname>
</person_name>
					</contributors>
					<titles><title>Audio Deepfake Source Tracing using Multi-Attribute Open-Set Identification and Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1528</first_page>
						<last_page>1532</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2001</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/falez25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jie</given_name>
<surname>Zhengjie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaofeng</given_name>
<surname>Cheng</surname>
</person_name>
					</contributors>
					<titles><title>Pinyin-Guided Chinese Speech Recognition with Large Language Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>564</first_page>
						<last_page>568</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2003</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhengjie25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziv</given_name>
<surname>Tamir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Thebaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jesus</given_name>
<surname>Villalba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Najim</given_name>
<surname>Dehak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oren</given_name>
<surname>Kurland</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Emotion Diarization: Frame-Wise Integration of Text and Audio Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4338</first_page>
						<last_page>4342</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2009</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tamir25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Firoj</given_name>
<surname>Alam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md Arid</given_name>
<surname>Hasan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shammur Absar</given_name>
<surname>Chowdhury</surname>
</person_name>
					</contributors>
					<titles><title>SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2685</first_page>
						<last_page>2689</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2011</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/alam25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zengqiang</given_name>
<surname>Shang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mou</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengyuan</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Restoring Harmonics: Enhancing Speech Quality with Deep Mask and Harmonic Restoration Network</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5168</first_page>
						<last_page>5172</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2012</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhao25l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minghan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxia</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thuy-Trang</given_name>
<surname>Vu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ehsan</given_name>
<surname>Shareghi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gholamreza</given_name>
<surname>Haffari</surname>
</person_name>
					</contributors>
					<titles><title>SpeechDialogueFactory: A Framework for Natural Speech Dialogue Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1758</first_page>
						<last_page>1762</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2013</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25x_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lucas</given_name>
<surname>Ueda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>João</given_name>
<surname>Lima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Marques</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paula</given_name>
<surname>Costa</surname>
</person_name>
					</contributors>
					<titles><title>Improving Speech Emotion Recognition Through Cross Modal Attention Alignment and Balanced Stacking Model</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4698</first_page>
						<last_page>4702</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2014</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ueda25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nagarathna</given_name>
<surname>Ravi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thishyan Raj</given_name>
<surname>T</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ravi Teja</given_name>
<surname>Chaganti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vipul</given_name>
<surname>Arora</surname>
</person_name>
					</contributors>
					<titles><title>ASR Confidence Estimation using True Class Lexical Similarity Score</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3658</first_page>
						<last_page>3662</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2016</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ravi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ming</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shilong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hang</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jun</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chin-Hui</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingdong</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sabato Marco</given_name>
<surname>Siniscalchi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name>
					</contributors>
					<titles><title>The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1888</first_page>
						<last_page>1892</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2017</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gao25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Craig</given_name>
<surname>Greenberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukas</given_name>
<surname>Diduch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Audrey</given_name>
<surname>Tong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elliot</given_name>
<surname>Singer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trang</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Dunn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisa</given_name>
<surname>Mason</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beth</given_name>
<surname>Matys</surname>
</person_name>
					</contributors>
					<titles><title>The 2024 NIST Speaker Recognition Evaluation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5748</first_page>
						<last_page>5752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2018</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/greenberg25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ingo</given_name>
<surname>Siegert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Marquenie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sven</given_name>
<surname>Grawunder</surname>
</person_name>
					</contributors>
					<titles><title>Queer Waves: A German Speech Dataset Capturing Gender and Sexual Diversity from Podcasts and YouTube</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>679</first_page>
						<last_page>683</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2022</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/siegert25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Carvalho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinchuan</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Linear Variant Transformers and k-NN Memory Inference for Long-Form ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3583</first_page>
						<last_page>3587</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2025</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/carvalho25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Miao</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aref</given_name>
<surname>Farhadipour</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Annie</given_name>
<surname>Baker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiachen</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bogdan</given_name>
<surname>Pricop</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eleanor</given_name>
<surname>Chodroff</surname>
</person_name>
					</contributors>
					<titles><title>Quantifying and Reducing Speaker Heterogeneity within the Common Voice Corpus for Phonetic Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3933</first_page>
						<last_page>3937</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2027</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anindita</given_name>
<surname>Mondal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rahul</given_name>
<surname>Biju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anil Kumar</given_name>
<surname>Vuppala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reni K</given_name>
<surname>Cherian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiranjeevi</given_name>
<surname>Yarra</surname>
</person_name>
					</contributors>
					<titles><title>ProBiEM: Acoustic and Lexical Correlates of Prosodic Prominence in English-Malayalam Bilingual Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5368</first_page>
						<last_page>5372</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2028</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mondal25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>N</given_name>
<surname>Shashaank</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiao</given_name>
<surname>Quan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Kaluzny</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonard</given_name>
<surname>Varghese</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marko</given_name>
<surname>Stamenovic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuan-Che</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>Towards Secure User Authentication for Headphones via In-Ear or In-Earcup Microphones</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1658</first_page>
						<last_page>1662</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2029</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shashaank25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>David</given_name>
<surname>Palzer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Maciejewski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>Fosler-Lussier</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Diarization utilizing Attractor Deep Clustering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1593</first_page>
						<last_page>1597</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2030</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/palzer25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Utkarsh</given_name>
<surname>Pathak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandra Sai Krishna</given_name>
<surname>Gunda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anusha</given_name>
<surname>Prakash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keshav</given_name>
<surname>Agarwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hema A.</given_name>
<surname>Murthy</surname>
</person_name>
					</contributors>
					<titles><title>Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4133</first_page>
						<last_page>4137</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2031</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pathak25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anindita</given_name>
<surname>Mondal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monica</given_name>
<surname>Surtani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anil Kumar</given_name>
<surname>Vuppala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Parameswari</given_name>
<surname>Krishnamurthy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiranjeevi</given_name>
<surname>Yarra</surname>
</person_name>
					</contributors>
					<titles><title>ExagTTS: An Approach Towards Controllable Word Stress Incorporated TTS for Exaggerated Synthesized Speech Aiding Second Language Learners</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>449</first_page>
						<last_page>453</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2032</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mondal25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bartłomiej</given_name>
<surname>Zgórzyński</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juliusz</given_name>
<surname>Wójtowicz-Kruk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Piotr</given_name>
<surname>Masztalski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Władysław</given_name>
<surname>Średniawa</surname>
</person_name>
					</contributors>
					<titles><title>Multi-task learning for speech emotion recognition in naturalistic conditions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4678</first_page>
						<last_page>4682</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2033</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zgorzynski25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna Seo Gyeong</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Richardson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryan</given_name>
<surname>Partlan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunny X.</given_name>
<surname>Tang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunghye</given_name>
<surname>Cho</surname>
</person_name>
					</contributors>
					<titles><title>Comparative Evaluation of Acoustic Feature Extraction Tools for Clinical Speech Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>524</first_page>
						<last_page>528</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2034</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/choi25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sean</given_name>
<surname>Foley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jihwan</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoonjeong</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dani</given_name>
<surname>Byrd</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>On the Relationship between Accent Strength and Articulatory Features</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4538</first_page>
						<last_page>4542</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2037</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/huang25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Valeska</given_name>
<surname>Slomianka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>May</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Torsten</given_name>
<surname>Dau</surname>
</person_name>
					</contributors>
					<titles><title>Impact of Background Noise on Turn-Taking Dynamics in Triadic Conversations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3025</first_page>
						<last_page>3029</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2038</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/slomianka25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Lobashev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Assel</given_name>
<surname>Yermekova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Larchenko</surname>
</person_name>
					</contributors>
					<titles><title>Training-Free Voice Conversion with Factorized Optimal Transport</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1373</first_page>
						<last_page>1377</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2043</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lobashev25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Greta</given_name>
<surname>Tuckute</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Klemen</given_name>
<surname>Kotar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Evelina</given_name>
<surname>Fedorenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Yamins</surname>
</person_name>
					</contributors>
					<titles><title>Representing Speech Through Autoregressive Prediction of Cochlear Tokens</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2180</first_page>
						<last_page>2184</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2044</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tuckute25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Roseline</given_name>
<surname>Polle</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Agnes</given_name>
<surname>Norbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandra Livia</given_name>
<surname>Georgescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefano</given_name>
<surname>Goria</surname>
</person_name>
					</contributors>
					<titles><title>Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2038</first_page>
						<last_page>2042</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2055</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/polle25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fritz</given_name>
<surname>Peters</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>W Richard</given_name>
<surname>Bevan-Jones</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Grace</given_name>
<surname>Threlfall</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jenny M</given_name>
<surname>Harris</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julie S</given_name>
<surname>Snowden</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Jones</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer C</given_name>
<surname>Thompson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel J</given_name>
<surname>Blackburn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heidi</given_name>
<surname>Christensen</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Detection and Sub-typing of Primary Progressive Aphasia from Speech: Integrating Task-Specific Features and Spatio-Semantic Graphs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5288</first_page>
						<last_page>5292</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2056</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/peters25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Martin</given_name>
<surname>Ratajczak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Philippe</given_name>
<surname>Robichaud</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jennifer</given_name>
<surname>Drexler Fox</surname>
</person_name>
					</contributors>
					<titles><title>Accurate, fast, cheap: Choose three. Replacing Multi-Head-Attention with Bidirectional Recurrent Attention for Long-Form ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3324</first_page>
						<last_page>3328</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2059</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ratajczak25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maxim</given_name>
<surname>Markitantov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elena</given_name>
<surname>Ryumina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heysem</given_name>
<surname>Kaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexey</given_name>
<surname>Karpov</surname>
</person_name>
					</contributors>
					<titles><title>Multi-Modal Multi-Task Affective States Recognition Based on Label Encoder Fusion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3010</first_page>
						<last_page>3014</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2060</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/markitantov25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amir</given_name>
<surname>Hussein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sameer</given_name>
<surname>Khurana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gordon</given_name>
<surname>Wichern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François G.</given_name>
<surname>Germain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Le Roux</surname>
</person_name>
					</contributors>
					<titles><title>HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5393</first_page>
						<last_page>5397</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2063</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hussein25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anton</given_name>
<surname>Firc</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Manasi</given_name>
<surname>Chhibber</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagabandhu</given_name>
<surname>Mishra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vishwanath</given_name>
<surname>Pratap Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kamil</given_name>
<surname>Malinka</surname>
</person_name>
					</contributors>
					<titles><title>STOPA: A Dataset of Systematic VariaTion Of DeePfake Audio for Open-Set Source Tracing and Attribution</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1553</first_page>
						<last_page>1557</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2065</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/firc25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sarenne</given_name>
<surname>Wallbridge</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Minixhofer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Lai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4723</first_page>
						<last_page>4727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2068</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wallbridge25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karl</given_name>
<surname>El Hajal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Enno</given_name>
<surname>Hermann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sevada</given_name>
<surname>Hovsepyan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew Magimai</given_name>
<surname>Doss</surname>
</person_name>
					</contributors>
					<titles><title>Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2760</first_page>
						<last_page>2764</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2069</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/elhajal25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mathilde</given_name>
<surname>Hutin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mélanie</given_name>
<surname>Lancien</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noam</given_name>
<surname>Faust</surname>
</person_name>
					</contributors>
					<titles><title>French schwa is not acoustically distinct  from its two lexical neighbors /ø/ and /œ/</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4748</first_page>
						<last_page>4752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2070</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hutin25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pradyoth</given_name>
<surname>Hegde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Santosh</given_name>
<surname>Kesiraju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ján</given_name>
<surname>Švec</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Šimon</given_name>
<surname>Sedláček</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bolaji</given_name>
<surname>Yusuf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deepak</given_name>
<surname>K T</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Factors affecting the in-context learning abilities of LLMs for dialogue state tracking</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4818</first_page>
						<last_page>4822</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2071</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hegde25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xize</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zehan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rongjie</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Huadai</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tao</given_name>
<surname>Jin</surname>
</person_name>
					</contributors>
					<titles><title>Robust Speech-Driven Body Language Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5033</first_page>
						<last_page>5037</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2073</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cheng25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuqing</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiyuan</given_name>
<surname>Gao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shekhar</given_name>
<surname>Nayak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Coler</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3973</first_page>
						<last_page>3977</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2074</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25ba_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Sanders</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanchao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Segmentation-Variant Codebooks for Preservation of Paralinguistic and Prosodic Information</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5403</first_page>
						<last_page>5407</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2075</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sanders25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Serre</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathieu</given_name>
<surname>Fontaine</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eric</given_name>
<surname>Benhaim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Slim</given_name>
<surname>Essid</surname>
</person_name>
					</contributors>
					<titles><title>MTSE: Multi-Target Speaker Extraction for Conversation Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2970</first_page>
						<last_page>2974</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2077</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/serre25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ajinkya</given_name>
<surname>Kulkarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandipana</given_name>
<surname>Dowerah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanel</given_name>
<surname>Alumäe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew Magimai</given_name>
<surname>Doss</surname>
</person_name>
					</contributors>
					<titles><title>Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1533</first_page>
						<last_page>1537</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2079</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kulkarni25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Filomene</given_name>
<surname>Roquefort</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandre</given_name>
<surname>Ducorroy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rachid</given_name>
<surname>Riad</surname>
</person_name>
					</contributors>
					<titles><title>In-context learning capabilities of Large Language Models to detect suicide risk among adolescents from speech transcripts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>414</first_page>
						<last_page>418</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2083</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/roquefort25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ruochu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Milind</given_name>
<surname>Rao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harshavardhan</given_name>
<surname>Sundar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anirudh</given_name>
<surname>Raju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aparna</given_name>
<surname>Khare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srinath</given_name>
<surname>Tankasala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Di</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Venkatesh</given_name>
<surname>Ravichandran</surname>
</person_name>
					</contributors>
					<titles><title>On Retrieval of Long Audios with Complex Text Queries</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2660</first_page>
						<last_page>2664</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2085</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oana</given_name>
<surname>Niculescu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monica</given_name>
<surname>Vasileanu</surname>
</person_name>
					</contributors>
					<titles><title>Prolongation in Romanian</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>379</first_page>
						<last_page>383</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2087</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/niculescu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shengyue</given_name>
<surname>Xiong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhe-chen</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bharath</given_name>
<surname>Chandrasekaran</surname>
</person_name>
					</contributors>
					<titles><title>Language and Accent Familiarity Effects on the Use of Acoustic Cues in Talker Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1293</first_page>
						<last_page>1297</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2088</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xiong25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zuzanna</given_name>
<surname>Miodonska</surname>
</person_name>
					</contributors>
					<titles><title>Hybrid HMM-SVM classifier using frication-based features for detection of non-normative sibilant articulation patterns in Polish children’s speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3563</first_page>
						<last_page>3567</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2089</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/miodonska25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juliette</given_name>
<surname>Dindart</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Agnès</given_name>
<surname>Rouxel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Crystal</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trung Kien</given_name>
<surname>Bui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muriel</given_name>
<surname>Lefort</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Claire</given_name>
<surname>Pillot-Loiseau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christophe</given_name>
<surname>Trésallet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Frédérique</given_name>
<surname>Frouin</surname>
</person_name>
					</contributors>
					<titles><title>Study of vocal fold vibration using M-mode ultrasound: a proof of concept</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>315</first_page>
						<last_page>319</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2091</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dindart25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Edmilson</given_name>
<surname>Morais</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hagai</given_name>
<surname>Aronowitz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aharon</given_name>
<surname>Satt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ron</given_name>
<surname>Hoory</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Avihu</given_name>
<surname>Dekel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brian</given_name>
<surname>Kingsbury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>George</given_name>
<surname>Saon</surname>
</person_name>
					</contributors>
					<titles><title>Exploring the Limits of Conformer CTC-Encoder for Speech Emotion Recognition using Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5443</first_page>
						<last_page>5447</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2093</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/morais25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Parismita</given_name>
<surname>Gogoi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vishwanath</given_name>
<surname>Pratap Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seema</given_name>
<surname>Khadirnaikar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soma</given_name>
<surname>Siddhartha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sishir</given_name>
<surname>Kalita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagabandhu</given_name>
<surname>Mishra</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Md</given_name>
<surname>Sahidullah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyankoo</given_name>
<surname>Sarmah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S. R. M.</given_name>
<surname>Prasanna</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging AM and FM Rhythm Spectrograms for Dementia Classification and Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>539</first_page>
						<last_page>543</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2097</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gogoi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Akinrintoyo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nadine</given_name>
<surname>Abdelhalim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicole</given_name>
<surname>Salomons</surname>
</person_name>
					</contributors>
					<titles><title>WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1413</first_page>
						<last_page>1417</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2099</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/akinrintoyo25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Petr</given_name>
<surname>Grinberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ankur</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Surya</given_name>
<surname>Koppisetti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gaurav</given_name>
<surname>Bharaj</surname>
</person_name>
					</contributors>
					<titles><title>A Data-Driven Diffusion-based Approach for Audio Deepfake Explanations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5348</first_page>
						<last_page>5352</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2105</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/grinberg25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anton</given_name>
<surname>Mitrofanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sergey</given_name>
<surname>Novoselov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatiana</given_name>
<surname>Prisyach</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladislav</given_name>
<surname>Marchevskiy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arseniy</given_name>
<surname>Karelin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikita</given_name>
<surname>Khmelev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dmitry</given_name>
<surname>Dutov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stepan</given_name>
<surname>Malykh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Igor</given_name>
<surname>Agafonov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aleksandr</given_name>
<surname>Nikitin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Petrov</surname>
</person_name>
					</contributors>
					<titles><title>Cryfish: On deep audio analysis with Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3249</first_page>
						<last_page>3253</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2109</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mitrofanov25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chih-Ning</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu-Lan</given_name>
<surname>Chuang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming-Jhang</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Cheng</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yung-An</given_name>
<surname>Tsou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Wen</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>Phonetic Posteriorgram-Based Phoneme Selection for Vocal Cord Disorder Classification in Continuous Mandarin Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3553</first_page>
						<last_page>3557</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2110</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25n_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hsin-Tien</given_name>
<surname>Chiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>A Deformable Convolution GAN Approach for Speech Dereverberation in Cochlear Implant Users</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>833</first_page>
						<last_page>837</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2111</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chiang25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pravin</given_name>
<surname>Mote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abinay Reddy</given_name>
<surname>Naini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donita</given_name>
<surname>Robinson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elizabeth</given_name>
<surname>Richerson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of Phonetic Level Similarities Across Languages in Emotional Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4343</first_page>
						<last_page>4347</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2112</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mote25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ajinkya</given_name>
<surname>Kulkarni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francisco</given_name>
<surname>Teixeira</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Enno</given_name>
<surname>Hermann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Rolland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isabel</given_name>
<surname>Trancoso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mathew Magimai</given_name>
<surname>Doss</surname>
</person_name>
					</contributors>
					<titles><title>Children's Voice Privacy: First Steps and Emerging Challenges</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2810</first_page>
						<last_page>2814</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2117</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kulkarni25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Spandan</given_name>
<surname>Dey</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hirak</given_name>
<surname>Mondal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjay Kumar</given_name>
<surname>Kurmi</surname>
</person_name>
					</contributors>
					<titles><title>Teacher-Free Knowledge Distillation for Improving Short-Utterance Spoken Language Identification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1483</first_page>
						<last_page>1487</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2120</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dey25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hadi</given_name>
<surname>Alizadeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rahil</given_name>
<surname>Mahdian Toroghi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hassan</given_name>
<surname>Zareian</surname>
</person_name>
					</contributors>
					<titles><title>ReSepNet: A Unified-Light Model for Recursive Speech Separation with Unknown Speaker Count</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1458</first_page>
						<last_page>1462</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2121</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/alizadeh25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pravin</given_name>
<surname>Mote</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donita</given_name>
<surname>Robinson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elizabeth</given_name>
<surname>Richerson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Busso</surname>
</person_name>
					</contributors>
					<titles><title>Vector Quantized Cross-lingual Unsupervised Domain Adaptation  for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>126</first_page>
						<last_page>130</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2123</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mote25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carlos</given_name>
<surname>Hartmann</surname>
</person_name>
					</contributors>
					<titles><title>Reddit FlairShare: A Human-Annotated Dataset of Gender-Progressive Online Discourse</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>684</first_page>
						<last_page>688</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2128</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hartmann25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Parismita</given_name>
<surname>Gogoi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sishir</given_name>
<surname>Kalita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wendy</given_name>
<surname>Lalhminghlui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viyazonuo</given_name>
<surname>Terhiija</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Moakala</given_name>
<surname>Tzudir</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyankoo</given_name>
<surname>Sarmah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S. R. M.</given_name>
<surname>Prasanna</surname>
</person_name>
					</contributors>
					<titles><title>Tone recognition in low-resource languages of North-East India: peeling the layers of SSL-based speech models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4173</first_page>
						<last_page>4177</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2135</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gogoi25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Laura</given_name>
<surname>Lechler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chamran</given_name>
<surname>Moradi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivana</given_name>
<surname>Balic</surname>
</person_name>
					</contributors>
					<titles><title>Crowdsourcing MUSHRA Tests in the Age of Generative Speech Technologies: A Comparative Analysis of Subjective and Objective Testing Methods</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3160</first_page>
						<last_page>3164</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2138</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lechler25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Weiqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taejin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Medennikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinhan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kunal</given_name>
<surname>Dhawan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>He</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nithin</given_name>
<surname>Rao Koluguri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagadeesh</given_name>
<surname>Balam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Targeting via Self-Speaker Adaptation for Multi-talker ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5498</first_page>
						<last_page>5502</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2142</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25y_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gabriel</given_name>
<surname>Ivucic</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurav</given_name>
<surname>Pahuja</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dashanka</given_name>
<surname>Da Silva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name>
					</contributors>
					<titles><title>Selective Auditory Attention Decoding in Naturalistic Conversations Using EEG-Based Speech Envelope Tracking in Multi-Speaker Environments</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2925</first_page>
						<last_page>2929</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2143</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ivucic25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ibrahim</given_name>
<surname>Ibrahimov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Csaba</given_name>
<surname>Zainkó</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gábor</given_name>
<surname>Gosztolya</surname>
</person_name>
					</contributors>
					<titles><title>Conformer-based Ultrasound-to-Speech Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5578</first_page>
						<last_page>5582</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2147</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ibrahimov25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mina</given_name>
<surname>Serajian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saeed Najafzadeh</given_name>
<surname>Rahaghi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hadi</given_name>
<surname>Veisi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saman</given_name>
<surname>Haratizadeh</surname>
</person_name>
					</contributors>
					<titles><title>FaVC: A Validated, Transcribed, Parallel Farsi Speech Dataset for Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4788</first_page>
						<last_page>4792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2151</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/serajian25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilja</given_name>
<surname>Baumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Natalie</given_name>
<surname>Engert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seanie</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elmar</given_name>
<surname>Nöth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name>
					</contributors>
					<titles><title>Personalized Fine-Tuning with Controllable Synthetic Speech from LLM-Generated Transcripts for Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3294</first_page>
						<last_page>3298</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2155</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wagner25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ha Eun</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olivia</given_name>
<surname>Yung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paige</given_name>
<surname>Tuttösí</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boey</given_name>
<surname>Kwan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angelica</given_name>
<surname>Lim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>H. Henny</given_name>
<surname>Yeung</surname>
</person_name>
					</contributors>
					<titles><title>Generating Consistent Prosodic Patterns from Open-Source TTS Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5383</first_page>
						<last_page>5387</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2159</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shim25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sophia</given_name>
<surname>Fünfgeld</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Angelika</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katharina</given_name>
<surname>Zahner-Ritter</surname>
</person_name>
					</contributors>
					<titles><title>Are You Being Sarcastic? Prosodic Cues to Irony Perception in German</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5373</first_page>
						<last_page>5377</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2161</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/funfgeld25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tanya</given_name>
<surname>Talkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kan</given_name>
<surname>Kawabata</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Connor</given_name>
<surname>Higgins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sean</given_name>
<surname>Tobyne</surname>
</person_name>
					</contributors>
					<titles><title>Development and Validation of a Wav2Vec 2.0-Based Cross-Language Methodology for Measurement of Articulatory Precision</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3748</first_page>
						<last_page>3752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2162</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/talkar25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Fang</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yin</given_name>
<surname>Cao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Face2VoiceSync: Lightweight Face-Voice Consistency for Text-Driven Talking Face Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3768</first_page>
						<last_page>3772</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2163</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kang25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>John</given_name>
<surname>Alderete</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Macarious Kin Fung</given_name>
<surname>Hui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aanchan</given_name>
<surname>Mohan</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating ASR Robustness to Spontaneous Speech Errors: A Study of WhisperX Using a Speech Error Database</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1803</first_page>
						<last_page>1807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2164</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/alderete25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexandre</given_name>
<surname>Ferro Filho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Diogo</given_name>
<surname>Fernandes Costa Silva</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pedro Elias</given_name>
<surname>Engelberg Silva Borges</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arlindo Rodrigues</given_name>
<surname>Galvão Filho</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Deep Speaker Embedding Robustness to Domain, Sampling Rate, and Codec Variations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1113</first_page>
						<last_page>1117</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2167</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ferrofilho25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Venkatesh</given_name>
<surname>Parvathala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K. Sri Rama</given_name>
<surname>Murty</surname>
</person_name>
					</contributors>
					<titles><title>MSFNet: A Nested Model for Multi-Sampling-Frequency Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5138</first_page>
						<last_page>5142</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2168</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/parvathala25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Stepan</given_name>
<surname>Malykh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Anikin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikita</given_name>
<surname>Khmelev</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasia</given_name>
<surname>Korenevskaya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasia</given_name>
<surname>Zorkina</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sergey</given_name>
<surname>Novoselov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladislav</given_name>
<surname>Marchevskiy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vladimir</given_name>
<surname>Volokhov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrey</given_name>
<surname>Shulipa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Kozlov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Melnikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vasiliy</given_name>
<surname>Galyuk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Timur</given_name>
<surname>Pekhovskiy</surname>
</person_name>
					</contributors>
					<titles><title>STCON NIST SRE24 System: Composite Speaker Recognition Solution for Challenging Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3983</first_page>
						<last_page>3987</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2170</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/malykh25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Orchid Chetia</given_name>
<surname>Phukan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>Girish</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohd Mujtaba</given_name>
<surname>Akhtar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swarup Ranjan</given_name>
<surname>Behera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyabrata</given_name>
<surname>Mallick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pailla Balakrishna</given_name>
<surname>Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun Balaji</given_name>
<surname>Buduru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajesh</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>Towards Source Attribution of Singing Voice Deepfake with Multimodal Foundation Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1673</first_page>
						<last_page>1677</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2171</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/phukan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Gebauer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lars</given_name>
<surname>Rumberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lars</given_name>
<surname>Köhn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanna</given_name>
<surname>Ehlert</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Edith</given_name>
<surname>Beaulac</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jörn</given_name>
<surname>Ostermann</surname>
</person_name>
					</contributors>
					<titles><title>Grammatical Error Detection on Spontaneous Children's Speech Using Iterative Pseudo Labeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2865</first_page>
						<last_page>2869</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2174</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gebauer25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Szymon</given_name>
<surname>Szmajdziński</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juliusz</given_name>
<surname>Wójtowicz-Kruk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Ryzhankow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Łukasz</given_name>
<surname>Łazarski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jakub</given_name>
<surname>Żak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Władysław</given_name>
<surname>Średniawa</surname>
</person_name>
					</contributors>
					<titles><title>Significance of Time-Frequency preprocessing for automatic Ultrasonic Vocalization classification in Autism Spectrum Disorder model detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1723</first_page>
						<last_page>1727</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2175</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/szmajdzinski25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Quang Minh</given_name>
<surname>Dinh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hoda</given_name>
<surname>Rezaee Kaviani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mehrdad</given_name>
<surname>Hosseinzadeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuanhao</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Extended Loss: Incorporating Long Context into Training Models when using Short Audio Frames</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>778</first_page>
						<last_page>782</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2177</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dinh25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Maciejewski</surname>
</person_name>
					</contributors>
					<titles><title>Speaker Conditioning of Voice Activity Detection via Implicit Separation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5798</first_page>
						<last_page>5802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2178</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/maciejewski25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Patrik</given_name>
<surname>Hrabánek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michaela</given_name>
<surname>Watkins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Silke</given_name>
<surname>Hamann</surname>
</person_name>
					</contributors>
					<titles><title>The function of creaky voice in South Korean: A perception study</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2310</first_page>
						<last_page>2314</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2179</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hrabanek25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Asim</given_name>
<surname>Ersoy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Basel Ahmad</given_name>
<surname>Mousi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shammur Absar</given_name>
<surname>Chowdhury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Firoj</given_name>
<surname>Alam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fahim I</given_name>
<surname>Dalvi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nadir</given_name>
<surname>Durrani</surname>
</person_name>
					</contributors>
					<titles><title>From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>241</first_page>
						<last_page>245</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2180</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ersoy25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Eray</given_name>
<surname>Eren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingju</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyeongwoo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Garrido</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abeer</given_name>
<surname>Alwan</surname>
</person_name>
					</contributors>
					<titles><title>ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>429</first_page>
						<last_page>433</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2189</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/eren25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Kuhlmann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fritz</given_name>
<surname>Seebauer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petra</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Reinhold</given_name>
<surname>Haeb-Umbach</surname>
</person_name>
					</contributors>
					<titles><title>Towards Frame-level Quality Predictions of Synthetic Speech </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2300</first_page>
						<last_page>2304</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2190</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kuhlmann25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Orchid Chetia</given_name>
<surname>Phukan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>Girish</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohd Mujtaba</given_name>
<surname>Akhtar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swarup Ranjan</given_name>
<surname>Behera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pailla Balakrishna</given_name>
<surname>Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun Balaji</given_name>
<surname>Buduru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajesh</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>HYFuse: Aligning Heterogeneous Speech Pre-Trained Representations in Hyperbolic Space for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>131</first_page>
						<last_page>135</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2191</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/phukan25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>John</given_name>
<surname>McGahay</surname>
</person_name>
					</contributors>
					<titles><title>Modeling Vowel System Typology Using Iterated Confusion Minimization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2955</first_page>
						<last_page>2959</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2192</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mcgahay25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Phuoc Hoang</given_name>
<surname>Ho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dragoș Alexandru</given_name>
<surname>Bălan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dirk K. J.</given_name>
<surname>Heylen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khiet P.</given_name>
<surname>Truong</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Transcripts of Open-Source Automatic Speech Recognition Models Through Fine-Tuning with Laughter and Speech-Laugh</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4513</first_page>
						<last_page>4517</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2193</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ho25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Katerina</given_name>
<surname>Papadimitriou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gerasimos</given_name>
<surname>Potamianos</surname>
</person_name>
					</contributors>
					<titles><title>A Multi-Stream Framework Utilizing 3D Human Reconstruction for Cued Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4543</first_page>
						<last_page>4547</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2195</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/papadimitriou25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sivakumar</given_name>
<surname>Balasubramanian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jose Antonio</given_name>
<surname>Jimenez Amador</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaustubh</given_name>
<surname>Kalgaonkar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>King Wei</given_name>
<surname>Hor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Srinivasan</surname>
</person_name>
					</contributors>
					<titles><title>SMARTMOS: Modeling Subjective Audio Quality Evaluation for Real-Time Applications</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3165</first_page>
						<last_page>3169</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2198</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/balasubramanian25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Etienne</given_name>
<surname>Gaudrain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarah</given_name>
<surname>Verhulst</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deniz</given_name>
<surname>Başkent</surname>
</person_name>
					</contributors>
					<titles><title>Speech stimulus design to study the neural coding of speech and the impact of cochlear synaptopathy</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1283</first_page>
						<last_page>1287</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2199</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gaudrain25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Venkatesh</given_name>
<surname>Parvathala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K. Sri Rama</given_name>
<surname>Murty</surname>
</person_name>
					</contributors>
					<titles><title>Dynamic Layer Gating for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2365</first_page>
						<last_page>2369</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2200</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/parvathala25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andre</given_name>
<surname>Batchelder-Schwab</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vasileios</given_name>
<surname>Michos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Barnes</surname>
</person_name>
					</contributors>
					<titles><title>Stress in Spoken and Whistled Greek</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>389</first_page>
						<last_page>393</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2203</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/batchelderschwab25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Orchid Chetia</given_name>
<surname>Phukan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>Girish</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohd Mujtaba</given_name>
<surname>Akhtar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swarup Ranjan</given_name>
<surname>Behera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyabrata</given_name>
<surname>Mallick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Santanu</given_name>
<surname>Roy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun Balaji</given_name>
<surname>Buduru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajesh</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>Towards Fusion of Neural Audio Codec-based Representations with Spectral for Heart Murmur Classification via Bandit-based Cross-Attention Mechanism</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2023</first_page>
						<last_page>2027</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2206</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/phukan25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinda</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aanchan</given_name>
<surname>Mohan</surname>
</person_name>
					</contributors>
					<titles><title>Towards atypical speech transcription using LLM-based ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>574</first_page>
						<last_page>578</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2214</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pooneh</given_name>
<surname>Mousavi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shubham</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cem</given_name>
<surname>Subakan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirco</given_name>
<surname>Ravanelli</surname>
</person_name>
					</contributors>
					<titles><title>LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3244</first_page>
						<last_page>3248</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2218</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mousavi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Teng</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sile</given_name>
<surname>Yin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li-Chia</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuo</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual Representations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>61</first_page>
						<last_page>65</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2223</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ma25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Emma Cathrine Liisborg</given_name>
<surname>Leschly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oliver</given_name>
<surname>Roesler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jackson</given_name>
<surname>Liscombe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abhishek</given_name>
<surname>Hosamath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lakshmi</given_name>
<surname>Arbatti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Line H.</given_name>
<surname>Clemmensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Melanie</given_name>
<surname>Ganz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name>
					</contributors>
					<titles><title>An Exploration of Interpretable Deep Learning Models for the Assessment of Mild Cognitive Impairment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>271</first_page>
						<last_page>275</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2225</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/leschly25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Orchid Chetia</given_name>
<surname>Phukan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>Girish</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohd Mujtaba</given_name>
<surname>Akhtar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swarup Ranjan</given_name>
<surname>Behera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pailla Balakrishna</given_name>
<surname>Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun Balaji</given_name>
<surname>Buduru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajesh</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>Investigating the Reasonable Effectiveness of Speaker Pre-Trained Models and their Synergistic Power for SingMOS Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3090</first_page>
						<last_page>3094</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2227</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/phukan25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maxwell</given_name>
<surname>Hope</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name>
					</contributors>
					<titles><title>Voices of `cyborg awesomeness': Posthuman embodiment of nonbinary gender expression in AI speech technologies</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>689</first_page>
						<last_page>693</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2229</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hope25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Orchid Chetia</given_name>
<surname>Phukan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohd Mujtaba</given_name>
<surname>Akhtar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>Girish</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swarup Ranjan</given_name>
<surname>Behera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaya Sai Kiran</given_name>
<surname>Patibandla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun Balaji</given_name>
<surname>Buduru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajesh</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>PARROT: Synergizing Mamba and Attention-based SSL Pre-Trained Models via Parallel Branch Hadamard Optimal Transport for Speech Emotion Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4468</first_page>
						<last_page>4472</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2233</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/phukan25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ivan</given_name>
<surname>Medennikov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Taejin</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weiqing</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>He</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kunal</given_name>
<surname>Dhawan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinhan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jagadeesh</given_name>
<surname>Balam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Boris</given_name>
<surname>Ginsburg</surname>
</person_name>
					</contributors>
					<titles><title>Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization with Arrival-Time Ordering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5238</first_page>
						<last_page>5242</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2244</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/medennikov25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Francesco</given_name>
<surname>Verdini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pierfrancesco</given_name>
<surname>Melucci</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefano</given_name>
<surname>Perna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francesco</given_name>
<surname>Cariaggi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco</given_name>
<surname>Gaido</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Papi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szymon</given_name>
<surname>Mazurek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marek</given_name>
<surname>Kasztelnik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luisa</given_name>
<surname>Bentivogli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastien</given_name>
<surname>Bratières</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paolo</given_name>
<surname>Merialdo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simone</given_name>
<surname>Scardapane</surname>
</person_name>
					</contributors>
					<titles><title>How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1813</first_page>
						<last_page>1817</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2245</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/verdini25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anna</given_name>
<surname>Stein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Tang</surname>
</person_name>
					</contributors>
					<titles><title>Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>330</first_page>
						<last_page>334</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2246</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/stein25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Brian</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Injy</given_name>
<surname>Hamed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuichiro</given_name>
<surname>Shimizu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vasista Sai</given_name>
<surname>Lodagala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Olga</given_name>
<surname>Iakovenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bashar</given_name>
<surname>Talafha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amir</given_name>
<surname>Hussein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Polok</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kalvin</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Klement</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sara</given_name>
<surname>Althubaiti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Puyuan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Wiesner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thamar</given_name>
<surname>Solorio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Ali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjeev</given_name>
<surname>Khudanpur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>743</first_page>
						<last_page>747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2247</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yan25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Orchid Chetia</given_name>
<surname>Phukan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohd Mujtaba</given_name>
<surname>Akhtar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>Girish</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swarup Ranjan</given_name>
<surname>Behera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Abu Osama</given_name>
<surname>Siddiqui</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sarthak</given_name>
<surname>Jain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Priyabrata</given_name>
<surname>Mallick</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaya Sai Kiran</given_name>
<surname>Patibandla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pailla Balakrishna</given_name>
<surname>Reddy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun Balaji</given_name>
<surname>Buduru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajesh</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>SNIFR : Boosting Fine-Grained Child Harmful Content Detection Through Audio-Visual Alignment with Cascaded Cross-Transformer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2700</first_page>
						<last_page>2704</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2251</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/phukan25f_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zofia</given_name>
<surname>Malisz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Foremski</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Magłorzata</given_name>
<surname>Kul</surname>
</person_name>
					</contributors>
					<titles><title>Contextual predictability effects on acoustic distinctiveness in read Polish speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>335</first_page>
						<last_page>339</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2256</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/malisz25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Le Xuan</given_name>
<surname>Chan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Annika</given_name>
<surname>Heuser</surname>
</person_name>
					</contributors>
					<titles><title>Relative cue weighting in multilingual stop voicing production</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>116</first_page>
						<last_page>120</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2259</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ömer Tarik</given_name>
<surname>Özyilmaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matt</given_name>
<surname>Coler</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matias</given_name>
<surname>Valdenegro-Toro</surname>
</person_name>
					</contributors>
					<titles><title>Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1158</first_page>
						<last_page>1162</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2260</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ozyilmaz25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sumaya Ahmed</given_name>
<surname>Salihs</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Isaac</given_name>
<surname>Wiafe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jamal-Deen</given_name>
<surname>Abdulai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elikem Doe</given_name>
<surname>Atsakpo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gifty</given_name>
<surname>Ayoka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Cave</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akon Obu</given_name>
<surname>Ekpezu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Catherine</given_name>
<surname>Holloway</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Katrin</given_name>
<surname>Tomanek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fiifi Baffoe Payin</given_name>
<surname>Winful</surname>
</person_name>
					</contributors>
					<titles><title>A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4623</first_page>
						<last_page>4627</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2261</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/salihs25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mattias</given_name>
<surname>Nilsson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Riccardo</given_name>
<surname>Miccini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julian</given_name>
<surname>Rossbroich</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clément</given_name>
<surname>Laroche</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Piechowiak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Friedemann</given_name>
<surname>Zenke</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Streaming Speech Quality Prediction with Spiking Neural Networks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5423</first_page>
						<last_page>5427</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2269</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/nilsson25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Meike</given_name>
<surname>Rommel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Míša</given_name>
<surname>Hejná</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicole</given_name>
<surname>Dehé</surname>
</person_name>
					</contributors>
					<titles><title>Pre-aspiration in Iceland Is Conditioned by Gender/Sex</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3354</first_page>
						<last_page>3358</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2271</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/rommel25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Kothare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Beverly</given_name>
<surname>Insel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anzalee</given_name>
<surname>Khan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Danyah</given_name>
<surname>Nadim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Pierre</given_name>
<surname>Lindenmayer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Speech, Language and Orofacial Analysis for Remote Assessment of Positive, Negative and Cognitive Symptoms in Schizophrenia</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5703</first_page>
						<last_page>5707</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2272</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/neumann25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Franziska</given_name>
<surname>Braun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Witzl</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Erzigkeit</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hartmut</given_name>
<surname>Lehfeld</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Hillemacher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name>
					</contributors>
					<titles><title>Pitfalls and Limits in Automatic Dementia Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5663</first_page>
						<last_page>5667</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2280</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/braun25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinzuomu</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suyuan</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dan</given_name>
<surname>Wells</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korin</given_name>
<surname>Richmond</surname>
</person_name>
					</contributors>
					<titles><title>Pairwise Evaluation of Accent Similarity in Speech Synthesis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2290</first_page>
						<last_page>2294</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2283</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhong25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Elvir</given_name>
<surname>Karimov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Varlamov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Danil</given_name>
<surname>Ivanov</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dmitrii</given_name>
<surname>Korzh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oleg</given_name>
<surname>Rogov</surname>
</person_name>
					</contributors>
					<titles><title>Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1513</first_page>
						<last_page>1517</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2290</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/karimov25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Razhan</given_name>
<surname>Hameed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sina</given_name>
<surname>Ahmadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hanah</given_name>
<surname>Hadi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rico</given_name>
<surname>Sennrich</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Speech Recognition for Low-Resourced Middle Eastern Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>733</first_page>
						<last_page>737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2296</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hameed25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Falih Gozi</given_name>
<surname>Febrinanto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kristen</given_name>
<surname>Moore</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandra</given_name>
<surname>Thapa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangang</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vidya</given_name>
<surname>Saikrishna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Feng</given_name>
<surname>Xia</surname>
</person_name>
					</contributors>
					<titles><title>Rehearsal with Auxiliary-Informed Sampling for Audio Deepfake Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5358</first_page>
						<last_page>5362</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2298</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/febrinanto25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Niyati</given_name>
<surname>Bafna</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Matthew</given_name>
<surname>Wiesner</surname>
</person_name>
					</contributors>
					<titles><title>LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1488</first_page>
						<last_page>1492</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2300</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bafna25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Laura</given_name>
<surname>Rachman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deniz</given_name>
<surname>Başkent</surname>
</person_name>
					</contributors>
					<titles><title>Characterization of voice cue sensitivity and vocal emotion recognition across the adult lifespan</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1298</first_page>
						<last_page>1302</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2303</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/rachman25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hardik</given_name>
<surname>Kothare</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Speech-Based Biomarkers Outperform the ALS Functional Rating Scale in Predicting Individual Disease Progression in ALS</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5313</first_page>
						<last_page>5317</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2307</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kothare25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Quentin</given_name>
<surname>Le Tellier</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Evrard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Rilliard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean-Sylvain</given_name>
<surname>Liénard</surname>
</person_name>
					</contributors>
					<titles><title>Robust Vocal Intensity Prediction: Overcoming Dataset Bias with Pretrained Deep Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1728</first_page>
						<last_page>1732</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2311</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/letellier25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Giulia</given_name>
<surname>Sanguedolce</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jón</given_name>
<surname>Guðnason</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dragos C.</given_name>
<surname>Gruia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emilie</given_name>
<surname>d'Olne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Fatemeh</given_name>
<surname>Geranmayeh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Patrick A.</given_name>
<surname>Naylor</surname>
</person_name>
					</contributors>
					<titles><title>Physiologically-Informed Feature Analysis of Acquired  Speech Disorders for Stroke Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>813</first_page>
						<last_page>817</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2313</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sanguedolce25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Imran E</given_name>
<surname>Kibria</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Donald S.</given_name>
<surname>Williamson</surname>
</person_name>
					</contributors>
					<titles><title>AttentiveMOS: A Lightweight Attention-Only Model forSpeech Quality Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2340</first_page>
						<last_page>2344</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2315</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kibria25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Natalia</given_name>
<surname>Tomashenko</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Vincent</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marc</given_name>
<surname>Tommasi</surname>
</person_name>
					</contributors>
					<titles><title>Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5128</first_page>
						<last_page>5132</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2317</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tomashenko25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yijing</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khalil</given_name>
<surname>Iskarous</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis</given_name>
<surname>Goldstein</surname>
</person_name>
					</contributors>
					<titles><title>Towards a dynamical model of transitions between fluent and stuttered speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>310</first_page>
						<last_page>314</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2320</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lu25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Md Asif</given_name>
<surname>Jalal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luca</given_name>
<surname>Remaggi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vasileios</given_name>
<surname>Moschopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thanasis</given_name>
<surname>Kotsiopoulos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vandana</given_name>
<surname>Rajan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Karthikeyan</given_name>
<surname>Saravanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasis</given_name>
<surname>Drosou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junho</given_name>
<surname>Heo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hyuk</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seokyeong</given_name>
<surname>Jeong</surname>
</person_name>
					</contributors>
					<titles><title>Robust Target Speaker Diarization and Separation via Augmented Speaker Embedding Sampling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1913</first_page>
						<last_page>1917</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2321</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jalal25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Tisdale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jackson</given_name>
<surname>Liscombe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Paulter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Neumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikram</given_name>
<surname>Ramanarayanan</surname>
</person_name>
					</contributors>
					<titles><title>Accessible Real-time Eye-gaze Tracking for Neurocognitive Health Assessment: A Multimodal Web-based Approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3060</first_page>
						<last_page>3064</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2322</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tisdale25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Orchid Chetia</given_name>
<surname>Phukan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>-</given_name>
<surname>Girish</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mohd Mujtaba</given_name>
<surname>Akhtar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shubham</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Swarup Ranjan</given_name>
<surname>Behera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vandana</given_name>
<surname>Rajan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Muskaan</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arun Balaji</given_name>
<surname>Buduru</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rajesh</given_name>
<surname>Sharma</surname>
</person_name>
					</contributors>
					<titles><title>Towards Machine Unlearning for Paralinguistic Speech Processing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4473</first_page>
						<last_page>4477</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2326</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/phukan25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yihan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ruibo</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Milis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junfeng</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heng</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>A Watermark for Auto-Regressive Speech Generation Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3474</first_page>
						<last_page>3478</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2328</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wu25k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gowtham</given_name>
<surname>Premananth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philip</given_name>
<surname>Resnik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sonia</given_name>
<surname>Bansal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deanna L.</given_name>
<surname>Kelly</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom Severity Estimation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3065</first_page>
						<last_page>3069</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2332</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/premananth25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hadrien</given_name>
<surname>Titeux</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quang Tuan Rémy</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andres</given_name>
<surname>Gil-Salcedo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anne-Catherine</given_name>
<surname>Bachoud-Levi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmanuel</given_name>
<surname>Dupoux</surname>
</person_name>
					</contributors>
					<titles><title>A simple method for predicting Clinical Scores in Huntington’s Disease by leveraging ASR's uncertainty on spontaneous speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1868</first_page>
						<last_page>1872</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2333</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/titeux25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haici</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gordon</given_name>
<surname>Wichern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Aihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiki</given_name>
<surname>Masuyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sameer</given_name>
<surname>Khurana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François G.</given_name>
<surname>Germain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Le Roux</surname>
</person_name>
					</contributors>
					<titles><title>Investigating continuous autoregressive generative speech enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2360</first_page>
						<last_page>2364</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2335</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinchuan</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hayato</given_name>
<surname>Futami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yosuke</given_name>
<surname>Kashiwagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emiru</given_name>
<surname>Tsunoo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Chain-of-Thought Training for Open E2E Spoken Dialogue Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4833</first_page>
						<last_page>4837</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2339</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/arora25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yu</given_name>
<surname>Pu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaoqian</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Guangyu</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zheng</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wei-Qiang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Empowering Large Language Models for End-to-End Speech Translation Leveraging Synthetic Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>26</first_page>
						<last_page>30</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2341</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gowtham</given_name>
<surname>Premananth</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vinith</given_name>
<surname>Kugathasan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Analyzing the Impact of Accent on English Speech: Acoustic and Articulatory Perspectives</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1493</first_page>
						<last_page>1497</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2342</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/premananth25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Karen</given_name>
<surname>Jones</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kevin</given_name>
<surname>Walker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christopher</given_name>
<surname>Caruso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elliot</given_name>
<surname>Singer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Trang</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Dunn</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stephanie</given_name>
<surname>Strassel</surname>
</person_name>
					</contributors>
					<titles><title>TELVID: A Multilingual Multi-modal Corpus for Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5738</first_page>
						<last_page>5742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2345</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jones25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zheng Xin</given_name>
<surname>Yong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vineel</given_name>
<surname>Pratap</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michael</given_name>
<surname>Auli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jean</given_name>
<surname>Maillard</surname>
</person_name>
					</contributors>
					<titles><title>Effects of Speaker Count, Duration, and Accent Diversity on Zero-Shot Accent Robustness in Low-Resource ASR</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1148</first_page>
						<last_page>1152</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2351</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yong25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Meenakshi</given_name>
<surname>Sirigiaju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiranjeevi</given_name>
<surname>Yarra</surname>
</person_name>
					</contributors>
					<titles><title>GoP2Vec: A few shot learning for pronunciation assessment with goodness of pronunciation (GoP) based representations from an i-vector framework and augmentation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5063</first_page>
						<last_page>5067</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2359</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sirigiaju25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abderrahim</given_name>
<surname>Fathan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jahangir</given_name>
<surname>Alam</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaolin</given_name>
<surname>Zhu</surname>
</person_name>
					</contributors>
					<titles><title>An Investigative Study on Recent Sharpness- and Flatness-Based Optimizers for Enhanced Self-Supervised Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1503</first_page>
						<last_page>1507</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2361</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fathan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zaid</given_name>
<surname>Sheikh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuichiro</given_name>
<surname>Shimizu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siddhant</given_name>
<surname>Arora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiatong</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Samuele</given_name>
<surname>Cornell</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xinjian</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Scalable Spontaneous Speech Dataset (SSSD): Crowdsourcing Data Collection to Promote Dialogue Research</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3963</first_page>
						<last_page>3967</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2362</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sheikh25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Atty</given_name>
<surname>Schouwenaars</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esther</given_name>
<surname>Ruigendijk</surname>
</person_name>
					</contributors>
					<titles><title>Processing of grammatical information in cochlear implant simulated speech by German adult listeners</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3788</first_page>
						<last_page>3792</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2369</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/schouwenaars25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Robin</given_name>
<surname>Netzorg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naomi</given_name>
<surname>Carvalho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrea</given_name>
<surname>Guzman</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lydia</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juliana</given_name>
<surname>Francis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Klo Vivienne</given_name>
<surname>Garoute</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Keith</given_name>
<surname>Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopala</given_name>
<surname>Anumanchipalli</surname>
</person_name>
					</contributors>
					<titles><title>On the Production and Perception of a Single Speaker's Gender</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>669</first_page>
						<last_page>673</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2372</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/netzorg25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dena</given_name>
<surname>Mujtaba</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nihar R.</given_name>
<surname>Mahapatra</surname>
</person_name>
					</contributors>
					<titles><title>Fine-Tuning ASR for Stuttered Speech: Personalized vs. Generalized Approaches</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3568</first_page>
						<last_page>3572</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2373</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mujtaba25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qingzheng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiancheng</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2088</first_page>
						<last_page>2092</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2377</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25z_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sri Harsha</given_name>
<surname>Dumpala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandramouli S.</given_name>
<surname>Sastry</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rudolf</given_name>
<surname>Uher</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sageev</given_name>
<surname>Oore</surname>
</person_name>
					</contributors>
					<titles><title>Test-Time Training for Speech-based Depression Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>479</first_page>
						<last_page>483</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2378</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dumpala25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Saba</given_name>
<surname>Tabatabaee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suzanne</given_name>
<surname>Boyce</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Liran</given_name>
<surname>Oren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Tiede</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Carol</given_name>
<surname>Espy-Wilson</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Acoustic-to-Articulatory Speech Inversion by Incorporating Nasality</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>325</first_page>
						<last_page>329</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2387</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tabatabaee25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiamin</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ju</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiteng</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tyler</given_name>
<surname>Vuong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaojiang</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhaojun</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peng</given_name>
<surname>Su</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prashant</given_name>
<surname>Rawat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sangeeta</given_name>
<surname>Srivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ming</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Florian</given_name>
<surname>Metze</surname>
</person_name>
					</contributors>
					<titles><title>Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3898</first_page>
						<last_page>3902</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2388</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xie25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Edem</given_name>
<surname>Ahadzi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vishwanath</given_name>
<surname>Pratap Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomi</given_name>
<surname>Kinnunen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ville</given_name>
<surname>Hautamaki</surname>
</person_name>
					</contributors>
					<titles><title>Continuous Learning for Children's ASR: Overcoming Catastrophic Forgetting with Elastic Weight Consolidation and Synaptic Intelligence</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2880</first_page>
						<last_page>2884</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2393</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ahadzi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yubin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yijing</given_name>
<surname>Lu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marcus</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tiantian</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Asterios</given_name>
<surname>Toutios</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haley</given_name>
<surname>Hsu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Louis</given_name>
<surname>Goldstein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name>
					</contributors>
					<titles><title>75-Speaker Annot-16: A benchmark dataset for speech articulatory rt-MRI annotation with articulator contours and phonetic alignment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2175</first_page>
						<last_page>2179</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2394</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shi25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alexandra</given_name>
<surname>Fort</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Francis</given_name>
<surname>Tyers</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Wav2Vec2-Bert for Computer-Assisted Pronunciation Training for isiZulu</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2160</first_page>
						<last_page>2164</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2400</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fort25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongchen</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yixin</given_name>
<surname>Gu</surname>
</person_name>
					</contributors>
					<titles><title>CrossPhon: An Auto Phone Mapping Tool to Streamline Cross-language Modeling for Phone Alignment of Low-resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>86</first_page>
						<last_page>90</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2401</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wu25l_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Amanda</given_name>
<surname>Eads</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heather</given_name>
<surname>Kabakoff</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nina</given_name>
<surname>Benway</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elaine</given_name>
<surname>Hitchcock</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Preston</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tara</given_name>
<surname>McAllister</surname>
</person_name>
					</contributors>
					<titles><title>PERCEPT-US: A Multimodal American English Child Speech Corpus Specialized for Articulatory Feedback</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2805</first_page>
						<last_page>2809</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2407</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/eads25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingran</given_name>
<surname>Xie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiang</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yue</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Xiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Generalization of Speech Large Language Models with Multi-Task Behavior Imitation and Speech-Text Interleaving</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2430</first_page>
						<last_page>2434</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2409</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xie25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiali</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hadi</given_name>
<surname>Amiri</surname>
</person_name>
					</contributors>
					<titles><title>Speech Unlearning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3209</first_page>
						<last_page>3213</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2412</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cheng25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aswin Shanmugam</given_name>
<surname>Subramanian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amit</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naoyuki</given_name>
<surname>Kanda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yifan</given_name>
<surname>Gong</surname>
</person_name>
					</contributors>
					<titles><title>Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5508</first_page>
						<last_page>5512</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2414</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/subramanian25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xuanru</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiachen</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cheol Jun</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tejas</given_name>
<surname>Prabhune</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuhe</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rodrigo</given_name>
<surname>Ortiz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoe</given_name>
<surname>Ezzes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jet</given_name>
<surname>Vonk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brittany</given_name>
<surname>Morin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rian</given_name>
<surname>Bogley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisa</given_name>
<surname>Wauters</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zachary</given_name>
<surname>Miller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Gorno-Tempini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopala</given_name>
<surname>Anumanchipalli</surname>
</person_name>
					</contributors>
					<titles><title>Towards Accurate Phonetic Error Detection Through Phoneme Similarity Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4738</first_page>
						<last_page>4742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2417</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhou25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hashim</given_name>
<surname>Ali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Surya</given_name>
<surname>Subramani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raksha</given_name>
<surname>Varahamurthy</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nithin</given_name>
<surname>Adupa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lekha</given_name>
<surname>Bollinani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hafiz</given_name>
<surname>Malik</surname>
</person_name>
					</contributors>
					<titles><title>Collecting, Curating, and Annotating Good Quality Speech deepfake dataset for Famous Figures: Process and Challenges</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3928</first_page>
						<last_page>3932</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2418</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ali25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Renzhe</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yanshen</given_name>
<surname>Tan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yiyi</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Quan</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ying</given_name>
<surname>Lin</surname>
</person_name>
					</contributors>
					<titles><title>Predicting Adolescent Suicidal Risk from Multi-task-based Speech: An Ensemble Learning Approach</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>409</first_page>
						<last_page>413</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2419</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25o_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shangkun</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jintao</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rong</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging LLM for Stuttering Speech: A Unified Architecture Bridging Recognition and Event Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1843</first_page>
						<last_page>1847</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2425</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/huang25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yao</given_name>
<surname>Xiao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heidi</given_name>
<surname>Christensen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Stefan</given_name>
<surname>Goetze</surname>
</person_name>
					</contributors>
					<titles><title>Alzheimer’s Dementia Detection Using Perplexity from Paired Large Language Models</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1423</first_page>
						<last_page>1427</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2428</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/xiao25e_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongchen</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yao</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zirong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yixin</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Disha Thotappala</given_name>
<surname>Jayaprakash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Li</given_name>
<surname>Sheng</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Automatic Speech Recognition Pipelines for Mandarin-English Bilingual Child Language Assessment in Telehealth</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3075</first_page>
						<last_page>3079</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2430</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wu25m_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nada</given_name>
<surname>Gohider</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Otman</given_name>
<surname>Basir</surname>
</person_name>
					</contributors>
					<titles><title>Towards Inclusive and Fair ASR: Insights from the SAPC Challenge for Optimizing Disordered Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3274</first_page>
						<last_page>3278</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2431</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gohider25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Abderrahim</given_name>
<surname>Fathan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jahangir</given_name>
<surname>Alam</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Labeling and Correction of Noisy Labels for Robust Self-Supervised Speaker Verification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4258</first_page>
						<last_page>4262</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2433</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fathan25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seonggyu</given_name>
<surname>Lee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sein</given_name>
<surname>Cheong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sangwook</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kihyuk</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jong Won</given_name>
<surname>Shin</surname>
</person_name>
					</contributors>
					<titles><title>Speech Enhancement based on cascaded two flows </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4863</first_page>
						<last_page>4867</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2436</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lee25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Xi</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mu</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szu-Jui</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>John H.L.</given_name>
<surname>Hansen</surname>
</person_name>
					</contributors>
					<titles><title>A Neural Codec Approach for Noise-Robust Bandwidth Expansion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4083</first_page>
						<last_page>4087</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2438</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/liu25p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Carey</given_name>
<surname>Smith</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hu</given_name>
<surname>Cheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pertti</given_name>
<surname>Palo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Aalto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Steven M.</given_name>
<surname>Lulich</surname>
</person_name>
					</contributors>
					<titles><title>Exploratory Analysis of Brainstem fMRI Data During Sustained Phonation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1063</first_page>
						<last_page>1067</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2444</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/smith25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chenxu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiachen</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuanru</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinming</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuhe</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zongli</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anaisha</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoe</given_name>
<surname>Ezzes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jet</given_name>
<surname>Vonk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brittany</given_name>
<surname>Morin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rian</given_name>
<surname>Bogley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisa</given_name>
<surname>Wauters</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zachary</given_name>
<surname>Miller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Gorno-Tempini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopala</given_name>
<surname>Anumanchipalli</surname>
</person_name>
					</contributors>
					<titles><title>Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency Transcription and Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2205</first_page>
						<last_page>2209</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2446</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/guo25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zijian</given_name>
<surname>Lin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yougen</given_name>
<surname>Yuan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuming</given_name>
<surname>Yan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinjiang</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pengfei</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qun</given_name>
<surname>Yu</surname>
</person_name>
					</contributors>
					<titles><title>Accelerating Autoregressive Speech Synthesis Inference With Speech Speculative Decoding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5533</first_page>
						<last_page>5537</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2447</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lin25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Gloria</given_name>
<surname>Araiza-Illan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Luke</given_name>
<surname>Meyer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bert</given_name>
<surname>Maat</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deniz</given_name>
<surname>Başkent</surname>
</person_name>
					</contributors>
					<titles><title>Robot-assisted Recognition of Vocal Emotions in Pseudospeech for Cochlear Implanted Adolescents</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>818</first_page>
						<last_page>822</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2448</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/araizaillan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qixi</given_name>
<surname>Zheng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yushen</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhikang</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyang</given_name>
<surname>Ma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaofei</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xie</given_name>
<surname>Chen</surname>
</person_name>
					</contributors>
					<titles><title>Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned Step Sampling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2445</first_page>
						<last_page>2449</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2449</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zheng25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenkai</given_name>
<surname>Hu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji</given_name>
<surname>Qi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhuojian</given_name>
<surname>Wei</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kexin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Viswadruth</given_name>
<surname>Akkaraju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Poeppel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dustin</given_name>
<surname>Freeman</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4313</first_page>
						<last_page>4317</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2451</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chang25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chris</given_name>
<surname>Zwilling</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heather</given_name>
<surname>Hodges</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lorraine</given_name>
<surname>Ramig</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adina</given_name>
<surname>Bradshaw</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Clarion</given_name>
<surname>Mendes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Heejin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandria</given_name>
<surname>Barkhimer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laura</given_name>
<surname>Mattie</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Meg</given_name>
<surname>Dickinson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shawnise</given_name>
<surname>Carter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marie Moore</given_name>
<surname>Channell</surname>
</person_name>
					</contributors>
					<titles><title>The Speech Accessibility Project: Best Practices for Collection and Curation of Disordered Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3938</first_page>
						<last_page>3942</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2454</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zwilling25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ahsan</given_name>
<surname>Cheema</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunil</given_name>
<surname>Puria</surname>
</person_name>
					</contributors>
					<titles><title>Using Neurogram Similarity Index Measure (NSIM) to Model Hearing Loss and Cochlear Neural Degeneration</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>823</first_page>
						<last_page>827</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2458</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cheema25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chloe D.</given_name>
<surname>Kwon</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-specific Patterns of Phonetic Covariation in Korean Word-medial Stops and the Role of Phonological and Morphological Contexts</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4773</first_page>
						<last_page>4777</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2465</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kwon25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kumud</given_name>
<surname>Tripathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chowdam Venkata</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pankaj</given_name>
<surname>Wasnik</surname>
</person_name>
					</contributors>
					<titles><title>Attention Is Not Always the Answer: Optimizing Voice Activity Detection with Simple Feature Fusion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>509</first_page>
						<last_page>513</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2466</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/tripathi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kohei</given_name>
<surname>Uehara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryoichi</given_name>
<surname>Takashima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuya</given_name>
<surname>Takiguchi</surname>
</person_name>
					</contributors>
					<titles><title>Zero-Shot Learning for Acoustic Event Classification Using an Attribute Vector and Conditional GAN</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2590</first_page>
						<last_page>2594</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2469</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/uehara25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vikram</given_name>
<surname>C M</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanjoy</given_name>
<surname>Pal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nidhi</given_name>
<surname>Mantri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopal Kumar</given_name>
<surname>Agrawal</surname>
</person_name>
					</contributors>
					<titles><title>Effect of Loudspeaker Emitted Speech on ASR performance</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3170</first_page>
						<last_page>3173</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2470</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/cm25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tahiya</given_name>
<surname>Chowdhury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Veronica</given_name>
<surname>Romero</surname>
</person_name>
					</contributors>
					<titles><title>Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>529</first_page>
						<last_page>533</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2472</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chowdhury25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Linya</given_name>
<surname>Fu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yu</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhijie</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zedong</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhong-Qiu</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Youfu</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>He</given_name>
<surname>Kong</surname>
</person_name>
					</contributors>
					<titles><title>AuralNet: Hierarchical Attention-based 3D Binaural Localization of Overlapping Speakers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>938</first_page>
						<last_page>942</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2478</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/fu25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shanhui</given_name>
<surname>Gan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zijian</given_name>
<surname>Liang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kai</given_name>
<surname>Niu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ping</given_name>
<surname>Zhang</surname>
</person_name>
					</contributors>
					<titles><title>Synonymity-Based Semantic Coding for Efficient Speech Compression</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>589</first_page>
						<last_page>593</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2483</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gan25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ju-Seok</given_name>
<surname>Seong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeong-Hwan</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ye-Rin</given_name>
<surname>Jeoung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ilseok</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Enhancing Target-speaker Automatic Speech Recognition Using Multiple Speaker Embedding Extractors with Virtual Speaker Embedding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4918</first_page>
						<last_page>4922</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2486</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/seong25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sujith</given_name>
<surname>Pulikodan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sahapthan</given_name>
<surname>K</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Visruth</given_name>
<surname>Sanka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nihar</given_name>
<surname>Desai</surname>
</person_name>
					</contributors>
					<titles><title>An approach to measuring the performance of Automatic Speech Recognition(ASR) models in the context of Large Language Model(LLM) powered applications</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5718</first_page>
						<last_page>5722</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2488</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pulikodan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Coralie</given_name>
<surname>Serrand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amira</given_name>
<surname>Morsli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gilles</given_name>
<surname>Boulianne</surname>
</person_name>
					</contributors>
					<titles><title>CommissionsQC: a Québec French Speech Corpus for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3918</first_page>
						<last_page>3922</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2490</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/serrand25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zongli</given_name>
<surname>Ye</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiachen</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuanru</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinming</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haodong</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuhe</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chenxu</given_name>
<surname>Guo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anaisha</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoe</given_name>
<surname>Ezzes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jet</given_name>
<surname>Vonk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brittany</given_name>
<surname>Morin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rian</given_name>
<surname>Bogley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisa</given_name>
<surname>Wauters</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zachary</given_name>
<surname>Miller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Gorno-Tempini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopala</given_name>
<surname>Anumanchipalli</surname>
</person_name>
					</contributors>
					<titles><title>Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1848</first_page>
						<last_page>1852</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2496</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ye25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chowdam Venkata Thirumala</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiranjeevi</given_name>
<surname>Yarra</surname>
</person_name>
					</contributors>
					<titles><title>SGED-Probe: Probing E2E ASR decoder and aligner for spoken grammar error detection under three speaking practice conditions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2400</first_page>
						<last_page>2404</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2500</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kumar25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nan</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yan</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qing</given_name>
<surname>Gu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haoyu</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lirong</given_name>
<surname>Dai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ian</given_name>
<surname>McLoughlin</surname>
</person_name>
					</contributors>
					<titles><title>Finetune Large Pre-Trained Model Based on Frequency-Wise Multi-Query Attention Pooling for Anomalous Sound Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3389</first_page>
						<last_page>3393</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2503</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jiang25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Berkin</given_name>
<surname>Durmus</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Blaise</given_name>
<surname>Munyampirwa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eduardo</given_name>
<surname>Pacheco</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atila</given_name>
<surname>Orhon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrey</given_name>
<surname>Leonov</surname>
</person_name>
					</contributors>
					<titles><title>SDBench: A Comprehensive Benchmark Suite for Speaker Diarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1598</first_page>
						<last_page>1602</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2505</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/durmus25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Maria</given_name>
<surname>Teleki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingfeng</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chengkai</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>James</given_name>
<surname>Caverlee</surname>
</person_name>
					</contributors>
					<titles><title>I want a horror – comedy – movie: Slips-of-the-Tongue Impact Conversational Recommender System Performance</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1778</first_page>
						<last_page>1782</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2509</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/teleki25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aditya</given_name>
<surname>Srinivas Menon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raj Prakash</given_name>
<surname>Gohil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kumud</given_name>
<surname>Tripathi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pankaj</given_name>
<surname>Wasnik</surname>
</person_name>
					</contributors>
					<titles><title>LASPA: Language Agnostic Speaker Disentanglement with Prefix-Tuned Cross-Attention</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3623</first_page>
						<last_page>3627</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2512</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/srinivasmenon25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhonghao</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuan</given_name>
<surname>Shi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anfeng</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tiantian</given_name>
<surname>Feng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harshvardhan</given_name>
<surname>Srivastava</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shrikanth</given_name>
<surname>Narayanan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maja</given_name>
<surname>Mataric</surname>
</person_name>
					</contributors>
					<titles><title>Examining Test-Time Adaptation for Personalized Child Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2820</first_page>
						<last_page>2824</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2513</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shi25h_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ezhini</given_name>
<surname>Rasendiran R</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chandresh Kumar</given_name>
<surname>Maurya</surname>
</person_name>
					</contributors>
					<titles><title>Improving Bird Classification with Primary Color Additives</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1703</first_page>
						<last_page>1707</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2516</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/rasendiranr25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jhansi</given_name>
<surname>Mallela</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Upendra Vishwanath</given_name>
<surname>Y. S.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sankara Bharadwaj</given_name>
<surname>Rangavajjala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhaskar</given_name>
<surname>Bhatt</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiranjeevi</given_name>
<surname>Yarra</surname>
</person_name>
					</contributors>
					<titles><title>SupraDoRAL: Automatic Word Prominence Detection Using Suprasegmental Dependencies of Representations with Acoustic and Linguistic Context</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5773</first_page>
						<last_page>5777</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2519</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mallela25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Qibing</given_name>
<surname>Bai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sho</given_name>
<surname>Inoue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhongjie</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yannan</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Accent Normalization Using Self-Supervised Discrete Tokens with Non-Parallel Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1618</first_page>
						<last_page>1622</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2520</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/bai25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aishwarya</given_name>
<surname>Pothula</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bhavana</given_name>
<surname>Akkiraju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srihari</given_name>
<surname>Bandarupalli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Charan</given_name>
<surname>D</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Santosh</given_name>
<surname>Kesiraju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anil Kumar</given_name>
<surname>Vuppala</surname>
</person_name>
					</contributors>
					<titles><title>End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>41</first_page>
						<last_page>45</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2525</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/pothula25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oguzhan</given_name>
<surname>Baser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmet Ege</given_name>
<surname>Tanriverdi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaan</given_name>
<surname>Kale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandeep</given_name>
<surname>Chinchali</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Vishwanath</surname>
</person_name>
					</contributors>
					<titles><title>WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4873</first_page>
						<last_page>4877</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2528</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/baser25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Enjamamul</given_name>
<surname>Hoq</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikhil</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Danielle</given_name>
<surname>Omondi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ifeoma</given_name>
<surname>Nwogu</surname>
</person_name>
					</contributors>
					<titles><title>FUSE-MOS: Fusion of Speech Embeddings for MOS Prediction with Uncertainty Quantification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2350</first_page>
						<last_page>2354</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2532</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hoq25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rishabh</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>MLNS</given_name>
<surname>Karthik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chelamkuri</given_name>
<surname>Omsrinath</surname>
</person_name>
					</contributors>
					<titles><title>Sub-band based Adaptive IIR Algorithm with Biquad Filter Stability Constraints for Feedforward Hear-Through Equalization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3120</first_page>
						<last_page>3124</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2533</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gupta25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yeseul</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bowon</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Fine-tuning Strategies for Automatic Speech Recognition of Low-Resource Speech with Autism Spectrum Disorder</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1858</first_page>
						<last_page>1862</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2535</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/park25g_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jee-weon</given_name>
<surname>Jung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wangyou</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Soumi</given_name>
<surname>Maiti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yihan</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xin</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ji-Hoon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuta</given_name>
<surname>Matsunaga</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seyun</given_name>
<surname>Um</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinchuan</given_name>
<surname>Tian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hye-jin</given_name>
<surname>Shim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Evans</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon Son</given_name>
<surname>Chung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinnosuke</given_name>
<surname>Takamichi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinji</given_name>
<surname>Watanabe</surname>
</person_name>
					</contributors>
					<titles><title>The Text-to-speech in the Wild (TITW) Database</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4798</first_page>
						<last_page>4802</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2536</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jung25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wenjie</given_name>
<surname>Zhong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jason</given_name>
<surname>Naradowsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Miyao</surname>
</person_name>
					</contributors>
					<titles><title>A Simple-Yet-Effective Data Augmentation Method for Speaker Identification in Novels</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5753</first_page>
						<last_page>5757</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2545</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhong25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hongli</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yizhou</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hao</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sheng</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5203</first_page>
						<last_page>5207</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2549</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jingya</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aashish N.</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sowmya Manojna</given_name>
<surname>Narasimha</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gal</given_name>
<surname>Mishne</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vikash</given_name>
<surname>Gilja</surname>
</person_name>
					</contributors>
					<titles><title>Word-Level Error Analysis in Decoding Systems: From Speech Recognition to Brain-Computer Interfaces</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5563</first_page>
						<last_page>5567</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2550</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/huang25j_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Robert</given_name>
<surname>Lewis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Szymon</given_name>
<surname>Fedor</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nelson</given_name>
<surname>Hidalgo Julia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joshua</given_name>
<surname>Curtiss</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiyeon</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Noah</given_name>
<surname>Jones</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>David</given_name>
<surname>Mischoulon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas F</given_name>
<surname>Quatieri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Paola</given_name>
<surname>Pedrelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rosalind</given_name>
<surname>Picard</surname>
</person_name>
					</contributors>
					<titles><title>Towards the Objective Characterisation of Major Depressive Disorder Using Speech Data from a 12-week Observational Study with Daily Measurements</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>494</first_page>
						<last_page>498</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2556</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lewis25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nelson</given_name>
<surname>Hidalgo Julia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert</given_name>
<surname>Lewis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Craig</given_name>
<surname>Ferguson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Goldberg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wendy</given_name>
<surname>Lau</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Caroline</given_name>
<surname>Swords</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gabriela</given_name>
<surname>Valdivia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Christine</given_name>
<surname>Wilson-Mendenhall</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Raquel</given_name>
<surname>Tartar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rosalind</given_name>
<surname>Picard</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richard</given_name>
<surname>Davidson</surname>
</person_name>
					</contributors>
					<titles><title>Identifying Vocal and Facial Biomarkers of Depression in Large-Scale Remote Recordings: A Multimodal Study Using Mixed-Effects Modeling</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5263</first_page>
						<last_page>5267</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2560</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hidalgojulia25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Atsumoto</given_name>
<surname>Ohashi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shinya</given_name>
<surname>Iizuka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingjing</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuichiro</given_name>
<surname>Higashinaka</surname>
</person_name>
					</contributors>
					<titles><title>Towards a Japanese Full-duplex Spoken Dialogue System</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1783</first_page>
						<last_page>1787</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2564</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ohashi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>ZhaoHui</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hui</given_name>
<surname>Luo</surname>
</person_name>
					</contributors>
					<titles><title>Cross-corpus open-set Speech Emotion Recognition Method Based on Spatiotemporal Features with Inverse-Entropy Regularization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4493</first_page>
						<last_page>4497</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2571</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhou25i_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Asahi</given_name>
<surname>Sakuma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hiroaki</given_name>
<surname>Sato</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryuga</given_name>
<surname>Sugano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tadashi</given_name>
<surname>Kumano</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshihiko</given_name>
<surname>Kawai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tetsuji</given_name>
<surname>Ogawa</surname>
</person_name>
					</contributors>
					<titles><title>Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for Multi-Talker Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5503</first_page>
						<last_page>5507</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2572</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sakuma25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vasista Sai</given_name>
<surname>Lodagala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lamya</given_name>
<surname>Alkanhal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Daniel</given_name>
<surname>Izham</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shivam</given_name>
<surname>Mehta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shammur Absar</given_name>
<surname>Chowdhury</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aqeelah</given_name>
<surname>Makki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hamdy S.</given_name>
<surname>Hussein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gustav Eje</given_name>
<surname>Henter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmed</given_name>
<surname>Ali</surname>
</person_name>
					</contributors>
					<titles><title>SawtArabi: A Benchmark Corpus for Arabic TTS.  Standard, Dialectal and Code-Switching</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4793</first_page>
						<last_page>4797</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2573</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/lodagala25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Patakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vassilis</given_name>
<surname>Lyberatos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Spyridon</given_name>
<surname>Kantarelis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Edmund</given_name>
<surname>Dervakos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Giorgos</given_name>
<surname>Stamou</surname>
</person_name>
					</contributors>
					<titles><title>Semantic-Aware Interpretable Multimodal Music Auto-Tagging</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>236</first_page>
						<last_page>240</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2574</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/patakis25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Pradeep</given_name>
<surname>Rangappa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrés</given_name>
<surname>Carofilis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeena</given_name>
<surname>Prakash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shashi</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sergio</given_name>
<surname>Burdisso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Madikeri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esaú</given_name>
<surname>Villatoro-Tello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bidisha</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kadri</given_name>
<surname>Hacioglu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shankar</given_name>
<surname>Venkatesan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Vyas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name>
					</contributors>
					<titles><title>Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4928</first_page>
						<last_page>4932</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2580</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/rangappa25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yunsik</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoonyoung</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Modality-Specific Speech Enhancement and Noise-Adaptive Fusion for Acoustic and Body-Conduction Microphone Framework</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3833</first_page>
						<last_page>3837</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2581</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25s_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Changhan</given_name>
<surname>Oh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kiyoung</given_name>
<surname>Park</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeomja</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Woo Yong</given_name>
<surname>Choi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hwa Jeon</given_name>
<surname>Song</surname>
</person_name>
					</contributors>
					<titles><title>Improving Cross-Attention based on Positional Alignment during Inference for Robust Long-form Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3329</first_page>
						<last_page>3333</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2582</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/oh25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Oguzhan</given_name>
<surname>Baser</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmet Ege</given_name>
<surname>Tanriverdi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sriram</given_name>
<surname>Vishwanath</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandeep</given_name>
<surname>Chinchali</surname>
</person_name>
					</contributors>
					<titles><title>PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5333</first_page>
						<last_page>5337</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2583</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/baser25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nam-Gyu</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deok-Hyeon</given_name>
<surname>Cho</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seung-Bin</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seong-Whan</given_name>
<surname>Lee</surname>
</person_name>
					</contributors>
					<titles><title>Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4378</first_page>
						<last_page>4382</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2586</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25t_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jongsuk</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jaemyung</given_name>
<surname>Yu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minchan</given_name>
<surname>Kwon</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junmo</given_name>
<surname>Kim</surname>
</person_name>
					</contributors>
					<titles><title>FairASR: Fair Audio Contrastive Learning for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3878</first_page>
						<last_page>3882</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2590</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lidea</given_name>
<surname>Shahidi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erdem Baha</given_name>
<surname>Topbas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thu Ngan</given_name>
<surname>Dang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Goehring</surname>
</person_name>
					</contributors>
					<titles><title>Harnessing Text-to-Speech Voice Cloning Models for Improved Audiological Speech Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2170</first_page>
						<last_page>2174</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2595</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/shahidi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ning</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bingyang</given_name>
<surname>Wen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minghui</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yang</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zongru</given_name>
<surname>Shao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haojie</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K.P.</given_name>
<surname>Subbalakshmi</surname>
</person_name>
					</contributors>
					<titles><title>Decoding Alzheimer’s: Interpretable Visual and Logical Attention in Picture Description Tasks</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2043</first_page>
						<last_page>2047</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2596</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25aa_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Chetan</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vaishnavi</given_name>
<surname>Chandwanshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shreya Shrikant</given_name>
<surname>Karkun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Aditya Anand</given_name>
<surname>Gupta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>A real-time MRI study on asymmetry in velum dynamics during VCV production with nasal sounds</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1058</first_page>
						<last_page>1062</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2597</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sharma25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Andrés</given_name>
<surname>Carofilis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pradeep</given_name>
<surname>Rangappa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Madikeri</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shashi</given_name>
<surname>Kumar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sergio</given_name>
<surname>Burdisso</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jeena</given_name>
<surname>Prakash</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Esaú</given_name>
<surname>Villatoro-Tello</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Motlicek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bidisha</given_name>
<surname>Sharma</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kadri</given_name>
<surname>Hacioglu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shankar</given_name>
<surname>Venkatesan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Saurabh</given_name>
<surname>Vyas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andreas</given_name>
<surname>Stolcke</surname>
</person_name>
					</contributors>
					<titles><title>Better Semi-supervised Learning for Multi-domain ASR Through Incremental Retraining and Data Filtering</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3618</first_page>
						<last_page>3622</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2601</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/carofilis25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Olli</given_name>
<surname>Kuparinen</surname>
</person_name>
					</contributors>
					<titles><title>Automatic Dialectal Transcription: An Evaluation on Finnish and Norwegian</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2390</first_page>
						<last_page>2394</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2602</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kuparinen25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alkis</given_name>
<surname>Koudounas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Claudio</given_name>
<surname>Savelli</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Flavio</given_name>
<surname>Giobergia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elena</given_name>
<surname>Baralis</surname>
</person_name>
					</contributors>
					<titles><title>``Alexa, can you forget me?'' Machine Unlearning Benchmark in Spoken Language Understanding</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1768</first_page>
						<last_page>1772</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2607</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/koudounas25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sameer</given_name>
<surname>Khurana</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Klement</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antoine</given_name>
<surname>Laurent</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Boboš</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Juraj</given_name>
<surname>Novosad</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Gazdik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ellen</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zili</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Amir</given_name>
<surname>Hussein</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ricard</given_name>
<surname>Marxer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoshiki</given_name>
<surname>Masuyama</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Aihara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chiori</given_name>
<surname>Hori</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>François G.</given_name>
<surname>Germain</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gordon</given_name>
<surname>Wichern</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonathan</given_name>
<surname>Le Roux</surname>
</person_name>
					</contributors>
					<titles><title>Factorized RVQ-GAN For Disentangled Speech Tokenization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3514</first_page>
						<last_page>3518</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2612</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/khurana25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Salvatore</given_name>
<surname>Carta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alessandro</given_name>
<surname>Giuliani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marco Manolo</given_name>
<surname>Manca</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mirko</given_name>
<surname>Marras</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Leonardo</given_name>
<surname>Piano</surname>
</person_name>
					</contributors>
					<titles><title>SardinianVoxes: A Speech Recognition Dataset for the Sardinian Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1938</first_page>
						<last_page>1942</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2615</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/carta25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shoutrik</given_name>
<surname>Das</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nishant</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Arjun</given_name>
<surname>Gangwar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>S</given_name>
<surname>Umesh</surname>
</person_name>
					</contributors>
					<titles><title>Improved Intelligibility of Dysarthric Speech using Conditional Flow Matching</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2118</first_page>
						<last_page>2122</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2617</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/das25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Aykut</given_name>
<surname>Büker</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oğuzhan</given_name>
<surname>Kurnaz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Şule</given_name>
<surname>Bekiryazıcı</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Selim Can</given_name>
<surname>Demirtaş</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Cemal</given_name>
<surname>Hanilçi</surname>
</person_name>
					</contributors>
					<titles><title>Evaluating Parameter Sharing for Spoofing-Aware Speaker Verification: A Case Study on the ASVspoof 5 Dataset</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4573</first_page>
						<last_page>4577</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2618</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/buker25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yui</given_name>
<surname>Sudo</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Kojima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoya</given_name>
<surname>Mizumoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianbo</given_name>
<surname>Liu</surname>
</person_name>
					</contributors>
					<titles><title>OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5188</first_page>
						<last_page>5192</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2621</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sudo25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhaolin</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Niehues</surname>
</person_name>
					</contributors>
					<titles><title>In-context Language Learning for Endangered Languages in Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>738</first_page>
						<last_page>742</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2626</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25ca_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Alicja</given_name>
<surname>Martinek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joanna</given_name>
<surname>Gajewska</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ewelina</given_name>
<surname>Bartuzi-Trokielewicz</surname>
</person_name>
					</contributors>
					<titles><title>Do you read me? - flow of speech effect on speaker recognition systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3643</first_page>
						<last_page>3647</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2629</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/martinek25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jaume</given_name>
<surname>Santamaría-Jordà</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pablo</given_name>
<surname>Segovia-Martínez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gonçal V.</given_name>
<surname>Garcés Díaz-Munío</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joan Albert</given_name>
<surname>Silvestre-Cerdà</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Adrià</given_name>
<surname>Giménez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rubén</given_name>
<surname>Gaspar Aparicio</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>René</given_name>
<surname>Fernández Sánchez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jorge</given_name>
<surname>Civera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Albert</given_name>
<surname>Sanchis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alfons</given_name>
<surname>Juan</surname>
</person_name>
					</contributors>
					<titles><title>LHCP-ASR: An English Speech Corpus of High-Energy Particle Physics Talks for Narrow-Domain ASR Benchmarking</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4033</first_page>
						<last_page>4037</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2630</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/santamariajorda25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Akanksha</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yi-Ping Phoebe</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vipul</given_name>
<surname>Arora</surname>
</person_name>
					</contributors>
					<titles><title>H-QuEST: Accelerating Query-by-Example Spoken Term Detection with Hierarchical Indexing</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2635</first_page>
						<last_page>2639</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2631</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/singh25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Georgios</given_name>
<surname>Chatzichristodoulou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Despoina</given_name>
<surname>Kosmopoulou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Antonios</given_name>
<surname>Kritikos</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anastasia</given_name>
<surname>Poulopoulou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Efthymios</given_name>
<surname>Georgiou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Athanasios</given_name>
<surname>Katsamanis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vassilis</given_name>
<surname>Katsouros</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexandros</given_name>
<surname>Potamianos</surname>
</person_name>
					</contributors>
					<titles><title>Medusa: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4683</first_page>
						<last_page>4687</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2636</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chatzichristodoulou25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yonghun</given_name>
<surname>Song</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yeeun</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoonyoung</given_name>
<surname>Chung</surname>
</person_name>
					</contributors>
					<titles><title>Lightweight Speech Enhancement Model Based on Harmonic Attention and Phase Estimation with Skin-Attachable Accelerometer</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>66</first_page>
						<last_page>70</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2642</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/song25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Koharu</given_name>
<surname>Horii</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Naohiro</given_name>
<surname>Tawara</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsunori</given_name>
<surname>Ogawa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shoko</given_name>
<surname>Araki</surname>
</person_name>
					</contributors>
					<titles><title>Why is children's ASR so difficult? Analyzing children's phonological error patterns using SSL-based phoneme recognizers</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2870</first_page>
						<last_page>2874</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2645</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/horii25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Leonora</given_name>
<surname>Vesterbacka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Faton</given_name>
<surname>Rekathati</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robin</given_name>
<surname>Kurtz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Justyna</given_name>
<surname>Sikora</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Agnes</given_name>
<surname>Toftgård</surname>
</person_name>
					</contributors>
					<titles><title>Swedish Whispers; Leveraging a Massive Speech Corpus for Swedish Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>758</first_page>
						<last_page>762</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2646</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/vesterbacka25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shangkun</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yuxuan</given_name>
<surname>Du</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingwen</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dejun</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xupeng</given_name>
<surname>Jia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jing</given_name>
<surname>Deng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jintao</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rong</given_name>
<surname>Zheng</surname>
</person_name>
					</contributors>
					<titles><title>Overlap-Adaptive Hybrid Speaker Diarization and ASR-Aware Observation Addition for MISP 2025 Challenge</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1908</first_page>
						<last_page>1912</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2648</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/huang25k_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jinming</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xuanru</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiachen</given_name>
<surname>Lian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuhe</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>William</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zoe</given_name>
<surname>Ezzes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rian</given_name>
<surname>Bogley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lisa</given_name>
<surname>Wauters</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zachary</given_name>
<surname>Miller</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jet</given_name>
<surname>Vonk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Brittany</given_name>
<surname>Morin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Maria</given_name>
<surname>Gorno-Tempini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Gopala</given_name>
<surname>Anumanchipalli</surname>
</person_name>
					</contributors>
					<titles><title>Analysis and Evaluation of Synthetic Data Generation in Speech Dysfluency Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1853</first_page>
						<last_page>1857</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2658</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/zhang25u_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rishabh</given_name>
<surname>Ranjan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kishan</given_name>
<surname>Pipariya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mayank</given_name>
<surname>Vatsa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richa</given_name>
<surname>Singh</surname>
</person_name>
					</contributors>
					<titles><title>SynHate: Detecting Hate Speech in Synthetic Deepfake Audio</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5623</first_page>
						<last_page>5627</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2659</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ranjan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Mikey</given_name>
<surname>Elmers</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koji</given_name>
<surname>Inoue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Divesh</given_name>
<surname>Lala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tatsuya</given_name>
<surname>Kawahara</surname>
</person_name>
					</contributors>
					<titles><title>Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3015</first_page>
						<last_page>3019</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2660</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/elmers25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Shaole</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shuai</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyu</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ke</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wupeng</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Haizhou</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>REAL-T: Real Conversational Mixtures for Target Speaker Extraction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1923</first_page>
						<last_page>1927</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2662</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/li25da_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Hyun-Soo</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Da-Hee</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joon-Hyuk</given_name>
<surname>Chang</surname>
</person_name>
					</contributors>
					<titles><title>Spatially Weighted Contrastive Learning for Robust Sound Source Localization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2490</first_page>
						<last_page>2494</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2666</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25v_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Rishabh</given_name>
<surname>Ranjan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Likhith</given_name>
<surname>Ayinala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mayank</given_name>
<surname>Vatsa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richa</given_name>
<surname>Singh</surname>
</person_name>
					</contributors>
					<titles><title>Multimodal Zero-Shot Framework for Deepfake Hate Speech Detection in Low-Resource Languages</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1678</first_page>
						<last_page>1682</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2668</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ranjan25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashish</given_name>
<surname>Mittal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Darshan</given_name>
<surname>Prabhu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sunita</given_name>
<surname>Sarawagi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Preethi</given_name>
<surname>Jyothi</surname>
</person_name>
					</contributors>
					<titles><title>Skip-Salsa: Skip Synchronous Fusion of ASR LLM Decoders</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>654</first_page>
						<last_page>658</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2669</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mittal25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Seongkyu</given_name>
<surname>Mun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jubum</given_name>
<surname>Han</surname>
</person_name>
					</contributors>
					<titles><title>Boundary-Conscious Pruning: Hard Set-Aware Model Compression for Efficient Speaker Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3683</first_page>
						<last_page>3687</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2675</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mun25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Nidheesh</given_name>
<surname>Gorthi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kartik</given_name>
<surname>Thakral</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rishabh</given_name>
<surname>Ranjan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richa</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mayank</given_name>
<surname>Vatsa</surname>
</person_name>
					</contributors>
					<titles><title>LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing Framework for Biometric Security</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5658</first_page>
						<last_page>5662</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2677</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gorthi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ariadna</given_name>
<surname>Sanchez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>King</surname>
</person_name>
					</contributors>
					<titles><title>Can We Reconstruct a Dysarthric Voice with the Large Speech Model Parler TTS?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4138</first_page>
						<last_page>4142</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2679</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sanchez25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Haris</given_name>
<surname>Gulzar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Monikka Roslianna</given_name>
<surname>Busto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Akiko</given_name>
<surname>Masaki</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Takeharu</given_name>
<surname>Eda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ryo</given_name>
<surname>Masumura</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging LLMs for Written to Spoken Style Data Transformation to Enhance Spoken Dialog State Tracking</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1743</first_page>
						<last_page>1747</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2681</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gulzar25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Wafaa</given_name>
<surname>Wardah</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Robert P.</given_name>
<surname>Spang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vincent</given_name>
<surname>Barriac</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Reimes</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Llagostera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jens</given_name>
<surname>Berger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sebastian</given_name>
<surname>Möller</surname>
</person_name>
					</contributors>
					<titles><title>SQ-AST: A Transformer-Based Model for Speech Quality Prediction</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2335</first_page>
						<last_page>2339</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2683</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wardah25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Kaidi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenhao</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ziyue</given_name>
<surname>Jiang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hukai</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peijie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weijie</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1383</first_page>
						<last_page>1387</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2684</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/wang25ba_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Vi Jun Sean</given_name>
<surname>Yong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Serkan</given_name>
<surname>Kumyol</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pau Le Lisa</given_name>
<surname>Low</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suk Wai Winnie</given_name>
<surname>Leung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tristan</given_name>
<surname>Braud</surname>
</person_name>
					</contributors>
					<titles><title>HK-GenSpeech: A Generative AI Scene Creation Framework for Speech Based Cognitive Assessment</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>534</first_page>
						<last_page>538</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2685</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yong25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jatin</given_name>
<surname>Agrawal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bramhendra</given_name>
<surname>Koilakuntla</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Srikanth</given_name>
<surname>Konjeti</surname>
</person_name>
					</contributors>
					<titles><title>Spot and Merge: A Hybrid Context Biasing Approach for Rare Word and Out of Vocabulary Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3319</first_page>
						<last_page>3323</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2692</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/agrawal25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tomoya</given_name>
<surname>Mizumoto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atsushi</given_name>
<surname>Kojima</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yusuke</given_name>
<surname>Fujita</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lianbo</given_name>
<surname>Liu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yui</given_name>
<surname>Sudo</surname>
</person_name>
					</contributors>
					<titles><title>Is Synthetic Data Truly Effective for Training Speech Language Models?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1808</first_page>
						<last_page>1812</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2693</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/mizumoto25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiajun</given_name>
<surname>He</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinyi</given_name>
<surname>Mi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomoki</given_name>
<surname>Toda</surname>
</person_name>
					</contributors>
					<titles><title>GIA-MIC: Multimodal Emotion Recognition with Gated Interactive Attention and Modality-Invariant Learning Constraints</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2695</first_page>
						<last_page>2699</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2696</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/he25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiawei</given_name>
<surname>Jin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhihan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yixuan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhiyong</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>In This Environment, As That Speaker: A Text-Driven Framework for Multi-Attribute Speech Conversion</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1393</first_page>
						<last_page>1397</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2697</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/jin25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Charan</given_name>
<surname>Sridhar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shaomei</given_name>
<surname>Wu</surname>
</person_name>
					</contributors>
					<titles><title>J-j-j-just Stutter: Benchmarking Whisper's Performance Disparities on Different Stuttering Patterns</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3753</first_page>
						<last_page>3757</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2700</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sridhar25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Bikash</given_name>
<surname>Dutta</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Rishabh</given_name>
<surname>Ranjan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shyam</given_name>
<surname>Sathvik</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mayank</given_name>
<surname>Vatsa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Richa</given_name>
<surname>Singh</surname>
</person_name>
					</contributors>
					<titles><title>Can Quantized Audio Language Models Perform Zero-Shot Spoofing Detection?</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4578</first_page>
						<last_page>4582</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2701</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/dutta25b_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Minseop</given_name>
<surname>Kim</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Minsu</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seokyoung</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Myoung-wan</given_name>
<surname>Koo</surname>
</person_name>
					</contributors>
					<titles><title>Data Augmentation using Speech Synthesis for Speaker-Independent Dysarthria Severity Classification</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2745</first_page>
						<last_page>2749</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2711</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kim25w_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Tahir</given_name>
<surname>Javed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaushal</given_name>
<surname>Bhogale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitesh M.</given_name>
<surname>Khapra</surname>
</person_name>
					</contributors>
					<titles><title>NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>918</first_page>
						<last_page>922</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2714</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/javed25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Anup</given_name>
<surname>Singh</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kris</given_name>
<surname>Demuynck</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Vipul</given_name>
<surname>Arora</surname>
</person_name>
					</contributors>
					<titles><title>Language-Agnostic Speech Tokenizer for Spoken Term Detection with Efficient Retrieval</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2630</first_page>
						<last_page>2634</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2722</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/singh25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ilja</given_name>
<surname>Baumann</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Wagner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name>
					</contributors>
					<titles><title>Pathology-Aware Speech Encoding and Data Augmentation for Dysarthric Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3289</first_page>
						<last_page>3293</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2724</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/baumann25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Avishkar</given_name>
<surname>Behera</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Riya Ann</given_name>
<surname>Easow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Venkatesh</given_name>
<surname>Parvathala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>K. Sri Rama</given_name>
<surname>Murty</surname>
</person_name>
					</contributors>
					<titles><title>Test-Time Training for Speech Enhancement</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2375</first_page>
						<last_page>2379</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2725</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/behera25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Peijie</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Wenhao</given_name>
<surname>Guan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaidi</given_name>
<surname>Wang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Weijie</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hukai</given_name>
<surname>Huang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Qingyang</given_name>
<surname>Hong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Li</surname>
</person_name>
					</contributors>
					<titles><title>DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture Switching for Speech Codec</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4908</first_page>
						<last_page>4912</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2726</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25p_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Rolland</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alberto</given_name>
<surname>Abad</surname>
</person_name>
					</contributors>
					<titles><title>Exploring Shared-Weight Mechanisms in Transformer and Conformer Architectures for Automatic Speech Recognition</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2885</first_page>
						<last_page>2889</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2733</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/rolland25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sakshi</given_name>
<surname>Joshi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Eldho</given_name>
<surname>Ittan George</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tahir</given_name>
<surname>Javed</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kaushal</given_name>
<surname>Bhogale</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nikhil</given_name>
<surname>Narasimhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitesh M.</given_name>
<surname>Khapra</surname>
</person_name>
					</contributors>
					<titles><title>Recognizing Every Voice: Towards Inclusive ASR for Rural Bhojpuri Women</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4243</first_page>
						<last_page>4247</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2734</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/joshi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sara</given_name>
<surname>Barahona</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Anna</given_name>
<surname>Silnova</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ladislav</given_name>
<surname>Mošner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Junyi</given_name>
<surname>Peng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Johan</given_name>
<surname>Rohdin</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jiangyu</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Petr</given_name>
<surname>Palka</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Federico</given_name>
<surname>Landini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lukáš</given_name>
<surname>Burget</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Themos</given_name>
<surname>Stafylakis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sandro</given_name>
<surname>Cumani</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dominik</given_name>
<surname>Boboš</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Miroslav</given_name>
<surname>Hlavaček</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Martin</given_name>
<surname>Kodovsky</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tomaš</given_name>
<surname>Pavliček</surname>
</person_name>
					</contributors>
					<titles><title>Analysis of ABC Frontend Audio Systems for the NIST-SRE24</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5763</first_page>
						<last_page>5767</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2737</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/barahona25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhuangqi</given_name>
<surname>Chen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xianjun</given_name>
<surname>Xia</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xiaohuai</given_name>
<surname>Le</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Siyu</given_name>
<surname>Sun</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chuanzeng</given_name>
<surname>Huang</surname>
</person_name>
					</contributors>
					<titles><title>AF-Vocoder: Artifact-Free Neural Vocoder with Global Artifact Filter</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4903</first_page>
						<last_page>4907</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2739</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chen25q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dirk</given_name>
<surname>Hoffner</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Simon</given_name>
<surname>Weihe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Brand</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bernd T.</given_name>
<surname>Meyer</surname>
</person_name>
					</contributors>
					<titles><title>Hearing deficits of transformer-based ASR for anechoic and spatial signals</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5733</first_page>
						<last_page>5737</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2741</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hoffner25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>SooHwan</given_name>
<surname>Eom</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mark</given_name>
<surname>Hasegawa-Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Chang D.</given_name>
<surname>Yoo</surname>
</person_name>
					</contributors>
					<titles><title>SiamCTC:  Learning Speech Representations through Monotonic Temporal Alignment </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3573</first_page>
						<last_page>3577</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2746</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/eom25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Christoph</given_name>
<surname>Minixhofer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ondřej</given_name>
<surname>Klejch</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Peter</given_name>
<surname>Bell</surname>
</person_name>
					</contributors>
					<titles><title>Scaling Laws for Synthetic Speech for Model Training</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3189</first_page>
						<last_page>3193</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2750</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/minixhofer25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Jiawen</given_name>
<surname>Kang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dongrui</given_name>
<surname>Han</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lingwei</given_name>
<surname>Meng</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jingyan</given_name>
<surname>Zhou</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jinchao</given_name>
<surname>Li</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Xixin</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Helen</given_name>
<surname>Meng</surname>
</person_name>
					</contributors>
					<titles><title>On the Within-class Variation Issue in Alzheimer's Disease Detection</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5668</first_page>
						<last_page>5672</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2751</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/kang25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Sparsh</given_name>
<surname>Rastogi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harsh</given_name>
<surname>Dadwal</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Khushboo</given_name>
<surname>Modi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jatin</given_name>
<surname>Bedi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jasmeet</given_name>
<surname>Singh</surname>
</person_name>
					</contributors>
					<titles><title>Towards Sentence Level Imagined Speech Generation from EEG signals</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>5558</first_page>
						<last_page>5562</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2752</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/rastogi25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Justyna</given_name>
<surname>Krzywdziak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bartłomiej</given_name>
<surname>Eljasiak</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joanna</given_name>
<surname>Stępień</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Michał</given_name>
<surname>Świątek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Agnieszka</given_name>
<surname>Pruszek</surname>
</person_name>
					</contributors>
					<titles><title>Leveraging Text and Speech Processing for Suicide Risk Classification in Chinese Adolescents</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>394</first_page>
						<last_page>398</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2755</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/krzywdziak25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ashwin</given_name>
<surname>Sankar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yoach</given_name>
<surname>Lacombe</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sherry</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Praveen</given_name>
<surname>Srinivasa Varadhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sanchit</given_name>
<surname>Gandhi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitesh M.</given_name>
<surname>Khapra</surname>
</person_name>
					</contributors>
					<titles><title>Rasmalai : Resources for Adaptive Speech Modeling in IndiAn Languages with Accents and Intonations</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4128</first_page>
						<last_page>4132</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2758</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sankar25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Šimon</given_name>
<surname>Sedláček</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Bolaji</given_name>
<surname>Yusuf</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ján</given_name>
<surname>Švec</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Pradyoth</given_name>
<surname>Hegde</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Santosh</given_name>
<surname>Kesiraju</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Oldřich</given_name>
<surname>Plchot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jan</given_name>
<surname>Černocký</surname>
</person_name>
					</contributors>
					<titles><title>Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>1748</first_page>
						<last_page>1752</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2764</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/sedlacek25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Praveen</given_name>
<surname>Srinivasa Varadhan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sherry</given_name>
<surname>Thomas</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sai</given_name>
<surname>Teja M S</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Suvrat</given_name>
<surname>Bhooshan</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mitesh M.</given_name>
<surname>Khapra</surname>
</person_name>
					</contributors>
					<titles><title>The State Of TTS: A Case Study with Human Fooling Rates</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2285</first_page>
						<last_page>2289</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2765</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/srinivasavaradhan25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Upendra Vishwanath</given_name>
<surname>Y. S.</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanuka</given_name>
<surname>Bhattacharjee</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Deekshitha</given_name>
<surname>G</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sathvik</given_name>
<surname>Udupa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kumar</given_name>
<surname>Chowdam Venkata Thirumala</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Madassu</given_name>
<surname>Keerthipriya</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Darshan</given_name>
<surname>Chikktimmegowda</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Dipti</given_name>
<surname>Baskar</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Yamini</given_name>
<surname>Belur</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Seena</given_name>
<surname>Vengalil</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Atchayaram</given_name>
<surname>Nalini</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Prasanta Kumar</given_name>
<surname>Ghosh</surname>
</person_name>
					</contributors>
					<titles><title>Comparison of Acoustic and Textual Features for Dysarthria Severity Classification in Amyotrophic Lateral Sclerosis</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>803</first_page>
						<last_page>807</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2767</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/ys25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Dimme</given_name>
<surname>de Groot</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanvina</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Devendra</given_name>
<surname>Kayande</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhengjun</given_name>
<surname>Yue</surname>
</person_name>
					</contributors>
					<titles><title>Objective and Subjective Evaluation of Diffusion-Based Speech  Enhancement for Dysarthric Speech</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2740</first_page>
						<last_page>2744</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2768</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/degroot25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Ziwei</given_name>
<surname>Gong</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lin</given_name>
<surname>Ai</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Harsh</given_name>
<surname>Deshpande</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Alexander</given_name>
<surname>Johnson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Emmy</given_name>
<surname>Phung</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zehui</given_name>
<surname>Wu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Ahmad</given_name>
<surname>Emami</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Julia</given_name>
<surname>Hirschberg</surname>
</person_name>
					</contributors>
					<titles><title>Comparison-Based Automatic Evaluation for Meeting Summarization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>291</first_page>
						<last_page>295</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2771</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/gong25c_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Zhengjun</given_name>
<surname>Yue</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mara</given_name>
<surname>Barberis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanvina</given_name>
<surname>Patel</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Judith</given_name>
<surname>Dineley</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Willemijn</given_name>
<surname>Doedens</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Lottie</given_name>
<surname>Stipdonk</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>YuanYuan</given_name>
<surname>Zhang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Elke de</given_name>
<surname>Witte</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Erfan</given_name>
<surname>Loweimi</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Hugo</given_name>
<surname>Van hamme</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Djaina</given_name>
<surname>Satoer</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Marina</given_name>
<surname>Ruiter</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Laureano Moro</given_name>
<surname>Velazquez</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Nicholas</given_name>
<surname>Cummins</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Odette</given_name>
<surname>Scharenborg</surname>
</person_name>
					</contributors>
					<titles><title>Challenges and practical guidelines for atypical speech data collection, annotation, usage and sharing: A multi-project perspective</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>3943</first_page>
						<last_page>3947</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2774</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yue25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Omer</given_name>
<surname>Moussa</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Mariya</given_name>
<surname>Toneva</surname>
</person_name>
					</contributors>
					<titles><title> Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2905</first_page>
						<last_page>2909</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2776</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/moussa25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yi</given_name>
<surname>Chang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhao</given_name>
<surname>Ren</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Zhonghao</given_name>
<surname>Zhao</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thanh Tam</given_name>
<surname>Nguyen</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Kun</given_name>
<surname>Qian</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tanja</given_name>
<surname>Schultz</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Björn W.</given_name>
<surname>Schuller</surname>
</person_name>
					</contributors>
					<titles><title>Breaking Resource Barriers in Speech Emotion Recognition via Data Distillation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>141</first_page>
						<last_page>145</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2778</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/chang25d_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Yiyuan</given_name>
<surname>Yang</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Shitong</given_name>
<surname>Xu</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Niki</given_name>
<surname>Trigoni</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Andrew</given_name>
<surname>Markham</surname>
</person_name>
					</contributors>
					<titles><title>Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2495</first_page>
						<last_page>2499</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2779</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/yang25q_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Roland</given_name>
<surname>Hartanto</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Sakriani</given_name>
<surname>Sakti</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Koichi</given_name>
<surname>Shinoda</surname>
</person_name>
					</contributors>
					<titles><title>SepVAC: Multitask Learning of Speaker Separation, Speaker Localization, Microphone Array Localization, and Room Acoustic Parameter Estimation in Various Acoustic Conditions </title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2480</first_page>
						<last_page>2484</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2784</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/hartanto25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Lubos</given_name>
<surname>Marcinek</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Jonas</given_name>
<surname>Beskow</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joakim</given_name>
<surname>Gustafsson</surname>
</person_name>
					</contributors>
					<titles><title>Towards Adaptable and Intelligible Speech Synthesis in Noisy Environments</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>2165</first_page>
						<last_page>2169</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2787</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/marcinek25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Steffen</given_name>
<surname>Freisinger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Philipp</given_name>
<surname>Seeberger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Thomas</given_name>
<surname>Ranzenberger</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Tobias</given_name>
<surname>Bocklet</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Korbinian</given_name>
<surname>Riedhammer</surname>
</person_name>
					</contributors>
					<titles><title>Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>276</first_page>
						<last_page>280</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2792</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/freisinger25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
				<conference_paper publication_type="full_text">
					<contributors>
						<person_name sequence="first" contributor_role="author">
<given_name>Juliana</given_name>
<surname>Francis</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Joakim</given_name>
<surname>Gustafsson</surname>
</person_name><person_name sequence="additional" contributor_role="author">
<given_name>Éva</given_name>
<surname>Székely</surname>
</person_name>
					</contributors>
					<titles><title>From Static to Dynamic: Enhancing AAC with Generative Imagery and Zero-Shot TTS</title></titles>
					<publication_date media_type='online'>
						<month>8</month>
						<day>17</day>
						<year>2025</year>
					</publication_date>
					<pages>
						<first_page>4960</first_page>
						<last_page>4962</last_page>
					</pages>
					<doi_data>
						<doi>10.21437/Interspeech.2025-2815</doi>
						<resource>https://www.isca-archive.org/interspeech_2025/francis25_interspeech.html</resource>
					</doi_data>
				</conference_paper>
		</conference>
	</body>
</doi_batch>
